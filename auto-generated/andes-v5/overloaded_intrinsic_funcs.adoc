
=== Andes Vector BFLOAT16 Conversion Extension(XAndesVBFHCvt)

[[overloaded-bf16-vector-unit-stride-load]]
==== Vector Unit-Stride Load Intrinsics(XAndesVBFHCvt)

[,c]
----
// masked functions
vbfloat16mf4_t __riscv_vle16(vbool64_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16(vbool32_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16(vbool16_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16(vbool8_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16(vbool4_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16(vbool2_t vm, const __bf16 *rs1, size_t vl);
----

[[overloaded-bf16-vector-unit-stride-store]]
==== Vector Unit-Stride Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vse16(__bf16 *rs1, vbfloat16mf4_t vs3, size_t vl);
void __riscv_vse16(__bf16 *rs1, vbfloat16mf2_t vs3, size_t vl);
void __riscv_vse16(__bf16 *rs1, vbfloat16m1_t vs3, size_t vl);
void __riscv_vse16(__bf16 *rs1, vbfloat16m2_t vs3, size_t vl);
void __riscv_vse16(__bf16 *rs1, vbfloat16m4_t vs3, size_t vl);
void __riscv_vse16(__bf16 *rs1, vbfloat16m8_t vs3, size_t vl);
// masked functions
void __riscv_vse16(vbool64_t vm, __bf16 *rs1, vbfloat16mf4_t vs3, size_t vl);
void __riscv_vse16(vbool32_t vm, __bf16 *rs1, vbfloat16mf2_t vs3, size_t vl);
void __riscv_vse16(vbool16_t vm, __bf16 *rs1, vbfloat16m1_t vs3, size_t vl);
void __riscv_vse16(vbool8_t vm, __bf16 *rs1, vbfloat16m2_t vs3, size_t vl);
void __riscv_vse16(vbool4_t vm, __bf16 *rs1, vbfloat16m4_t vs3, size_t vl);
void __riscv_vse16(vbool2_t vm, __bf16 *rs1, vbfloat16m8_t vs3, size_t vl);
----

[[overloaded-vector-strided-load]]
==== Vector Strided Load Intrinsics(XAndesVBFHCvt)

[,c]
----
// masked functions
vbfloat16mf4_t __riscv_vlse16(vbool64_t vm, const __bf16 *rs1, ptrdiff_t rs2,
                              size_t vl);
vbfloat16mf2_t __riscv_vlse16(vbool32_t vm, const __bf16 *rs1, ptrdiff_t rs2,
                              size_t vl);
vbfloat16m1_t __riscv_vlse16(vbool16_t vm, const __bf16 *rs1, ptrdiff_t rs2,
                             size_t vl);
vbfloat16m2_t __riscv_vlse16(vbool8_t vm, const __bf16 *rs1, ptrdiff_t rs2,
                             size_t vl);
vbfloat16m4_t __riscv_vlse16(vbool4_t vm, const __bf16 *rs1, ptrdiff_t rs2,
                             size_t vl);
vbfloat16m8_t __riscv_vlse16(vbool2_t vm, const __bf16 *rs1, ptrdiff_t rs2,
                             size_t vl);
----

[[overloaded-vector-strided-store]]
==== Vector Strided Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vsse16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsse16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsse16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1_t vs3, size_t vl);
void __riscv_vsse16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m2_t vs3, size_t vl);
void __riscv_vsse16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m4_t vs3, size_t vl);
void __riscv_vsse16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m8_t vs3, size_t vl);
// masked functions
void __riscv_vsse16(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                    vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsse16(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                    vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsse16(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2, vbfloat16m1_t vs3,
                    size_t vl);
void __riscv_vsse16(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2, vbfloat16m2_t vs3,
                    size_t vl);
void __riscv_vsse16(vbool4_t vm, __bf16 *rs1, ptrdiff_t rs2, vbfloat16m4_t vs3,
                    size_t vl);
void __riscv_vsse16(vbool2_t vm, __bf16 *rs1, ptrdiff_t rs2, vbfloat16m8_t vs3,
                    size_t vl);
----

[[overloaded-vector-indexed-load]]
==== Vector Indexed Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vloxei16(const __bf16 *rs1, vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vloxei16(const __bf16 *rs1, vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vloxei16(const __bf16 *rs1, vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vloxei16(const __bf16 *rs1, vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vloxei16(const __bf16 *rs1, vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vloxei16(const __bf16 *rs1, vuint16m8_t rs2, size_t vl);
vbfloat16mf4_t __riscv_vluxei16(const __bf16 *rs1, vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vluxei16(const __bf16 *rs1, vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vluxei16(const __bf16 *rs1, vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vluxei16(const __bf16 *rs1, vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vluxei16(const __bf16 *rs1, vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vluxei16(const __bf16 *rs1, vuint16m8_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16(vbool64_t vm, const __bf16 *rs1,
                                vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vloxei16(vbool32_t vm, const __bf16 *rs1,
                                vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vloxei16(vbool16_t vm, const __bf16 *rs1, vuint16m1_t rs2,
                               size_t vl);
vbfloat16m2_t __riscv_vloxei16(vbool8_t vm, const __bf16 *rs1, vuint16m2_t rs2,
                               size_t vl);
vbfloat16m4_t __riscv_vloxei16(vbool4_t vm, const __bf16 *rs1, vuint16m4_t rs2,
                               size_t vl);
vbfloat16m8_t __riscv_vloxei16(vbool2_t vm, const __bf16 *rs1, vuint16m8_t rs2,
                               size_t vl);
vbfloat16mf4_t __riscv_vluxei16(vbool64_t vm, const __bf16 *rs1,
                                vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vluxei16(vbool32_t vm, const __bf16 *rs1,
                                vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vluxei16(vbool16_t vm, const __bf16 *rs1, vuint16m1_t rs2,
                               size_t vl);
vbfloat16m2_t __riscv_vluxei16(vbool8_t vm, const __bf16 *rs1, vuint16m2_t rs2,
                               size_t vl);
vbfloat16m4_t __riscv_vluxei16(vbool4_t vm, const __bf16 *rs1, vuint16m4_t rs2,
                               size_t vl);
vbfloat16m8_t __riscv_vluxei16(vbool2_t vm, const __bf16 *rs1, vuint16m8_t rs2,
                               size_t vl);
----

[[overloaded-vector-indexed-store]]
==== Vector Indexed Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vsoxei16(__bf16 *rs1, vuint16mf4_t rs2, vbfloat16mf4_t vs3,
                      size_t vl);
void __riscv_vsoxei16(__bf16 *rs1, vuint16mf2_t rs2, vbfloat16mf2_t vs3,
                      size_t vl);
void __riscv_vsoxei16(__bf16 *rs1, vuint16m1_t rs2, vbfloat16m1_t vs3,
                      size_t vl);
void __riscv_vsoxei16(__bf16 *rs1, vuint16m2_t rs2, vbfloat16m2_t vs3,
                      size_t vl);
void __riscv_vsoxei16(__bf16 *rs1, vuint16m4_t rs2, vbfloat16m4_t vs3,
                      size_t vl);
void __riscv_vsoxei16(__bf16 *rs1, vuint16m8_t rs2, vbfloat16m8_t vs3,
                      size_t vl);
void __riscv_vsuxei16(__bf16 *rs1, vuint16mf4_t rs2, vbfloat16mf4_t vs3,
                      size_t vl);
void __riscv_vsuxei16(__bf16 *rs1, vuint16mf2_t rs2, vbfloat16mf2_t vs3,
                      size_t vl);
void __riscv_vsuxei16(__bf16 *rs1, vuint16m1_t rs2, vbfloat16m1_t vs3,
                      size_t vl);
void __riscv_vsuxei16(__bf16 *rs1, vuint16m2_t rs2, vbfloat16m2_t vs3,
                      size_t vl);
void __riscv_vsuxei16(__bf16 *rs1, vuint16m4_t rs2, vbfloat16m4_t vs3,
                      size_t vl);
void __riscv_vsuxei16(__bf16 *rs1, vuint16m8_t rs2, vbfloat16m8_t vs3,
                      size_t vl);
// masked functions
void __riscv_vsoxei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t rs2,
                      vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsoxei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t rs2,
                      vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsoxei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t rs2,
                      vbfloat16m1_t vs3, size_t vl);
void __riscv_vsoxei16(vbool8_t vm, __bf16 *rs1, vuint16m2_t rs2,
                      vbfloat16m2_t vs3, size_t vl);
void __riscv_vsoxei16(vbool4_t vm, __bf16 *rs1, vuint16m4_t rs2,
                      vbfloat16m4_t vs3, size_t vl);
void __riscv_vsoxei16(vbool2_t vm, __bf16 *rs1, vuint16m8_t rs2,
                      vbfloat16m8_t vs3, size_t vl);
void __riscv_vsuxei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t rs2,
                      vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsuxei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t rs2,
                      vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsuxei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t rs2,
                      vbfloat16m1_t vs3, size_t vl);
void __riscv_vsuxei16(vbool8_t vm, __bf16 *rs1, vuint16m2_t rs2,
                      vbfloat16m2_t vs3, size_t vl);
void __riscv_vsuxei16(vbool4_t vm, __bf16 *rs1, vuint16m4_t rs2,
                      vbfloat16m4_t vs3, size_t vl);
void __riscv_vsuxei16(vbool2_t vm, __bf16 *rs1, vuint16m8_t rs2,
                      vbfloat16m8_t vs3, size_t vl);
----

[[overloaded-unit-stride-fault-only-first-loads]]
==== Unit-stride Fault-Only-First Loads Intrinsics(XAndesVBFHCvt)

[,c]
----
// masked functions
vbfloat16mf4_t __riscv_vle16ff(vbool64_t vm, const __bf16 *rs1, size_t *new_vl,
                               size_t vl);
vbfloat16mf2_t __riscv_vle16ff(vbool32_t vm, const __bf16 *rs1, size_t *new_vl,
                               size_t vl);
vbfloat16m1_t __riscv_vle16ff(vbool16_t vm, const __bf16 *rs1, size_t *new_vl,
                              size_t vl);
vbfloat16m2_t __riscv_vle16ff(vbool8_t vm, const __bf16 *rs1, size_t *new_vl,
                              size_t vl);
vbfloat16m4_t __riscv_vle16ff(vbool4_t vm, const __bf16 *rs1, size_t *new_vl,
                              size_t vl);
vbfloat16m8_t __riscv_vle16ff(vbool2_t vm, const __bf16 *rs1, size_t *new_vl,
                              size_t vl);
----

[[overloaded-vector-unit-stride-segment-load]]
==== Vector Unit-Stride Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
// masked functions
vbfloat16mf4x2_t __riscv_vlseg2e16(vbool64_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16(vbool64_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16(vbool64_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16(vbool64_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16(vbool64_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16(vbool64_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16(vbool64_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16(vbool32_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16(vbool32_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16(vbool32_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16(vbool32_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16(vbool32_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16(vbool32_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16(vbool32_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16(vbool16_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16(vbool16_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16(vbool16_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16(vbool16_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16(vbool16_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16(vbool16_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16(vbool16_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16(vbool8_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16(vbool8_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16(vbool8_t vm, const __bf16 *rs1, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16(vbool4_t vm, const __bf16 *rs1, size_t vl);
vbfloat16mf4x2_t __riscv_vlseg2e16ff(vbool64_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16ff(vbool64_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16ff(vbool64_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16ff(vbool64_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16ff(vbool64_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16ff(vbool64_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16ff(vbool64_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16ff(vbool32_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16ff(vbool32_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16ff(vbool32_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16ff(vbool32_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16ff(vbool32_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16ff(vbool32_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16ff(vbool32_t vm, const __bf16 *rs1,
                                     size_t *new_vl, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16ff(vbool16_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16ff(vbool16_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16ff(vbool16_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16ff(vbool16_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16ff(vbool16_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16ff(vbool16_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16ff(vbool16_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16ff(vbool8_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16ff(vbool8_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16ff(vbool8_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16ff(vbool4_t vm, const __bf16 *rs1,
                                    size_t *new_vl, size_t vl);
----

[[overloaded-vecrtor-unit-stride-segment-store]]
==== Vector Unit-Stride Segment Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vsseg2e16(__bf16 *rs1, vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vsseg3e16(__bf16 *rs1, vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vsseg4e16(__bf16 *rs1, vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vsseg5e16(__bf16 *rs1, vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vsseg6e16(__bf16 *rs1, vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vsseg7e16(__bf16 *rs1, vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vsseg8e16(__bf16 *rs1, vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vsseg2e16(__bf16 *rs1, vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vsseg3e16(__bf16 *rs1, vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vsseg4e16(__bf16 *rs1, vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vsseg5e16(__bf16 *rs1, vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vsseg6e16(__bf16 *rs1, vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vsseg7e16(__bf16 *rs1, vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vsseg8e16(__bf16 *rs1, vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vsseg2e16(__bf16 *rs1, vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vsseg3e16(__bf16 *rs1, vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vsseg4e16(__bf16 *rs1, vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vsseg5e16(__bf16 *rs1, vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vsseg6e16(__bf16 *rs1, vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vsseg7e16(__bf16 *rs1, vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vsseg8e16(__bf16 *rs1, vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vsseg2e16(__bf16 *rs1, vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vsseg3e16(__bf16 *rs1, vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vsseg4e16(__bf16 *rs1, vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vsseg2e16(__bf16 *rs1, vbfloat16m4x2_t vs3, size_t vl);
// masked functions
void __riscv_vsseg2e16(vbool64_t vm, __bf16 *rs1, vbfloat16mf4x2_t vs3,
                       size_t vl);
void __riscv_vsseg3e16(vbool64_t vm, __bf16 *rs1, vbfloat16mf4x3_t vs3,
                       size_t vl);
void __riscv_vsseg4e16(vbool64_t vm, __bf16 *rs1, vbfloat16mf4x4_t vs3,
                       size_t vl);
void __riscv_vsseg5e16(vbool64_t vm, __bf16 *rs1, vbfloat16mf4x5_t vs3,
                       size_t vl);
void __riscv_vsseg6e16(vbool64_t vm, __bf16 *rs1, vbfloat16mf4x6_t vs3,
                       size_t vl);
void __riscv_vsseg7e16(vbool64_t vm, __bf16 *rs1, vbfloat16mf4x7_t vs3,
                       size_t vl);
void __riscv_vsseg8e16(vbool64_t vm, __bf16 *rs1, vbfloat16mf4x8_t vs3,
                       size_t vl);
void __riscv_vsseg2e16(vbool32_t vm, __bf16 *rs1, vbfloat16mf2x2_t vs3,
                       size_t vl);
void __riscv_vsseg3e16(vbool32_t vm, __bf16 *rs1, vbfloat16mf2x3_t vs3,
                       size_t vl);
void __riscv_vsseg4e16(vbool32_t vm, __bf16 *rs1, vbfloat16mf2x4_t vs3,
                       size_t vl);
void __riscv_vsseg5e16(vbool32_t vm, __bf16 *rs1, vbfloat16mf2x5_t vs3,
                       size_t vl);
void __riscv_vsseg6e16(vbool32_t vm, __bf16 *rs1, vbfloat16mf2x6_t vs3,
                       size_t vl);
void __riscv_vsseg7e16(vbool32_t vm, __bf16 *rs1, vbfloat16mf2x7_t vs3,
                       size_t vl);
void __riscv_vsseg8e16(vbool32_t vm, __bf16 *rs1, vbfloat16mf2x8_t vs3,
                       size_t vl);
void __riscv_vsseg2e16(vbool16_t vm, __bf16 *rs1, vbfloat16m1x2_t vs3,
                       size_t vl);
void __riscv_vsseg3e16(vbool16_t vm, __bf16 *rs1, vbfloat16m1x3_t vs3,
                       size_t vl);
void __riscv_vsseg4e16(vbool16_t vm, __bf16 *rs1, vbfloat16m1x4_t vs3,
                       size_t vl);
void __riscv_vsseg5e16(vbool16_t vm, __bf16 *rs1, vbfloat16m1x5_t vs3,
                       size_t vl);
void __riscv_vsseg6e16(vbool16_t vm, __bf16 *rs1, vbfloat16m1x6_t vs3,
                       size_t vl);
void __riscv_vsseg7e16(vbool16_t vm, __bf16 *rs1, vbfloat16m1x7_t vs3,
                       size_t vl);
void __riscv_vsseg8e16(vbool16_t vm, __bf16 *rs1, vbfloat16m1x8_t vs3,
                       size_t vl);
void __riscv_vsseg2e16(vbool8_t vm, __bf16 *rs1, vbfloat16m2x2_t vs3,
                       size_t vl);
void __riscv_vsseg3e16(vbool8_t vm, __bf16 *rs1, vbfloat16m2x3_t vs3,
                       size_t vl);
void __riscv_vsseg4e16(vbool8_t vm, __bf16 *rs1, vbfloat16m2x4_t vs3,
                       size_t vl);
void __riscv_vsseg2e16(vbool4_t vm, __bf16 *rs1, vbfloat16m4x2_t vs3,
                       size_t vl);
----

[[overloaded-vector-strided-segment-load]]
==== Vector Strided Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
// masked functions
vbfloat16mf4x2_t __riscv_vlsseg2e16(vbool64_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vlsseg3e16(vbool64_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vlsseg4e16(vbool64_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vlsseg5e16(vbool64_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vlsseg6e16(vbool64_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vlsseg7e16(vbool64_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vlsseg8e16(vbool64_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vlsseg2e16(vbool32_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vlsseg3e16(vbool32_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vlsseg4e16(vbool32_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vlsseg5e16(vbool32_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vlsseg6e16(vbool32_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vlsseg7e16(vbool32_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vlsseg8e16(vbool32_t vm, const __bf16 *rs1,
                                    ptrdiff_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vlsseg2e16(vbool16_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vlsseg3e16(vbool16_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vlsseg4e16(vbool16_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vlsseg5e16(vbool16_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vlsseg6e16(vbool16_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vlsseg7e16(vbool16_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vlsseg8e16(vbool16_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vlsseg2e16(vbool8_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vlsseg3e16(vbool8_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vlsseg4e16(vbool8_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vlsseg2e16(vbool4_t vm, const __bf16 *rs1,
                                   ptrdiff_t rs2, size_t vl);
----

[[overloaded-vector-strided-segment-store]]
==== Vector Strided Segment Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vssseg2e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4x2_t vs3,
                        size_t vl);
void __riscv_vssseg3e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4x3_t vs3,
                        size_t vl);
void __riscv_vssseg4e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4x4_t vs3,
                        size_t vl);
void __riscv_vssseg5e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4x5_t vs3,
                        size_t vl);
void __riscv_vssseg6e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4x6_t vs3,
                        size_t vl);
void __riscv_vssseg7e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4x7_t vs3,
                        size_t vl);
void __riscv_vssseg8e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4x8_t vs3,
                        size_t vl);
void __riscv_vssseg2e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2x2_t vs3,
                        size_t vl);
void __riscv_vssseg3e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2x3_t vs3,
                        size_t vl);
void __riscv_vssseg4e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2x4_t vs3,
                        size_t vl);
void __riscv_vssseg5e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2x5_t vs3,
                        size_t vl);
void __riscv_vssseg6e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2x6_t vs3,
                        size_t vl);
void __riscv_vssseg7e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2x7_t vs3,
                        size_t vl);
void __riscv_vssseg8e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2x8_t vs3,
                        size_t vl);
void __riscv_vssseg2e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1x2_t vs3,
                        size_t vl);
void __riscv_vssseg3e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1x3_t vs3,
                        size_t vl);
void __riscv_vssseg4e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1x4_t vs3,
                        size_t vl);
void __riscv_vssseg5e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1x5_t vs3,
                        size_t vl);
void __riscv_vssseg6e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1x6_t vs3,
                        size_t vl);
void __riscv_vssseg7e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1x7_t vs3,
                        size_t vl);
void __riscv_vssseg8e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1x8_t vs3,
                        size_t vl);
void __riscv_vssseg2e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m2x2_t vs3,
                        size_t vl);
void __riscv_vssseg3e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m2x3_t vs3,
                        size_t vl);
void __riscv_vssseg4e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m2x4_t vs3,
                        size_t vl);
void __riscv_vssseg2e16(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m4x2_t vs3,
                        size_t vl);
// masked functions
void __riscv_vssseg2e16(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vssseg3e16(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vssseg4e16(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vssseg5e16(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vssseg6e16(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vssseg7e16(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vssseg8e16(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vssseg2e16(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vssseg3e16(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vssseg4e16(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vssseg5e16(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vssseg6e16(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vssseg7e16(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vssseg8e16(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vssseg2e16(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vssseg3e16(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vssseg4e16(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vssseg5e16(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vssseg6e16(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vssseg7e16(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vssseg8e16(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vssseg2e16(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vssseg3e16(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vssseg4e16(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vssseg2e16(vbool4_t vm, __bf16 *rs1, ptrdiff_t rs2,
                        vbfloat16m4x2_t vs3, size_t vl);
----

[[overloaded-vector-indexed-segment-load]]
==== Vector Indexed Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4x2_t __riscv_vloxseg2ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x3_t __riscv_vloxseg3ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x4_t __riscv_vloxseg4ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x5_t __riscv_vloxseg5ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x6_t __riscv_vloxseg6ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x7_t __riscv_vloxseg7ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x8_t __riscv_vloxseg8ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf2x2_t __riscv_vloxseg2ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x3_t __riscv_vloxseg3ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x4_t __riscv_vloxseg4ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x5_t __riscv_vloxseg5ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x6_t __riscv_vloxseg6ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x7_t __riscv_vloxseg7ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x8_t __riscv_vloxseg8ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16m1x2_t __riscv_vloxseg2ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x3_t __riscv_vloxseg3ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x4_t __riscv_vloxseg4ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x5_t __riscv_vloxseg5ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x6_t __riscv_vloxseg6ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x7_t __riscv_vloxseg7ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x8_t __riscv_vloxseg8ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m2x2_t __riscv_vloxseg2ei16(const __bf16 *rs1, vuint16m2_t rs2,
                                     size_t vl);
vbfloat16m2x3_t __riscv_vloxseg3ei16(const __bf16 *rs1, vuint16m2_t rs2,
                                     size_t vl);
vbfloat16m2x4_t __riscv_vloxseg4ei16(const __bf16 *rs1, vuint16m2_t rs2,
                                     size_t vl);
vbfloat16m4x2_t __riscv_vloxseg2ei16(const __bf16 *rs1, vuint16m4_t rs2,
                                     size_t vl);
vbfloat16mf4x2_t __riscv_vluxseg2ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x3_t __riscv_vluxseg3ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x4_t __riscv_vluxseg4ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x5_t __riscv_vluxseg5ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x6_t __riscv_vluxseg6ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x7_t __riscv_vluxseg7ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf4x8_t __riscv_vluxseg8ei16(const __bf16 *rs1, vuint16mf4_t rs2,
                                      size_t vl);
vbfloat16mf2x2_t __riscv_vluxseg2ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x3_t __riscv_vluxseg3ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x4_t __riscv_vluxseg4ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x5_t __riscv_vluxseg5ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x6_t __riscv_vluxseg6ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x7_t __riscv_vluxseg7ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16mf2x8_t __riscv_vluxseg8ei16(const __bf16 *rs1, vuint16mf2_t rs2,
                                      size_t vl);
vbfloat16m1x2_t __riscv_vluxseg2ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x3_t __riscv_vluxseg3ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x4_t __riscv_vluxseg4ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x5_t __riscv_vluxseg5ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x6_t __riscv_vluxseg6ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x7_t __riscv_vluxseg7ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m1x8_t __riscv_vluxseg8ei16(const __bf16 *rs1, vuint16m1_t rs2,
                                     size_t vl);
vbfloat16m2x2_t __riscv_vluxseg2ei16(const __bf16 *rs1, vuint16m2_t rs2,
                                     size_t vl);
vbfloat16m2x3_t __riscv_vluxseg3ei16(const __bf16 *rs1, vuint16m2_t rs2,
                                     size_t vl);
vbfloat16m2x4_t __riscv_vluxseg4ei16(const __bf16 *rs1, vuint16m2_t rs2,
                                     size_t vl);
vbfloat16m4x2_t __riscv_vluxseg2ei16(const __bf16 *rs1, vuint16m4_t rs2,
                                     size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vloxseg2ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vloxseg3ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vloxseg4ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vloxseg5ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vloxseg6ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vloxseg7ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vloxseg8ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vloxseg2ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vloxseg3ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vloxseg4ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vloxseg5ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vloxseg6ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vloxseg7ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vloxseg8ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vloxseg2ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vloxseg3ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vloxseg4ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vloxseg5ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vloxseg6ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vloxseg7ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vloxseg8ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vloxseg2ei16(vbool8_t vm, const __bf16 *rs1,
                                     vuint16m2_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vloxseg3ei16(vbool8_t vm, const __bf16 *rs1,
                                     vuint16m2_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vloxseg4ei16(vbool8_t vm, const __bf16 *rs1,
                                     vuint16m2_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vloxseg2ei16(vbool4_t vm, const __bf16 *rs1,
                                     vuint16m4_t rs2, size_t vl);
vbfloat16mf4x2_t __riscv_vluxseg2ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vluxseg3ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vluxseg4ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vluxseg5ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vluxseg6ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vluxseg7ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vluxseg8ei16(vbool64_t vm, const __bf16 *rs1,
                                      vuint16mf4_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vluxseg2ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vluxseg3ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vluxseg4ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vluxseg5ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vluxseg6ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vluxseg7ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vluxseg8ei16(vbool32_t vm, const __bf16 *rs1,
                                      vuint16mf2_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vluxseg2ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vluxseg3ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vluxseg4ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vluxseg5ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vluxseg6ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vluxseg7ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vluxseg8ei16(vbool16_t vm, const __bf16 *rs1,
                                     vuint16m1_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vluxseg2ei16(vbool8_t vm, const __bf16 *rs1,
                                     vuint16m2_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vluxseg3ei16(vbool8_t vm, const __bf16 *rs1,
                                     vuint16m2_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vluxseg4ei16(vbool8_t vm, const __bf16 *rs1,
                                     vuint16m2_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vluxseg2ei16(vbool4_t vm, const __bf16 *rs1,
                                     vuint16m4_t rs2, size_t vl);
----

[[overloaded-vector-indexed-segment-store]]
==== Vector Indexed Segment Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vsoxseg2ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x2_t vs3,
                          size_t vl);
void __riscv_vsoxseg3ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x3_t vs3,
                          size_t vl);
void __riscv_vsoxseg4ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x4_t vs3,
                          size_t vl);
void __riscv_vsoxseg5ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x5_t vs3,
                          size_t vl);
void __riscv_vsoxseg6ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x6_t vs3,
                          size_t vl);
void __riscv_vsoxseg7ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x7_t vs3,
                          size_t vl);
void __riscv_vsoxseg8ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x8_t vs3,
                          size_t vl);
void __riscv_vsoxseg2ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x2_t vs3,
                          size_t vl);
void __riscv_vsoxseg3ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x3_t vs3,
                          size_t vl);
void __riscv_vsoxseg4ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x4_t vs3,
                          size_t vl);
void __riscv_vsoxseg5ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x5_t vs3,
                          size_t vl);
void __riscv_vsoxseg6ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x6_t vs3,
                          size_t vl);
void __riscv_vsoxseg7ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x7_t vs3,
                          size_t vl);
void __riscv_vsoxseg8ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x8_t vs3,
                          size_t vl);
void __riscv_vsoxseg2ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x2_t vs3,
                          size_t vl);
void __riscv_vsoxseg3ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x3_t vs3,
                          size_t vl);
void __riscv_vsoxseg4ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x4_t vs3,
                          size_t vl);
void __riscv_vsoxseg5ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x5_t vs3,
                          size_t vl);
void __riscv_vsoxseg6ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x6_t vs3,
                          size_t vl);
void __riscv_vsoxseg7ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x7_t vs3,
                          size_t vl);
void __riscv_vsoxseg8ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x8_t vs3,
                          size_t vl);
void __riscv_vsoxseg2ei16(__bf16 *rs1, vuint16m2_t vs2, vbfloat16m2x2_t vs3,
                          size_t vl);
void __riscv_vsoxseg3ei16(__bf16 *rs1, vuint16m2_t vs2, vbfloat16m2x3_t vs3,
                          size_t vl);
void __riscv_vsoxseg4ei16(__bf16 *rs1, vuint16m2_t vs2, vbfloat16m2x4_t vs3,
                          size_t vl);
void __riscv_vsoxseg2ei16(__bf16 *rs1, vuint16m4_t vs2, vbfloat16m4x2_t vs3,
                          size_t vl);
void __riscv_vsuxseg2ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x2_t vs3,
                          size_t vl);
void __riscv_vsuxseg3ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x3_t vs3,
                          size_t vl);
void __riscv_vsuxseg4ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x4_t vs3,
                          size_t vl);
void __riscv_vsuxseg5ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x5_t vs3,
                          size_t vl);
void __riscv_vsuxseg6ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x6_t vs3,
                          size_t vl);
void __riscv_vsuxseg7ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x7_t vs3,
                          size_t vl);
void __riscv_vsuxseg8ei16(__bf16 *rs1, vuint16mf4_t vs2, vbfloat16mf4x8_t vs3,
                          size_t vl);
void __riscv_vsuxseg2ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x2_t vs3,
                          size_t vl);
void __riscv_vsuxseg3ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x3_t vs3,
                          size_t vl);
void __riscv_vsuxseg4ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x4_t vs3,
                          size_t vl);
void __riscv_vsuxseg5ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x5_t vs3,
                          size_t vl);
void __riscv_vsuxseg6ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x6_t vs3,
                          size_t vl);
void __riscv_vsuxseg7ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x7_t vs3,
                          size_t vl);
void __riscv_vsuxseg8ei16(__bf16 *rs1, vuint16mf2_t vs2, vbfloat16mf2x8_t vs3,
                          size_t vl);
void __riscv_vsuxseg2ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x2_t vs3,
                          size_t vl);
void __riscv_vsuxseg3ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x3_t vs3,
                          size_t vl);
void __riscv_vsuxseg4ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x4_t vs3,
                          size_t vl);
void __riscv_vsuxseg5ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x5_t vs3,
                          size_t vl);
void __riscv_vsuxseg6ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x6_t vs3,
                          size_t vl);
void __riscv_vsuxseg7ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x7_t vs3,
                          size_t vl);
void __riscv_vsuxseg8ei16(__bf16 *rs1, vuint16m1_t vs2, vbfloat16m1x8_t vs3,
                          size_t vl);
void __riscv_vsuxseg2ei16(__bf16 *rs1, vuint16m2_t vs2, vbfloat16m2x2_t vs3,
                          size_t vl);
void __riscv_vsuxseg3ei16(__bf16 *rs1, vuint16m2_t vs2, vbfloat16m2x3_t vs3,
                          size_t vl);
void __riscv_vsuxseg4ei16(__bf16 *rs1, vuint16m2_t vs2, vbfloat16m2x4_t vs3,
                          size_t vl);
void __riscv_vsuxseg2ei16(__bf16 *rs1, vuint16m4_t vs2, vbfloat16m4x2_t vs3,
                          size_t vl);
// masked functions
void __riscv_vsoxseg2ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vsoxseg3ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vsoxseg4ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vsoxseg5ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vsoxseg6ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vsoxseg7ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vsoxseg8ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vsoxseg2ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vsoxseg3ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vsoxseg4ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vsoxseg5ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vsoxseg6ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vsoxseg7ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vsoxseg8ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vsoxseg2ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vsoxseg3ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vsoxseg4ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vsoxseg5ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vsoxseg6ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vsoxseg7ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vsoxseg8ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vsoxseg2ei16(vbool8_t vm, __bf16 *rs1, vuint16m2_t vs2,
                          vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vsoxseg3ei16(vbool8_t vm, __bf16 *rs1, vuint16m2_t vs2,
                          vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vsoxseg4ei16(vbool8_t vm, __bf16 *rs1, vuint16m2_t vs2,
                          vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vsoxseg2ei16(vbool4_t vm, __bf16 *rs1, vuint16m4_t vs2,
                          vbfloat16m4x2_t vs3, size_t vl);
void __riscv_vsuxseg2ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vsuxseg3ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vsuxseg4ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vsuxseg5ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vsuxseg6ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vsuxseg7ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vsuxseg8ei16(vbool64_t vm, __bf16 *rs1, vuint16mf4_t vs2,
                          vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vsuxseg2ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vsuxseg3ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vsuxseg4ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vsuxseg5ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vsuxseg6ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vsuxseg7ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vsuxseg8ei16(vbool32_t vm, __bf16 *rs1, vuint16mf2_t vs2,
                          vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vsuxseg2ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vsuxseg3ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vsuxseg4ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vsuxseg5ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vsuxseg6ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vsuxseg7ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vsuxseg8ei16(vbool16_t vm, __bf16 *rs1, vuint16m1_t vs2,
                          vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vsuxseg2ei16(vbool8_t vm, __bf16 *rs1, vuint16m2_t vs2,
                          vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vsuxseg3ei16(vbool8_t vm, __bf16 *rs1, vuint16m2_t vs2,
                          vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vsuxseg4ei16(vbool8_t vm, __bf16 *rs1, vuint16m2_t vs2,
                          vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vsuxseg2ei16(vbool4_t vm, __bf16 *rs1, vuint16m4_t vs2,
                          vbfloat16m4x2_t vs3, size_t vl);
----

[[overloaded-bf16-vector-narrow-convert]]
==== Vector Narrowing Convert Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_nds_vfncvt_bf16(vfloat32mf2_t vs2, size_t vl);
vbfloat16mf2_t __riscv_nds_vfncvt_bf16(vfloat32m1_t vs2, size_t vl);
vbfloat16m1_t __riscv_nds_vfncvt_bf16(vfloat32m2_t vs2, size_t vl);
vbfloat16m2_t __riscv_nds_vfncvt_bf16(vfloat32m4_t vs2, size_t vl);
vbfloat16m4_t __riscv_nds_vfncvt_bf16(vfloat32m8_t vs2, size_t vl);
vbfloat16mf4_t __riscv_nds_vfncvt_bf16(vfloat32mf2_t vs2, unsigned int frm,
                                       size_t vl);
vbfloat16mf2_t __riscv_nds_vfncvt_bf16(vfloat32m1_t vs2, unsigned int frm,
                                       size_t vl);
vbfloat16m1_t __riscv_nds_vfncvt_bf16(vfloat32m2_t vs2, unsigned int frm,
                                      size_t vl);
vbfloat16m2_t __riscv_nds_vfncvt_bf16(vfloat32m4_t vs2, unsigned int frm,
                                      size_t vl);
vbfloat16m4_t __riscv_nds_vfncvt_bf16(vfloat32m8_t vs2, unsigned int frm,
                                      size_t vl);
----

[[overloaded-bf16-vector-widening-convert]]
==== Vector Widening Convert Intrinsics(XAndesVBFHCvt)

[,c]
----
vfloat32mf2_t __riscv_nds_vfwcvt_s(vbfloat16mf4_t vs2, size_t vl);
vfloat32m1_t __riscv_nds_vfwcvt_s(vbfloat16mf2_t vs2, size_t vl);
vfloat32m2_t __riscv_nds_vfwcvt_s(vbfloat16m1_t vs2, size_t vl);
vfloat32m4_t __riscv_nds_vfwcvt_s(vbfloat16m2_t vs2, size_t vl);
vfloat32m8_t __riscv_nds_vfwcvt_s(vbfloat16m4_t vs2, size_t vl);
----

[[overloaded-reinterpret-cast-conversion]]
==== Reinterpret Cast Conversion Intrinsics(XAndesVBFHCvt)

[,c]
----
// Reinterpret between different type under the same SEW/LMUL
vbfloat16mf4_t __riscv_vreinterpret_bf16mf4(vint16mf4_t src);
vbfloat16mf2_t __riscv_vreinterpret_bf16mf2(vint16mf2_t src);
vbfloat16m1_t __riscv_vreinterpret_bf16m1(vint16m1_t src);
vbfloat16m2_t __riscv_vreinterpret_bf16m2(vint16m2_t src);
vbfloat16m4_t __riscv_vreinterpret_bf16m4(vint16m4_t src);
vbfloat16m8_t __riscv_vreinterpret_bf16m8(vint16m8_t src);
vbfloat16mf4_t __riscv_vreinterpret_bf16mf4(vuint16mf4_t src);
vbfloat16mf2_t __riscv_vreinterpret_bf16mf2(vuint16mf2_t src);
vbfloat16m1_t __riscv_vreinterpret_bf16m1(vuint16m1_t src);
vbfloat16m2_t __riscv_vreinterpret_bf16m2(vuint16m2_t src);
vbfloat16m4_t __riscv_vreinterpret_bf16m4(vuint16m4_t src);
vbfloat16m8_t __riscv_vreinterpret_bf16m8(vuint16m8_t src);
vint16mf4_t __riscv_vreinterpret_i16mf4(vbfloat16mf4_t src);
vint16mf2_t __riscv_vreinterpret_i16mf2(vbfloat16mf2_t src);
vint16m1_t __riscv_vreinterpret_i16m1(vbfloat16m1_t src);
vint16m2_t __riscv_vreinterpret_i16m2(vbfloat16m2_t src);
vint16m4_t __riscv_vreinterpret_i16m4(vbfloat16m4_t src);
vint16m8_t __riscv_vreinterpret_i16m8(vbfloat16m8_t src);
vuint16mf4_t __riscv_vreinterpret_u16mf4(vbfloat16mf4_t src);
vuint16mf2_t __riscv_vreinterpret_u16mf2(vbfloat16mf2_t src);
vuint16m1_t __riscv_vreinterpret_u16m1(vbfloat16m1_t src);
vuint16m2_t __riscv_vreinterpret_u16m2(vbfloat16m2_t src);
vuint16m4_t __riscv_vreinterpret_u16m4(vbfloat16m4_t src);
vuint16m8_t __riscv_vreinterpret_u16m8(vbfloat16m8_t src);
----

[[overloaded-vector-lmul-extensionn]]
==== Vector LMUL Extension Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf2_t __riscv_vlmul_ext_bf16mf2(vbfloat16mf4_t value);
vbfloat16m1_t __riscv_vlmul_ext_bf16m1(vbfloat16mf4_t value);
vbfloat16m2_t __riscv_vlmul_ext_bf16m2(vbfloat16mf4_t value);
vbfloat16m4_t __riscv_vlmul_ext_bf16m4(vbfloat16mf4_t value);
vbfloat16m8_t __riscv_vlmul_ext_bf16m8(vbfloat16mf4_t value);
vbfloat16m1_t __riscv_vlmul_ext_bf16m1(vbfloat16mf2_t value);
vbfloat16m2_t __riscv_vlmul_ext_bf16m2(vbfloat16mf2_t value);
vbfloat16m4_t __riscv_vlmul_ext_bf16m4(vbfloat16mf2_t value);
vbfloat16m8_t __riscv_vlmul_ext_bf16m8(vbfloat16mf2_t value);
vbfloat16m2_t __riscv_vlmul_ext_bf16m2(vbfloat16m1_t value);
vbfloat16m4_t __riscv_vlmul_ext_bf16m4(vbfloat16m1_t value);
vbfloat16m8_t __riscv_vlmul_ext_bf16m8(vbfloat16m1_t value);
vbfloat16m4_t __riscv_vlmul_ext_bf16m4(vbfloat16m2_t value);
vbfloat16m8_t __riscv_vlmul_ext_bf16m8(vbfloat16m2_t value);
vbfloat16m8_t __riscv_vlmul_ext_bf16m8(vbfloat16m4_t value);
----

[[overloaded-vector-lmul-truncation]]
==== Vector LMUL Truncation Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vlmul_trunc_bf16mf4(vbfloat16mf2_t value);
vbfloat16mf4_t __riscv_vlmul_trunc_bf16mf4(vbfloat16m1_t value);
vbfloat16mf2_t __riscv_vlmul_trunc_bf16mf2(vbfloat16m1_t value);
vbfloat16mf4_t __riscv_vlmul_trunc_bf16mf4(vbfloat16m2_t value);
vbfloat16mf2_t __riscv_vlmul_trunc_bf16mf2(vbfloat16m2_t value);
vbfloat16m1_t __riscv_vlmul_trunc_bf16m1(vbfloat16m2_t value);
vbfloat16mf4_t __riscv_vlmul_trunc_bf16mf4(vbfloat16m4_t value);
vbfloat16mf2_t __riscv_vlmul_trunc_bf16mf2(vbfloat16m4_t value);
vbfloat16m1_t __riscv_vlmul_trunc_bf16m1(vbfloat16m4_t value);
vbfloat16m2_t __riscv_vlmul_trunc_bf16m2(vbfloat16m4_t value);
vbfloat16mf4_t __riscv_vlmul_trunc_bf16mf4(vbfloat16m8_t value);
vbfloat16mf2_t __riscv_vlmul_trunc_bf16mf2(vbfloat16m8_t value);
vbfloat16m1_t __riscv_vlmul_trunc_bf16m1(vbfloat16m8_t value);
vbfloat16m2_t __riscv_vlmul_trunc_bf16m2(vbfloat16m8_t value);
vbfloat16m4_t __riscv_vlmul_trunc_bf16m4(vbfloat16m8_t value);
----

[[overloaded-vector-initialization]]
==== Vector Initialization Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have an overloaded variant.

[[overloaded-vector-insertion]]
==== Vector Insertion Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16m2_t __riscv_vset(vbfloat16m2_t dest, size_t index,
                           vbfloat16m1_t value);
vbfloat16m4_t __riscv_vset(vbfloat16m4_t dest, size_t index,
                           vbfloat16m1_t value);
vbfloat16m4_t __riscv_vset(vbfloat16m4_t dest, size_t index,
                           vbfloat16m2_t value);
vbfloat16m8_t __riscv_vset(vbfloat16m8_t dest, size_t index,
                           vbfloat16m1_t value);
vbfloat16m8_t __riscv_vset(vbfloat16m8_t dest, size_t index,
                           vbfloat16m2_t value);
vbfloat16m8_t __riscv_vset(vbfloat16m8_t dest, size_t index,
                           vbfloat16m4_t value);
vbfloat16mf4x2_t __riscv_vset(vbfloat16mf4x2_t dest, size_t index,
                              vbfloat16mf4_t value);
vbfloat16mf4x3_t __riscv_vset(vbfloat16mf4x3_t dest, size_t index,
                              vbfloat16mf4_t value);
vbfloat16mf4x4_t __riscv_vset(vbfloat16mf4x4_t dest, size_t index,
                              vbfloat16mf4_t value);
vbfloat16mf4x5_t __riscv_vset(vbfloat16mf4x5_t dest, size_t index,
                              vbfloat16mf4_t value);
vbfloat16mf4x6_t __riscv_vset(vbfloat16mf4x6_t dest, size_t index,
                              vbfloat16mf4_t value);
vbfloat16mf4x7_t __riscv_vset(vbfloat16mf4x7_t dest, size_t index,
                              vbfloat16mf4_t value);
vbfloat16mf4x8_t __riscv_vset(vbfloat16mf4x8_t dest, size_t index,
                              vbfloat16mf4_t value);
vbfloat16mf2x2_t __riscv_vset(vbfloat16mf2x2_t dest, size_t index,
                              vbfloat16mf2_t value);
vbfloat16mf2x3_t __riscv_vset(vbfloat16mf2x3_t dest, size_t index,
                              vbfloat16mf2_t value);
vbfloat16mf2x4_t __riscv_vset(vbfloat16mf2x4_t dest, size_t index,
                              vbfloat16mf2_t value);
vbfloat16mf2x5_t __riscv_vset(vbfloat16mf2x5_t dest, size_t index,
                              vbfloat16mf2_t value);
vbfloat16mf2x6_t __riscv_vset(vbfloat16mf2x6_t dest, size_t index,
                              vbfloat16mf2_t value);
vbfloat16mf2x7_t __riscv_vset(vbfloat16mf2x7_t dest, size_t index,
                              vbfloat16mf2_t value);
vbfloat16mf2x8_t __riscv_vset(vbfloat16mf2x8_t dest, size_t index,
                              vbfloat16mf2_t value);
vbfloat16m1x2_t __riscv_vset(vbfloat16m1x2_t dest, size_t index,
                             vbfloat16m1_t value);
vbfloat16m1x3_t __riscv_vset(vbfloat16m1x3_t dest, size_t index,
                             vbfloat16m1_t value);
vbfloat16m1x4_t __riscv_vset(vbfloat16m1x4_t dest, size_t index,
                             vbfloat16m1_t value);
vbfloat16m1x5_t __riscv_vset(vbfloat16m1x5_t dest, size_t index,
                             vbfloat16m1_t value);
vbfloat16m1x6_t __riscv_vset(vbfloat16m1x6_t dest, size_t index,
                             vbfloat16m1_t value);
vbfloat16m1x7_t __riscv_vset(vbfloat16m1x7_t dest, size_t index,
                             vbfloat16m1_t value);
vbfloat16m1x8_t __riscv_vset(vbfloat16m1x8_t dest, size_t index,
                             vbfloat16m1_t value);
vbfloat16m2x2_t __riscv_vset(vbfloat16m2x2_t dest, size_t index,
                             vbfloat16m2_t value);
vbfloat16m2x3_t __riscv_vset(vbfloat16m2x3_t dest, size_t index,
                             vbfloat16m2_t value);
vbfloat16m2x4_t __riscv_vset(vbfloat16m2x4_t dest, size_t index,
                             vbfloat16m2_t value);
vbfloat16m4x2_t __riscv_vset(vbfloat16m4x2_t dest, size_t index,
                             vbfloat16m4_t value);
----

[[overloaded-vector-extraction]]
==== Vector Extraction Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m2_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m4_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m8_t src, size_t index);
vbfloat16m2_t __riscv_vget_bf16m2(vbfloat16m4_t src, size_t index);
vbfloat16m2_t __riscv_vget_bf16m2(vbfloat16m8_t src, size_t index);
vbfloat16m4_t __riscv_vget_bf16m4(vbfloat16m8_t src, size_t index);
vbfloat16mf4_t __riscv_vget_bf16mf4(vbfloat16mf4x2_t src, size_t index);
vbfloat16mf4_t __riscv_vget_bf16mf4(vbfloat16mf4x3_t src, size_t index);
vbfloat16mf4_t __riscv_vget_bf16mf4(vbfloat16mf4x4_t src, size_t index);
vbfloat16mf4_t __riscv_vget_bf16mf4(vbfloat16mf4x5_t src, size_t index);
vbfloat16mf4_t __riscv_vget_bf16mf4(vbfloat16mf4x6_t src, size_t index);
vbfloat16mf4_t __riscv_vget_bf16mf4(vbfloat16mf4x7_t src, size_t index);
vbfloat16mf4_t __riscv_vget_bf16mf4(vbfloat16mf4x8_t src, size_t index);
vbfloat16mf2_t __riscv_vget_bf16mf2(vbfloat16mf2x2_t src, size_t index);
vbfloat16mf2_t __riscv_vget_bf16mf2(vbfloat16mf2x3_t src, size_t index);
vbfloat16mf2_t __riscv_vget_bf16mf2(vbfloat16mf2x4_t src, size_t index);
vbfloat16mf2_t __riscv_vget_bf16mf2(vbfloat16mf2x5_t src, size_t index);
vbfloat16mf2_t __riscv_vget_bf16mf2(vbfloat16mf2x6_t src, size_t index);
vbfloat16mf2_t __riscv_vget_bf16mf2(vbfloat16mf2x7_t src, size_t index);
vbfloat16mf2_t __riscv_vget_bf16mf2(vbfloat16mf2x8_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m1x2_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m1x3_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m1x4_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m1x5_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m1x6_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m1x7_t src, size_t index);
vbfloat16m1_t __riscv_vget_bf16m1(vbfloat16m1x8_t src, size_t index);
vbfloat16m2_t __riscv_vget_bf16m2(vbfloat16m2x2_t src, size_t index);
vbfloat16m2_t __riscv_vget_bf16m2(vbfloat16m2x3_t src, size_t index);
vbfloat16m2_t __riscv_vget_bf16m2(vbfloat16m2x4_t src, size_t index);
vbfloat16m4_t __riscv_vget_bf16m4(vbfloat16m4x2_t src, size_t index);
----

[[overloaded-vector-creation]]
==== Vector Creation Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have an overloaded variant.

=== Andes Vector Quad-Widening Integer Multiply-Add Extension(XAndesVQMac)

[[overloaded-vector-quad-widening-integer-multiply-add-operations]]
==== Vector Quad-Widening Integer Multiply-Add Intrinsics(XAndesVQMac)

[,c]
----
vint32mf2_t __riscv_nds_vqmacc(vint32mf2_t vd, vint8mf8_t vs1, vint8mf8_t vs2,
                               size_t vl);
vint32mf2_t __riscv_nds_vqmacc(vint32mf2_t vd, int8_t rs1, vint8mf8_t vs2,
                               size_t vl);
vint32m1_t __riscv_nds_vqmacc(vint32m1_t vd, vint8mf4_t vs1, vint8mf4_t vs2,
                              size_t vl);
vint32m1_t __riscv_nds_vqmacc(vint32m1_t vd, int8_t rs1, vint8mf4_t vs2,
                              size_t vl);
vint32m2_t __riscv_nds_vqmacc(vint32m2_t vd, vint8mf2_t vs1, vint8mf2_t vs2,
                              size_t vl);
vint32m2_t __riscv_nds_vqmacc(vint32m2_t vd, int8_t rs1, vint8mf2_t vs2,
                              size_t vl);
vint32m4_t __riscv_nds_vqmacc(vint32m4_t vd, vint8m1_t vs1, vint8m1_t vs2,
                              size_t vl);
vint32m4_t __riscv_nds_vqmacc(vint32m4_t vd, int8_t rs1, vint8m1_t vs2,
                              size_t vl);
vint32m8_t __riscv_nds_vqmacc(vint32m8_t vd, vint8m2_t vs1, vint8m2_t vs2,
                              size_t vl);
vint32m8_t __riscv_nds_vqmacc(vint32m8_t vd, int8_t rs1, vint8m2_t vs2,
                              size_t vl);
vint64m1_t __riscv_nds_vqmacc(vint64m1_t vd, vint16mf4_t vs1, vint16mf4_t vs2,
                              size_t vl);
vint64m1_t __riscv_nds_vqmacc(vint64m1_t vd, int16_t rs1, vint16mf4_t vs2,
                              size_t vl);
vint64m2_t __riscv_nds_vqmacc(vint64m2_t vd, vint16mf2_t vs1, vint16mf2_t vs2,
                              size_t vl);
vint64m2_t __riscv_nds_vqmacc(vint64m2_t vd, int16_t rs1, vint16mf2_t vs2,
                              size_t vl);
vint64m4_t __riscv_nds_vqmacc(vint64m4_t vd, vint16m1_t vs1, vint16m1_t vs2,
                              size_t vl);
vint64m4_t __riscv_nds_vqmacc(vint64m4_t vd, int16_t rs1, vint16m1_t vs2,
                              size_t vl);
vint64m8_t __riscv_nds_vqmacc(vint64m8_t vd, vint16m2_t vs1, vint16m2_t vs2,
                              size_t vl);
vint64m8_t __riscv_nds_vqmacc(vint64m8_t vd, int16_t rs1, vint16m2_t vs2,
                              size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu(vint32mf2_t vd, vint8mf8_t vs1,
                                 vuint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu(vint32mf2_t vd, int8_t rs1, vuint8mf8_t vs2,
                                 size_t vl);
vint32m1_t __riscv_nds_vqmaccsu(vint32m1_t vd, vint8mf4_t vs1, vuint8mf4_t vs2,
                                size_t vl);
vint32m1_t __riscv_nds_vqmaccsu(vint32m1_t vd, int8_t rs1, vuint8mf4_t vs2,
                                size_t vl);
vint32m2_t __riscv_nds_vqmaccsu(vint32m2_t vd, vint8mf2_t vs1, vuint8mf2_t vs2,
                                size_t vl);
vint32m2_t __riscv_nds_vqmaccsu(vint32m2_t vd, int8_t rs1, vuint8mf2_t vs2,
                                size_t vl);
vint32m4_t __riscv_nds_vqmaccsu(vint32m4_t vd, vint8m1_t vs1, vuint8m1_t vs2,
                                size_t vl);
vint32m4_t __riscv_nds_vqmaccsu(vint32m4_t vd, int8_t rs1, vuint8m1_t vs2,
                                size_t vl);
vint32m8_t __riscv_nds_vqmaccsu(vint32m8_t vd, vint8m2_t vs1, vuint8m2_t vs2,
                                size_t vl);
vint32m8_t __riscv_nds_vqmaccsu(vint32m8_t vd, int8_t rs1, vuint8m2_t vs2,
                                size_t vl);
vint64m1_t __riscv_nds_vqmaccsu(vint64m1_t vd, vint16mf4_t vs1,
                                vuint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu(vint64m1_t vd, int16_t rs1, vuint16mf4_t vs2,
                                size_t vl);
vint64m2_t __riscv_nds_vqmaccsu(vint64m2_t vd, vint16mf2_t vs1,
                                vuint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu(vint64m2_t vd, int16_t rs1, vuint16mf2_t vs2,
                                size_t vl);
vint64m4_t __riscv_nds_vqmaccsu(vint64m4_t vd, vint16m1_t vs1, vuint16m1_t vs2,
                                size_t vl);
vint64m4_t __riscv_nds_vqmaccsu(vint64m4_t vd, int16_t rs1, vuint16m1_t vs2,
                                size_t vl);
vint64m8_t __riscv_nds_vqmaccsu(vint64m8_t vd, vint16m2_t vs1, vuint16m2_t vs2,
                                size_t vl);
vint64m8_t __riscv_nds_vqmaccsu(vint64m8_t vd, int16_t rs1, vuint16m2_t vs2,
                                size_t vl);
vint32mf2_t __riscv_nds_vqmaccus(vint32mf2_t vd, uint8_t rs1, vint8mf8_t vs2,
                                 size_t vl);
vint32m1_t __riscv_nds_vqmaccus(vint32m1_t vd, uint8_t rs1, vint8mf4_t vs2,
                                size_t vl);
vint32m2_t __riscv_nds_vqmaccus(vint32m2_t vd, uint8_t rs1, vint8mf2_t vs2,
                                size_t vl);
vint32m4_t __riscv_nds_vqmaccus(vint32m4_t vd, uint8_t rs1, vint8m1_t vs2,
                                size_t vl);
vint32m8_t __riscv_nds_vqmaccus(vint32m8_t vd, uint8_t rs1, vint8m2_t vs2,
                                size_t vl);
vint64m1_t __riscv_nds_vqmaccus(vint64m1_t vd, uint16_t rs1, vint16mf4_t vs2,
                                size_t vl);
vint64m2_t __riscv_nds_vqmaccus(vint64m2_t vd, uint16_t rs1, vint16mf2_t vs2,
                                size_t vl);
vint64m4_t __riscv_nds_vqmaccus(vint64m4_t vd, uint16_t rs1, vint16m1_t vs2,
                                size_t vl);
vint64m8_t __riscv_nds_vqmaccus(vint64m8_t vd, uint16_t rs1, vint16m2_t vs2,
                                size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu(vuint32mf2_t vd, vuint8mf8_t vs1,
                                 vuint8mf8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu(vuint32mf2_t vd, uint8_t rs1, vuint8mf8_t vs2,
                                 size_t vl);
vuint32m1_t __riscv_nds_vqmaccu(vuint32m1_t vd, vuint8mf4_t vs1,
                                vuint8mf4_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu(vuint32m1_t vd, uint8_t rs1, vuint8mf4_t vs2,
                                size_t vl);
vuint32m2_t __riscv_nds_vqmaccu(vuint32m2_t vd, vuint8mf2_t vs1,
                                vuint8mf2_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu(vuint32m2_t vd, uint8_t rs1, vuint8mf2_t vs2,
                                size_t vl);
vuint32m4_t __riscv_nds_vqmaccu(vuint32m4_t vd, vuint8m1_t vs1, vuint8m1_t vs2,
                                size_t vl);
vuint32m4_t __riscv_nds_vqmaccu(vuint32m4_t vd, uint8_t rs1, vuint8m1_t vs2,
                                size_t vl);
vuint32m8_t __riscv_nds_vqmaccu(vuint32m8_t vd, vuint8m2_t vs1, vuint8m2_t vs2,
                                size_t vl);
vuint32m8_t __riscv_nds_vqmaccu(vuint32m8_t vd, uint8_t rs1, vuint8m2_t vs2,
                                size_t vl);
vuint64m1_t __riscv_nds_vqmaccu(vuint64m1_t vd, vuint16mf4_t vs1,
                                vuint16mf4_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu(vuint64m1_t vd, uint16_t rs1, vuint16mf4_t vs2,
                                size_t vl);
vuint64m2_t __riscv_nds_vqmaccu(vuint64m2_t vd, vuint16mf2_t vs1,
                                vuint16mf2_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu(vuint64m2_t vd, uint16_t rs1, vuint16mf2_t vs2,
                                size_t vl);
vuint64m4_t __riscv_nds_vqmaccu(vuint64m4_t vd, vuint16m1_t vs1,
                                vuint16m1_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu(vuint64m4_t vd, uint16_t rs1, vuint16m1_t vs2,
                                size_t vl);
vuint64m8_t __riscv_nds_vqmaccu(vuint64m8_t vd, vuint16m2_t vs1,
                                vuint16m2_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu(vuint64m8_t vd, uint16_t rs1, vuint16m2_t vs2,
                                size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vqmacc(vbool64_t vm, vint32mf2_t vd, vint8mf8_t vs1,
                               vint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmacc(vbool64_t vm, vint32mf2_t vd, int8_t rs1,
                               vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc(vbool32_t vm, vint32m1_t vd, vint8mf4_t vs1,
                              vint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc(vbool32_t vm, vint32m1_t vd, int8_t rs1,
                              vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc(vbool16_t vm, vint32m2_t vd, vint8mf2_t vs1,
                              vint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc(vbool16_t vm, vint32m2_t vd, int8_t rs1,
                              vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc(vbool8_t vm, vint32m4_t vd, vint8m1_t vs1,
                              vint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                              vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc(vbool4_t vm, vint32m8_t vd, vint8m2_t vs1,
                              vint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                              vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc(vbool64_t vm, vint64m1_t vd, vint16mf4_t vs1,
                              vint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc(vbool64_t vm, vint64m1_t vd, int16_t rs1,
                              vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc(vbool32_t vm, vint64m2_t vd, vint16mf2_t vs1,
                              vint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc(vbool32_t vm, vint64m2_t vd, int16_t rs1,
                              vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc(vbool16_t vm, vint64m4_t vd, vint16m1_t vs1,
                              vint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc(vbool16_t vm, vint64m4_t vd, int16_t rs1,
                              vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc(vbool8_t vm, vint64m8_t vd, vint16m2_t vs1,
                              vint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc(vbool8_t vm, vint64m8_t vd, int16_t rs1,
                              vint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu(vbool64_t vm, vint32mf2_t vd, vint8mf8_t vs1,
                                 vuint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu(vbool64_t vm, vint32mf2_t vd, int8_t rs1,
                                 vuint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu(vbool32_t vm, vint32m1_t vd, vint8mf4_t vs1,
                                vuint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu(vbool32_t vm, vint32m1_t vd, int8_t rs1,
                                vuint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu(vbool16_t vm, vint32m2_t vd, vint8mf2_t vs1,
                                vuint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu(vbool16_t vm, vint32m2_t vd, int8_t rs1,
                                vuint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu(vbool8_t vm, vint32m4_t vd, vint8m1_t vs1,
                                vuint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                                vuint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu(vbool4_t vm, vint32m8_t vd, vint8m2_t vs1,
                                vuint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                                vuint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu(vbool64_t vm, vint64m1_t vd, vint16mf4_t vs1,
                                vuint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu(vbool64_t vm, vint64m1_t vd, int16_t rs1,
                                vuint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu(vbool32_t vm, vint64m2_t vd, vint16mf2_t vs1,
                                vuint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu(vbool32_t vm, vint64m2_t vd, int16_t rs1,
                                vuint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu(vbool16_t vm, vint64m4_t vd, vint16m1_t vs1,
                                vuint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu(vbool16_t vm, vint64m4_t vd, int16_t rs1,
                                vuint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu(vbool8_t vm, vint64m8_t vd, vint16m2_t vs1,
                                vuint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu(vbool8_t vm, vint64m8_t vd, int16_t rs1,
                                vuint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccus(vbool64_t vm, vint32mf2_t vd, uint8_t rs1,
                                 vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccus(vbool32_t vm, vint32m1_t vd, uint8_t rs1,
                                vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccus(vbool16_t vm, vint32m2_t vd, uint8_t rs1,
                                vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccus(vbool8_t vm, vint32m4_t vd, uint8_t rs1,
                                vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccus(vbool4_t vm, vint32m8_t vd, uint8_t rs1,
                                vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccus(vbool64_t vm, vint64m1_t vd, uint16_t rs1,
                                vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccus(vbool32_t vm, vint64m2_t vd, uint16_t rs1,
                                vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccus(vbool16_t vm, vint64m4_t vd, uint16_t rs1,
                                vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccus(vbool8_t vm, vint64m8_t vd, uint16_t rs1,
                                vint16m2_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu(vbool64_t vm, vuint32mf2_t vd, vuint8mf8_t vs1,
                                 vuint8mf8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu(vbool64_t vm, vuint32mf2_t vd, uint8_t rs1,
                                 vuint8mf8_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu(vbool32_t vm, vuint32m1_t vd, vuint8mf4_t vs1,
                                vuint8mf4_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu(vbool32_t vm, vuint32m1_t vd, uint8_t rs1,
                                vuint8mf4_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu(vbool16_t vm, vuint32m2_t vd, vuint8mf2_t vs1,
                                vuint8mf2_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu(vbool16_t vm, vuint32m2_t vd, uint8_t rs1,
                                vuint8mf2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu(vbool8_t vm, vuint32m4_t vd, vuint8m1_t vs1,
                                vuint8m1_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu(vbool8_t vm, vuint32m4_t vd, uint8_t rs1,
                                vuint8m1_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu(vbool4_t vm, vuint32m8_t vd, vuint8m2_t vs1,
                                vuint8m2_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu(vbool4_t vm, vuint32m8_t vd, uint8_t rs1,
                                vuint8m2_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu(vbool64_t vm, vuint64m1_t vd, vuint16mf4_t vs1,
                                vuint16mf4_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu(vbool64_t vm, vuint64m1_t vd, uint16_t rs1,
                                vuint16mf4_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu(vbool32_t vm, vuint64m2_t vd, vuint16mf2_t vs1,
                                vuint16mf2_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu(vbool32_t vm, vuint64m2_t vd, uint16_t rs1,
                                vuint16mf2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu(vbool16_t vm, vuint64m4_t vd, vuint16m1_t vs1,
                                vuint16m1_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu(vbool16_t vm, vuint64m4_t vd, uint16_t rs1,
                                vuint16m1_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu(vbool8_t vm, vuint64m8_t vd, vuint16m2_t vs1,
                                vuint16m2_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu(vbool8_t vm, vuint64m8_t vd, uint16_t rs1,
                                vuint16m2_t vs2, size_t vl);
----

=== Andes Vector Dot Product Extension(XAndesVDot)

[[overloaded-vector-dot-product-operations]]
==== Vector Dot Product Intrinsics(XAndesVDot)

[,c]
----
vint32mf2_t __riscv_nds_vd4dots(vint32mf2_t vd, vint8mf2_t vs1, vint8mf2_t vs2,
                                size_t vl);
vint32m1_t __riscv_nds_vd4dots(vint32m1_t vd, vint8m1_t vs1, vint8m1_t vs2,
                               size_t vl);
vint32m2_t __riscv_nds_vd4dots(vint32m2_t vd, vint8m2_t vs1, vint8m2_t vs2,
                               size_t vl);
vint32m4_t __riscv_nds_vd4dots(vint32m4_t vd, vint8m4_t vs1, vint8m4_t vs2,
                               size_t vl);
vint32m8_t __riscv_nds_vd4dots(vint32m8_t vd, vint8m8_t vs1, vint8m8_t vs2,
                               size_t vl);
vint64m1_t __riscv_nds_vd4dots(vint64m1_t vd, vint16m1_t vs1, vint16m1_t vs2,
                               size_t vl);
vint64m2_t __riscv_nds_vd4dots(vint64m2_t vd, vint16m2_t vs1, vint16m2_t vs2,
                               size_t vl);
vint64m4_t __riscv_nds_vd4dots(vint64m4_t vd, vint16m4_t vs1, vint16m4_t vs2,
                               size_t vl);
vint64m8_t __riscv_nds_vd4dots(vint64m8_t vd, vint16m8_t vs1, vint16m8_t vs2,
                               size_t vl);
vint32mf2_t __riscv_nds_vd4dotsu(vint32mf2_t vd, vint8mf2_t vs1,
                                 vuint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dotsu(vint32m1_t vd, vint8m1_t vs1, vuint8m1_t vs2,
                                size_t vl);
vint32m2_t __riscv_nds_vd4dotsu(vint32m2_t vd, vint8m2_t vs1, vuint8m2_t vs2,
                                size_t vl);
vint32m4_t __riscv_nds_vd4dotsu(vint32m4_t vd, vint8m4_t vs1, vuint8m4_t vs2,
                                size_t vl);
vint32m8_t __riscv_nds_vd4dotsu(vint32m8_t vd, vint8m8_t vs1, vuint8m8_t vs2,
                                size_t vl);
vint64m1_t __riscv_nds_vd4dotsu(vint64m1_t vd, vint16m1_t vs1, vuint16m1_t vs2,
                                size_t vl);
vint64m2_t __riscv_nds_vd4dotsu(vint64m2_t vd, vint16m2_t vs1, vuint16m2_t vs2,
                                size_t vl);
vint64m4_t __riscv_nds_vd4dotsu(vint64m4_t vd, vint16m4_t vs1, vuint16m4_t vs2,
                                size_t vl);
vint64m8_t __riscv_nds_vd4dotsu(vint64m8_t vd, vint16m8_t vs1, vuint16m8_t vs2,
                                size_t vl);
vuint32mf2_t __riscv_nds_vd4dotu(vuint32mf2_t vd, vuint8mf2_t vs1,
                                 vuint8mf2_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vd4dotu(vuint32m1_t vd, vuint8m1_t vs1, vuint8m1_t vs2,
                                size_t vl);
vuint32m2_t __riscv_nds_vd4dotu(vuint32m2_t vd, vuint8m2_t vs1, vuint8m2_t vs2,
                                size_t vl);
vuint32m4_t __riscv_nds_vd4dotu(vuint32m4_t vd, vuint8m4_t vs1, vuint8m4_t vs2,
                                size_t vl);
vuint32m8_t __riscv_nds_vd4dotu(vuint32m8_t vd, vuint8m8_t vs1, vuint8m8_t vs2,
                                size_t vl);
vuint64m1_t __riscv_nds_vd4dotu(vuint64m1_t vd, vuint16m1_t vs1,
                                vuint16m1_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vd4dotu(vuint64m2_t vd, vuint16m2_t vs1,
                                vuint16m2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vd4dotu(vuint64m4_t vd, vuint16m4_t vs1,
                                vuint16m4_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vd4dotu(vuint64m8_t vd, vuint16m8_t vs1,
                                vuint16m8_t vs2, size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vd4dots(vbool64_t vm, vint32mf2_t vd, vint8mf2_t vs1,
                                vint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dots(vbool32_t vm, vint32m1_t vd, vint8m1_t vs1,
                               vint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dots(vbool16_t vm, vint32m2_t vd, vint8m2_t vs1,
                               vint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dots(vbool8_t vm, vint32m4_t vd, vint8m4_t vs1,
                               vint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dots(vbool4_t vm, vint32m8_t vd, vint8m8_t vs1,
                               vint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dots(vbool64_t vm, vint64m1_t vd, vint16m1_t vs1,
                               vint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dots(vbool32_t vm, vint64m2_t vd, vint16m2_t vs1,
                               vint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dots(vbool16_t vm, vint64m4_t vd, vint16m4_t vs1,
                               vint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dots(vbool8_t vm, vint64m8_t vd, vint16m8_t vs1,
                               vint16m8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vd4dotsu(vbool64_t vm, vint32mf2_t vd, vint8mf2_t vs1,
                                 vuint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dotsu(vbool32_t vm, vint32m1_t vd, vint8m1_t vs1,
                                vuint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dotsu(vbool16_t vm, vint32m2_t vd, vint8m2_t vs1,
                                vuint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dotsu(vbool8_t vm, vint32m4_t vd, vint8m4_t vs1,
                                vuint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dotsu(vbool4_t vm, vint32m8_t vd, vint8m8_t vs1,
                                vuint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dotsu(vbool64_t vm, vint64m1_t vd, vint16m1_t vs1,
                                vuint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dotsu(vbool32_t vm, vint64m2_t vd, vint16m2_t vs1,
                                vuint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dotsu(vbool16_t vm, vint64m4_t vd, vint16m4_t vs1,
                                vuint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dotsu(vbool8_t vm, vint64m8_t vd, vint16m8_t vs1,
                                vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vd4dotu(vbool64_t vm, vuint32mf2_t vd, vuint8mf2_t vs1,
                                 vuint8mf2_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vd4dotu(vbool32_t vm, vuint32m1_t vd, vuint8m1_t vs1,
                                vuint8m1_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vd4dotu(vbool16_t vm, vuint32m2_t vd, vuint8m2_t vs1,
                                vuint8m2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vd4dotu(vbool8_t vm, vuint32m4_t vd, vuint8m4_t vs1,
                                vuint8m4_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vd4dotu(vbool4_t vm, vuint32m8_t vd, vuint8m8_t vs1,
                                vuint8m8_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vd4dotu(vbool64_t vm, vuint64m1_t vd, vuint16m1_t vs1,
                                vuint16m1_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vd4dotu(vbool32_t vm, vuint64m2_t vd, vuint16m2_t vs1,
                                vuint16m2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vd4dotu(vbool16_t vm, vuint64m4_t vd, vuint16m4_t vs1,
                                vuint16m4_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vd4dotu(vbool8_t vm, vuint64m8_t vd, vuint16m8_t vs1,
                                vuint16m8_t vs2, size_t vl);
----

=== Andes Vector Packed FP16 Extension(XAndesVPackFPH)

[[overloaded-vector-packed-fp16-operations]]
==== Vector Packed FP16 Intrinsics(XAndesVPackFPH)

[,c]
----
vfloat16mf4_t __riscv_nds_vfpmadt(vfloat16mf4_t vs2, float32_t rs1, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt(vfloat16mf2_t vs2, float32_t rs1, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt(vfloat16m1_t vs2, float32_t rs1, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt(vfloat16m2_t vs2, float32_t rs1, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt(vfloat16m4_t vs2, float32_t rs1, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt(vfloat16m8_t vs2, float32_t rs1, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb(vfloat16mf4_t vs2, float32_t rs1, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb(vfloat16mf2_t vs2, float32_t rs1, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb(vfloat16m1_t vs2, float32_t rs1, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb(vfloat16m2_t vs2, float32_t rs1, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb(vfloat16m4_t vs2, float32_t rs1, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb(vfloat16m8_t vs2, float32_t rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt(vbool64_t vm, vfloat16mf4_t vs2,
                                  float32_t rs1, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt(vbool32_t vm, vfloat16mf2_t vs2,
                                  float32_t rs1, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt(vbool16_t vm, vfloat16m1_t vs2, float32_t rs1,
                                 size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt(vbool8_t vm, vfloat16m2_t vs2, float32_t rs1,
                                 size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt(vbool4_t vm, vfloat16m4_t vs2, float32_t rs1,
                                 size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt(vbool2_t vm, vfloat16m8_t vs2, float32_t rs1,
                                 size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb(vbool64_t vm, vfloat16mf4_t vs2,
                                  float32_t rs1, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb(vbool32_t vm, vfloat16mf2_t vs2,
                                  float32_t rs1, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb(vbool16_t vm, vfloat16m1_t vs2, float32_t rs1,
                                 size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb(vbool8_t vm, vfloat16m2_t vs2, float32_t rs1,
                                 size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb(vbool4_t vm, vfloat16m4_t vs2, float32_t rs1,
                                 size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb(vbool2_t vm, vfloat16m8_t vs2, float32_t rs1,
                                 size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadt(vfloat16mf4_t vs2, float32_t rs1,
                                  unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt(vfloat16mf2_t vs2, float32_t rs1,
                                  unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt(vfloat16m1_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt(vfloat16m2_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt(vfloat16m4_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt(vfloat16m8_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb(vfloat16mf4_t vs2, float32_t rs1,
                                  unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb(vfloat16mf2_t vs2, float32_t rs1,
                                  unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb(vfloat16m1_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb(vfloat16m2_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb(vfloat16m4_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb(vfloat16m8_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt(vbool64_t vm, vfloat16mf4_t vs2,
                                  float32_t rs1, unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt(vbool32_t vm, vfloat16mf2_t vs2,
                                  float32_t rs1, unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt(vbool16_t vm, vfloat16m1_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt(vbool8_t vm, vfloat16m2_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt(vbool4_t vm, vfloat16m4_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt(vbool2_t vm, vfloat16m8_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb(vbool64_t vm, vfloat16mf4_t vs2,
                                  float32_t rs1, unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb(vbool32_t vm, vfloat16mf2_t vs2,
                                  float32_t rs1, unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb(vbool16_t vm, vfloat16m1_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb(vbool8_t vm, vfloat16m2_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb(vbool4_t vm, vfloat16m4_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb(vbool2_t vm, vfloat16m8_t vs2, float32_t rs1,
                                 unsigned int frm, size_t vl);
----

=== Andes Vector INT4 Load Extension(XAndesVSIntLoad)

[[overloaded-vector-int4-load-operations]]
==== Andes Vector INT4 Load Intrinsics(XAndesVSIntLoad)

[,c]
----
// masked functions
vint8mf8_t __riscv_nds_vln8(vbool64_t vm, const void *rs1, size_t vl);
vint8mf4_t __riscv_nds_vln8(vbool32_t vm, const void *rs1, size_t vl);
vint8mf2_t __riscv_nds_vln8(vbool16_t vm, const void *rs1, size_t vl);
vint8m1_t __riscv_nds_vln8(vbool8_t vm, const void *rs1, size_t vl);
vint8m2_t __riscv_nds_vln8(vbool4_t vm, const void *rs1, size_t vl);
vint8m4_t __riscv_nds_vln8(vbool2_t vm, const void *rs1, size_t vl);
vint8m8_t __riscv_nds_vln8(vbool1_t vm, const void *rs1, size_t vl);
vuint8mf8_t __riscv_nds_vlnu8(vbool64_t vm, const void *rs1, size_t vl);
vuint8mf4_t __riscv_nds_vlnu8(vbool32_t vm, const void *rs1, size_t vl);
vuint8mf2_t __riscv_nds_vlnu8(vbool16_t vm, const void *rs1, size_t vl);
vuint8m1_t __riscv_nds_vlnu8(vbool8_t vm, const void *rs1, size_t vl);
vuint8m2_t __riscv_nds_vlnu8(vbool4_t vm, const void *rs1, size_t vl);
vuint8m4_t __riscv_nds_vlnu8(vbool2_t vm, const void *rs1, size_t vl);
vuint8m8_t __riscv_nds_vlnu8(vbool1_t vm, const void *rs1, size_t vl);
----
