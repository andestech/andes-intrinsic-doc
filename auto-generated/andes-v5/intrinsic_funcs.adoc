
=== Andes Vector BFLOAT16 Conversion Extension(XAndesVBFHCvt)

[[bf16-vector-unit-stride-load]]
==== Vector Unit-Stride Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vle16_v_bf16mf4(const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16_v_bf16mf2(const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16_v_bf16m1(const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_v_bf16m2(const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16_v_bf16m4(const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16_v_bf16m8(const __bf16 *rs1, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                         size_t vl);
vbfloat16mf2_t __riscv_vle16_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                         size_t vl);
vbfloat16m1_t __riscv_vle16_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                       size_t vl);
vbfloat16m2_t __riscv_vle16_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                       size_t vl);
vbfloat16m4_t __riscv_vle16_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                       size_t vl);
vbfloat16m8_t __riscv_vle16_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                       size_t vl);
----

[[bf16-vector-unit-stride-store]]
==== Vector Unit-Stride Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vse16_v_bf16mf4(__bf16 *rs1, vbfloat16mf4_t vs3, size_t vl);
void __riscv_vse16_v_bf16mf2(__bf16 *rs1, vbfloat16mf2_t vs3, size_t vl);
void __riscv_vse16_v_bf16m1(__bf16 *rs1, vbfloat16m1_t vs3, size_t vl);
void __riscv_vse16_v_bf16m2(__bf16 *rs1, vbfloat16m2_t vs3, size_t vl);
void __riscv_vse16_v_bf16m4(__bf16 *rs1, vbfloat16m4_t vs3, size_t vl);
void __riscv_vse16_v_bf16m8(__bf16 *rs1, vbfloat16m8_t vs3, size_t vl);
// masked functions
void __riscv_vse16_v_bf16mf4_m(vbool64_t vm, __bf16 *rs1, vbfloat16mf4_t vs3,
                               size_t vl);
void __riscv_vse16_v_bf16mf2_m(vbool32_t vm, __bf16 *rs1, vbfloat16mf2_t vs3,
                               size_t vl);
void __riscv_vse16_v_bf16m1_m(vbool16_t vm, __bf16 *rs1, vbfloat16m1_t vs3,
                              size_t vl);
void __riscv_vse16_v_bf16m2_m(vbool8_t vm, __bf16 *rs1, vbfloat16m2_t vs3,
                              size_t vl);
void __riscv_vse16_v_bf16m4_m(vbool4_t vm, __bf16 *rs1, vbfloat16m4_t vs3,
                              size_t vl);
void __riscv_vse16_v_bf16m8_m(vbool2_t vm, __bf16 *rs1, vbfloat16m8_t vs3,
                              size_t vl);
----

[[vector-strided-load]]
==== Vector Strided Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vlse16_v_bf16mf4(const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2_t __riscv_vlse16_v_bf16mf2(const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1_t __riscv_vlse16_v_bf16m1(const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m2_t __riscv_vlse16_v_bf16m2(const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m4_t __riscv_vlse16_v_bf16m4(const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m8_t __riscv_vlse16_v_bf16m8(const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vlse16_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                          ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                          ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                        ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                        ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                        ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                        ptrdiff_t rs2, size_t vl);
----

[[vector-strided-store]]
==== Vector Strided Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vsse16_v_bf16mf4(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf4_t vs3,
                              size_t vl);
void __riscv_vsse16_v_bf16mf2(__bf16 *rs1, ptrdiff_t rs2, vbfloat16mf2_t vs3,
                              size_t vl);
void __riscv_vsse16_v_bf16m1(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m1_t vs3,
                             size_t vl);
void __riscv_vsse16_v_bf16m2(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m2_t vs3,
                             size_t vl);
void __riscv_vsse16_v_bf16m4(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m4_t vs3,
                             size_t vl);
void __riscv_vsse16_v_bf16m8(__bf16 *rs1, ptrdiff_t rs2, vbfloat16m8_t vs3,
                             size_t vl);
// masked functions
void __riscv_vsse16_v_bf16mf4_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsse16_v_bf16mf2_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsse16_v_bf16m1_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                               vbfloat16m1_t vs3, size_t vl);
void __riscv_vsse16_v_bf16m2_m(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2,
                               vbfloat16m2_t vs3, size_t vl);
void __riscv_vsse16_v_bf16m4_m(vbool4_t vm, __bf16 *rs1, ptrdiff_t rs2,
                               vbfloat16m4_t vs3, size_t vl);
void __riscv_vsse16_v_bf16m8_m(vbool2_t vm, __bf16 *rs1, ptrdiff_t rs2,
                               vbfloat16m8_t vs3, size_t vl);
----

[[vector-indexed-load]]
==== Vector Indexed Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vloxei16_v_bf16mf4(const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf2_t __riscv_vloxei16_v_bf16mf2(const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16m1_t __riscv_vloxei16_v_bf16m1(const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m2_t __riscv_vloxei16_v_bf16m2(const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m4_t __riscv_vloxei16_v_bf16m4(const __bf16 *rs1, vuint16m4_t rs2,
                                        size_t vl);
vbfloat16m8_t __riscv_vloxei16_v_bf16m8(const __bf16 *rs1, vuint16m8_t rs2,
                                        size_t vl);
vbfloat16mf4_t __riscv_vluxei16_v_bf16mf4(const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf2_t __riscv_vluxei16_v_bf16mf2(const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16m1_t __riscv_vluxei16_v_bf16m1(const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m2_t __riscv_vluxei16_v_bf16m2(const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m4_t __riscv_vluxei16_v_bf16m4(const __bf16 *rs1, vuint16m4_t rs2,
                                        size_t vl);
vbfloat16m8_t __riscv_vluxei16_v_bf16m8(const __bf16 *rs1, vuint16m8_t rs2,
                                        size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                            vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vloxei16_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                            vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vloxei16_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                          vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vloxei16_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                          vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vloxei16_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                          vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vloxei16_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                          vuint16m8_t rs2, size_t vl);
vbfloat16mf4_t __riscv_vluxei16_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                            vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vluxei16_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                            vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vluxei16_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                          vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vluxei16_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                          vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vluxei16_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                          vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vluxei16_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                          vuint16m8_t rs2, size_t vl);
----

[[vector-indexed-store]]
==== Vector Indexed Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vsoxei16_v_bf16mf4(__bf16 *rs1, vuint16mf4_t rs2,
                                vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16mf2(__bf16 *rs1, vuint16mf2_t rs2,
                                vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m1(__bf16 *rs1, vuint16m1_t rs2, vbfloat16m1_t vs3,
                               size_t vl);
void __riscv_vsoxei16_v_bf16m2(__bf16 *rs1, vuint16m2_t rs2, vbfloat16m2_t vs3,
                               size_t vl);
void __riscv_vsoxei16_v_bf16m4(__bf16 *rs1, vuint16m4_t rs2, vbfloat16m4_t vs3,
                               size_t vl);
void __riscv_vsoxei16_v_bf16m8(__bf16 *rs1, vuint16m8_t rs2, vbfloat16m8_t vs3,
                               size_t vl);
void __riscv_vsuxei16_v_bf16mf4(__bf16 *rs1, vuint16mf4_t rs2,
                                vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16mf2(__bf16 *rs1, vuint16mf2_t rs2,
                                vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m1(__bf16 *rs1, vuint16m1_t rs2, vbfloat16m1_t vs3,
                               size_t vl);
void __riscv_vsuxei16_v_bf16m2(__bf16 *rs1, vuint16m2_t rs2, vbfloat16m2_t vs3,
                               size_t vl);
void __riscv_vsuxei16_v_bf16m4(__bf16 *rs1, vuint16m4_t rs2, vbfloat16m4_t vs3,
                               size_t vl);
void __riscv_vsuxei16_v_bf16m8(__bf16 *rs1, vuint16m8_t rs2, vbfloat16m8_t vs3,
                               size_t vl);
// masked functions
void __riscv_vsoxei16_v_bf16mf4_m(vbool64_t vm, __bf16 *rs1, vuint16mf4_t rs2,
                                  vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16mf2_m(vbool32_t vm, __bf16 *rs1, vuint16mf2_t rs2,
                                  vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m1_m(vbool16_t vm, __bf16 *rs1, vuint16m1_t rs2,
                                 vbfloat16m1_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m2_m(vbool8_t vm, __bf16 *rs1, vuint16m2_t rs2,
                                 vbfloat16m2_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m4_m(vbool4_t vm, __bf16 *rs1, vuint16m4_t rs2,
                                 vbfloat16m4_t vs3, size_t vl);
void __riscv_vsoxei16_v_bf16m8_m(vbool2_t vm, __bf16 *rs1, vuint16m8_t rs2,
                                 vbfloat16m8_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16mf4_m(vbool64_t vm, __bf16 *rs1, vuint16mf4_t rs2,
                                  vbfloat16mf4_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16mf2_m(vbool32_t vm, __bf16 *rs1, vuint16mf2_t rs2,
                                  vbfloat16mf2_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m1_m(vbool16_t vm, __bf16 *rs1, vuint16m1_t rs2,
                                 vbfloat16m1_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m2_m(vbool8_t vm, __bf16 *rs1, vuint16m2_t rs2,
                                 vbfloat16m2_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m4_m(vbool4_t vm, __bf16 *rs1, vuint16m4_t rs2,
                                 vbfloat16m4_t vs3, size_t vl);
void __riscv_vsuxei16_v_bf16m8_m(vbool2_t vm, __bf16 *rs1, vuint16m8_t rs2,
                                 vbfloat16m8_t vs3, size_t vl);
----

[[unit-stride-fault-only-first-loads]]
==== Unit-stride Fault-Only-First Loads Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vle16ff_v_bf16mf4(const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2_t __riscv_vle16ff_v_bf16mf2(const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1_t __riscv_vle16ff_v_bf16m1(const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m2_t __riscv_vle16ff_v_bf16m2(const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m4_t __riscv_vle16ff_v_bf16m4(const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m8_t __riscv_vle16ff_v_bf16m8(const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16ff_v_bf16mf4_m(vbool64_t vm, const __bf16 *rs1,
                                           size_t *new_vl, size_t vl);
vbfloat16mf2_t __riscv_vle16ff_v_bf16mf2_m(vbool32_t vm, const __bf16 *rs1,
                                           size_t *new_vl, size_t vl);
vbfloat16m1_t __riscv_vle16ff_v_bf16m1_m(vbool16_t vm, const __bf16 *rs1,
                                         size_t *new_vl, size_t vl);
vbfloat16m2_t __riscv_vle16ff_v_bf16m2_m(vbool8_t vm, const __bf16 *rs1,
                                         size_t *new_vl, size_t vl);
vbfloat16m4_t __riscv_vle16ff_v_bf16m4_m(vbool4_t vm, const __bf16 *rs1,
                                         size_t *new_vl, size_t vl);
vbfloat16m8_t __riscv_vle16ff_v_bf16m8_m(vbool2_t vm, const __bf16 *rs1,
                                         size_t *new_vl, size_t vl);
----

[[vector-unit-stride-segment-load]]
==== Vector Unit-Stride Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4x2_t __riscv_vlseg2e16_v_bf16mf4x2(const __bf16 *rs1, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16_v_bf16mf4x3(const __bf16 *rs1, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16_v_bf16mf4x4(const __bf16 *rs1, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16_v_bf16mf4x5(const __bf16 *rs1, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16_v_bf16mf4x6(const __bf16 *rs1, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16_v_bf16mf4x7(const __bf16 *rs1, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16_v_bf16mf4x8(const __bf16 *rs1, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16_v_bf16mf2x2(const __bf16 *rs1, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16_v_bf16mf2x3(const __bf16 *rs1, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16_v_bf16mf2x4(const __bf16 *rs1, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16_v_bf16mf2x5(const __bf16 *rs1, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16_v_bf16mf2x6(const __bf16 *rs1, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16_v_bf16mf2x7(const __bf16 *rs1, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16_v_bf16mf2x8(const __bf16 *rs1, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16_v_bf16m1x2(const __bf16 *rs1, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16_v_bf16m1x3(const __bf16 *rs1, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16_v_bf16m1x4(const __bf16 *rs1, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16_v_bf16m1x5(const __bf16 *rs1, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16_v_bf16m1x6(const __bf16 *rs1, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16_v_bf16m1x7(const __bf16 *rs1, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16_v_bf16m1x8(const __bf16 *rs1, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16_v_bf16m2x2(const __bf16 *rs1, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16_v_bf16m2x3(const __bf16 *rs1, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16_v_bf16m2x4(const __bf16 *rs1, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16_v_bf16m4x2(const __bf16 *rs1, size_t vl);
vbfloat16mf4x2_t __riscv_vlseg2e16ff_v_bf16mf4x2(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16ff_v_bf16mf4x3(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16ff_v_bf16mf4x4(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16ff_v_bf16mf4x5(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16ff_v_bf16mf4x6(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16ff_v_bf16mf4x7(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16ff_v_bf16mf4x8(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16ff_v_bf16mf2x2(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16ff_v_bf16mf2x3(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16ff_v_bf16mf2x4(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16ff_v_bf16mf2x5(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16ff_v_bf16mf2x6(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16ff_v_bf16mf2x7(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16ff_v_bf16mf2x8(const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16ff_v_bf16m1x2(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16ff_v_bf16m1x3(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16ff_v_bf16m1x4(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16ff_v_bf16m1x5(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16ff_v_bf16m1x6(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16ff_v_bf16m1x7(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16ff_v_bf16m1x8(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16ff_v_bf16m2x2(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16ff_v_bf16m2x3(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16ff_v_bf16m2x4(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16ff_v_bf16m4x2(const __bf16 *rs1,
                                               size_t *new_vl, size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vlseg2e16_v_bf16mf4x2_m(vbool64_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16_v_bf16mf4x3_m(vbool64_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16_v_bf16mf4x4_m(vbool64_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16_v_bf16mf4x5_m(vbool64_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16_v_bf16mf4x6_m(vbool64_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16_v_bf16mf4x7_m(vbool64_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16_v_bf16mf4x8_m(vbool64_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16_v_bf16mf2x2_m(vbool32_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16_v_bf16mf2x3_m(vbool32_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16_v_bf16mf2x4_m(vbool32_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16_v_bf16mf2x5_m(vbool32_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16_v_bf16mf2x6_m(vbool32_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16_v_bf16mf2x7_m(vbool32_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16_v_bf16mf2x8_m(vbool32_t vm,
                                                 const __bf16 *rs1, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16_v_bf16m1x2_m(vbool16_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16_v_bf16m1x3_m(vbool16_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16_v_bf16m1x4_m(vbool16_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16_v_bf16m1x5_m(vbool16_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16_v_bf16m1x6_m(vbool16_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16_v_bf16m1x7_m(vbool16_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16_v_bf16m1x8_m(vbool16_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16_v_bf16m2x2_m(vbool8_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16_v_bf16m2x3_m(vbool8_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16_v_bf16m2x4_m(vbool8_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16_v_bf16m4x2_m(vbool4_t vm, const __bf16 *rs1,
                                               size_t vl);
vbfloat16mf4x2_t __riscv_vlseg2e16ff_v_bf16mf4x2_m(vbool64_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16ff_v_bf16mf4x3_m(vbool64_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16ff_v_bf16mf4x4_m(vbool64_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16ff_v_bf16mf4x5_m(vbool64_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16ff_v_bf16mf4x6_m(vbool64_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16ff_v_bf16mf4x7_m(vbool64_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16ff_v_bf16mf4x8_m(vbool64_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16ff_v_bf16mf2x2_m(vbool32_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16ff_v_bf16mf2x3_m(vbool32_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16ff_v_bf16mf2x4_m(vbool32_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16ff_v_bf16mf2x5_m(vbool32_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16ff_v_bf16mf2x6_m(vbool32_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16ff_v_bf16mf2x7_m(vbool32_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16ff_v_bf16mf2x8_m(vbool32_t vm,
                                                   const __bf16 *rs1,
                                                   size_t *new_vl, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16ff_v_bf16m1x2_m(vbool16_t vm,
                                                 const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16ff_v_bf16m1x3_m(vbool16_t vm,
                                                 const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16ff_v_bf16m1x4_m(vbool16_t vm,
                                                 const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16ff_v_bf16m1x5_m(vbool16_t vm,
                                                 const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16ff_v_bf16m1x6_m(vbool16_t vm,
                                                 const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16ff_v_bf16m1x7_m(vbool16_t vm,
                                                 const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16ff_v_bf16m1x8_m(vbool16_t vm,
                                                 const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16ff_v_bf16m2x2_m(vbool8_t vm, const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16ff_v_bf16m2x3_m(vbool8_t vm, const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16ff_v_bf16m2x4_m(vbool8_t vm, const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16ff_v_bf16m4x2_m(vbool4_t vm, const __bf16 *rs1,
                                                 size_t *new_vl, size_t vl);
----

[[vecrtor-unit-stride-segment-store]]
==== Vector Unit-Stride Segment Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vsseg2e16_v_bf16mf4x2(__bf16 *rs1, vbfloat16mf4x2_t vs3,
                                   size_t vl);
void __riscv_vsseg3e16_v_bf16mf4x3(__bf16 *rs1, vbfloat16mf4x3_t vs3,
                                   size_t vl);
void __riscv_vsseg4e16_v_bf16mf4x4(__bf16 *rs1, vbfloat16mf4x4_t vs3,
                                   size_t vl);
void __riscv_vsseg5e16_v_bf16mf4x5(__bf16 *rs1, vbfloat16mf4x5_t vs3,
                                   size_t vl);
void __riscv_vsseg6e16_v_bf16mf4x6(__bf16 *rs1, vbfloat16mf4x6_t vs3,
                                   size_t vl);
void __riscv_vsseg7e16_v_bf16mf4x7(__bf16 *rs1, vbfloat16mf4x7_t vs3,
                                   size_t vl);
void __riscv_vsseg8e16_v_bf16mf4x8(__bf16 *rs1, vbfloat16mf4x8_t vs3,
                                   size_t vl);
void __riscv_vsseg2e16_v_bf16mf2x2(__bf16 *rs1, vbfloat16mf2x2_t vs3,
                                   size_t vl);
void __riscv_vsseg3e16_v_bf16mf2x3(__bf16 *rs1, vbfloat16mf2x3_t vs3,
                                   size_t vl);
void __riscv_vsseg4e16_v_bf16mf2x4(__bf16 *rs1, vbfloat16mf2x4_t vs3,
                                   size_t vl);
void __riscv_vsseg5e16_v_bf16mf2x5(__bf16 *rs1, vbfloat16mf2x5_t vs3,
                                   size_t vl);
void __riscv_vsseg6e16_v_bf16mf2x6(__bf16 *rs1, vbfloat16mf2x6_t vs3,
                                   size_t vl);
void __riscv_vsseg7e16_v_bf16mf2x7(__bf16 *rs1, vbfloat16mf2x7_t vs3,
                                   size_t vl);
void __riscv_vsseg8e16_v_bf16mf2x8(__bf16 *rs1, vbfloat16mf2x8_t vs3,
                                   size_t vl);
void __riscv_vsseg2e16_v_bf16m1x2(__bf16 *rs1, vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vsseg3e16_v_bf16m1x3(__bf16 *rs1, vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vsseg4e16_v_bf16m1x4(__bf16 *rs1, vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vsseg5e16_v_bf16m1x5(__bf16 *rs1, vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vsseg6e16_v_bf16m1x6(__bf16 *rs1, vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vsseg7e16_v_bf16m1x7(__bf16 *rs1, vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vsseg8e16_v_bf16m1x8(__bf16 *rs1, vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vsseg2e16_v_bf16m2x2(__bf16 *rs1, vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vsseg3e16_v_bf16m2x3(__bf16 *rs1, vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vsseg4e16_v_bf16m2x4(__bf16 *rs1, vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vsseg2e16_v_bf16m4x2(__bf16 *rs1, vbfloat16m4x2_t vs3, size_t vl);
// masked functions
void __riscv_vsseg2e16_v_bf16mf4x2_m(vbool64_t vm, __bf16 *rs1,
                                     vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vsseg3e16_v_bf16mf4x3_m(vbool64_t vm, __bf16 *rs1,
                                     vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vsseg4e16_v_bf16mf4x4_m(vbool64_t vm, __bf16 *rs1,
                                     vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vsseg5e16_v_bf16mf4x5_m(vbool64_t vm, __bf16 *rs1,
                                     vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vsseg6e16_v_bf16mf4x6_m(vbool64_t vm, __bf16 *rs1,
                                     vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vsseg7e16_v_bf16mf4x7_m(vbool64_t vm, __bf16 *rs1,
                                     vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vsseg8e16_v_bf16mf4x8_m(vbool64_t vm, __bf16 *rs1,
                                     vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vsseg2e16_v_bf16mf2x2_m(vbool32_t vm, __bf16 *rs1,
                                     vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vsseg3e16_v_bf16mf2x3_m(vbool32_t vm, __bf16 *rs1,
                                     vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vsseg4e16_v_bf16mf2x4_m(vbool32_t vm, __bf16 *rs1,
                                     vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vsseg5e16_v_bf16mf2x5_m(vbool32_t vm, __bf16 *rs1,
                                     vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vsseg6e16_v_bf16mf2x6_m(vbool32_t vm, __bf16 *rs1,
                                     vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vsseg7e16_v_bf16mf2x7_m(vbool32_t vm, __bf16 *rs1,
                                     vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vsseg8e16_v_bf16mf2x8_m(vbool32_t vm, __bf16 *rs1,
                                     vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vsseg2e16_v_bf16m1x2_m(vbool16_t vm, __bf16 *rs1,
                                    vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vsseg3e16_v_bf16m1x3_m(vbool16_t vm, __bf16 *rs1,
                                    vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vsseg4e16_v_bf16m1x4_m(vbool16_t vm, __bf16 *rs1,
                                    vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vsseg5e16_v_bf16m1x5_m(vbool16_t vm, __bf16 *rs1,
                                    vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vsseg6e16_v_bf16m1x6_m(vbool16_t vm, __bf16 *rs1,
                                    vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vsseg7e16_v_bf16m1x7_m(vbool16_t vm, __bf16 *rs1,
                                    vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vsseg8e16_v_bf16m1x8_m(vbool16_t vm, __bf16 *rs1,
                                    vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vsseg2e16_v_bf16m2x2_m(vbool8_t vm, __bf16 *rs1,
                                    vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vsseg3e16_v_bf16m2x3_m(vbool8_t vm, __bf16 *rs1,
                                    vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vsseg4e16_v_bf16m2x4_m(vbool8_t vm, __bf16 *rs1,
                                    vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vsseg2e16_v_bf16m4x2_m(vbool4_t vm, __bf16 *rs1,
                                    vbfloat16m4x2_t vs3, size_t vl);
----

[[vector-strided-segment-load]]
==== Vector Strided Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4x2_t __riscv_vlsseg2e16_v_bf16mf4x2(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vlsseg3e16_v_bf16mf4x3(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vlsseg4e16_v_bf16mf4x4(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vlsseg5e16_v_bf16mf4x5(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vlsseg6e16_v_bf16mf4x6(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vlsseg7e16_v_bf16mf4x7(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vlsseg8e16_v_bf16mf4x8(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vlsseg2e16_v_bf16mf2x2(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vlsseg3e16_v_bf16mf2x3(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vlsseg4e16_v_bf16mf2x4(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vlsseg5e16_v_bf16mf2x5(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vlsseg6e16_v_bf16mf2x6(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vlsseg7e16_v_bf16mf2x7(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vlsseg8e16_v_bf16mf2x8(const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vlsseg2e16_v_bf16m1x2(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m1x3_t __riscv_vlsseg3e16_v_bf16m1x3(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m1x4_t __riscv_vlsseg4e16_v_bf16m1x4(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m1x5_t __riscv_vlsseg5e16_v_bf16m1x5(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m1x6_t __riscv_vlsseg6e16_v_bf16m1x6(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m1x7_t __riscv_vlsseg7e16_v_bf16m1x7(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m1x8_t __riscv_vlsseg8e16_v_bf16m1x8(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m2x2_t __riscv_vlsseg2e16_v_bf16m2x2(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m2x3_t __riscv_vlsseg3e16_v_bf16m2x3(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m2x4_t __riscv_vlsseg4e16_v_bf16m2x4(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
vbfloat16m4x2_t __riscv_vlsseg2e16_v_bf16m4x2(const __bf16 *rs1, ptrdiff_t rs2,
                                              size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vlsseg2e16_v_bf16mf4x2_m(vbool64_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vlsseg3e16_v_bf16mf4x3_m(vbool64_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vlsseg4e16_v_bf16mf4x4_m(vbool64_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vlsseg5e16_v_bf16mf4x5_m(vbool64_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vlsseg6e16_v_bf16mf4x6_m(vbool64_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vlsseg7e16_v_bf16mf4x7_m(vbool64_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vlsseg8e16_v_bf16mf4x8_m(vbool64_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vlsseg2e16_v_bf16mf2x2_m(vbool32_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vlsseg3e16_v_bf16mf2x3_m(vbool32_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vlsseg4e16_v_bf16mf2x4_m(vbool32_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vlsseg5e16_v_bf16mf2x5_m(vbool32_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vlsseg6e16_v_bf16mf2x6_m(vbool32_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vlsseg7e16_v_bf16mf2x7_m(vbool32_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vlsseg8e16_v_bf16mf2x8_m(vbool32_t vm,
                                                  const __bf16 *rs1,
                                                  ptrdiff_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vlsseg2e16_v_bf16m1x2_m(vbool16_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vlsseg3e16_v_bf16m1x3_m(vbool16_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vlsseg4e16_v_bf16m1x4_m(vbool16_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vlsseg5e16_v_bf16m1x5_m(vbool16_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vlsseg6e16_v_bf16m1x6_m(vbool16_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vlsseg7e16_v_bf16m1x7_m(vbool16_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vlsseg8e16_v_bf16m1x8_m(vbool16_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vlsseg2e16_v_bf16m2x2_m(vbool8_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vlsseg3e16_v_bf16m2x3_m(vbool8_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vlsseg4e16_v_bf16m2x4_m(vbool8_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vlsseg2e16_v_bf16m4x2_m(vbool4_t vm, const __bf16 *rs1,
                                                ptrdiff_t rs2, size_t vl);
----

[[vector-strided-segment-store]]
==== Vector Strided Segment Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vssseg2e16_v_bf16mf4x2(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vssseg3e16_v_bf16mf4x3(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vssseg4e16_v_bf16mf4x4(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vssseg5e16_v_bf16mf4x5(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vssseg6e16_v_bf16mf4x6(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vssseg7e16_v_bf16mf4x7(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vssseg8e16_v_bf16mf4x8(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vssseg2e16_v_bf16mf2x2(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vssseg3e16_v_bf16mf2x3(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vssseg4e16_v_bf16mf2x4(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vssseg5e16_v_bf16mf2x5(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vssseg6e16_v_bf16mf2x6(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vssseg7e16_v_bf16mf2x7(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vssseg8e16_v_bf16mf2x8(__bf16 *rs1, ptrdiff_t rs2,
                                    vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vssseg2e16_v_bf16m1x2(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vssseg3e16_v_bf16m1x3(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vssseg4e16_v_bf16m1x4(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vssseg5e16_v_bf16m1x5(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vssseg6e16_v_bf16m1x6(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vssseg7e16_v_bf16m1x7(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vssseg8e16_v_bf16m1x8(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vssseg2e16_v_bf16m2x2(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vssseg3e16_v_bf16m2x3(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vssseg4e16_v_bf16m2x4(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vssseg2e16_v_bf16m4x2(__bf16 *rs1, ptrdiff_t rs2,
                                   vbfloat16m4x2_t vs3, size_t vl);
// masked functions
void __riscv_vssseg2e16_v_bf16mf4x2_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vssseg3e16_v_bf16mf4x3_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vssseg4e16_v_bf16mf4x4_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vssseg5e16_v_bf16mf4x5_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vssseg6e16_v_bf16mf4x6_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vssseg7e16_v_bf16mf4x7_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vssseg8e16_v_bf16mf4x8_m(vbool64_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vssseg2e16_v_bf16mf2x2_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vssseg3e16_v_bf16mf2x3_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vssseg4e16_v_bf16mf2x4_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vssseg5e16_v_bf16mf2x5_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vssseg6e16_v_bf16mf2x6_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vssseg7e16_v_bf16mf2x7_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vssseg8e16_v_bf16mf2x8_m(vbool32_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                      vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vssseg2e16_v_bf16m1x2_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vssseg3e16_v_bf16m1x3_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vssseg4e16_v_bf16m1x4_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vssseg5e16_v_bf16m1x5_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vssseg6e16_v_bf16m1x6_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vssseg7e16_v_bf16m1x7_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vssseg8e16_v_bf16m1x8_m(vbool16_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vssseg2e16_v_bf16m2x2_m(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vssseg3e16_v_bf16m2x3_m(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vssseg4e16_v_bf16m2x4_m(vbool8_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vssseg2e16_v_bf16m4x2_m(vbool4_t vm, __bf16 *rs1, ptrdiff_t rs2,
                                     vbfloat16m4x2_t vs3, size_t vl);
----

[[vector-indexed-segment-load]]
==== Vector Indexed Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4x2_t __riscv_vloxseg2ei16_v_bf16mf4x2(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vloxseg3ei16_v_bf16mf4x3(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vloxseg4ei16_v_bf16mf4x4(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vloxseg5ei16_v_bf16mf4x5(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vloxseg6ei16_v_bf16mf4x6(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vloxseg7ei16_v_bf16mf4x7(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vloxseg8ei16_v_bf16mf4x8(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vloxseg2ei16_v_bf16mf2x2(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vloxseg3ei16_v_bf16mf2x3(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vloxseg4ei16_v_bf16mf2x4(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vloxseg5ei16_v_bf16mf2x5(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vloxseg6ei16_v_bf16mf2x6(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vloxseg7ei16_v_bf16mf2x7(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vloxseg8ei16_v_bf16mf2x8(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vloxseg2ei16_v_bf16m1x2(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vloxseg3ei16_v_bf16m1x3(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vloxseg4ei16_v_bf16m1x4(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vloxseg5ei16_v_bf16m1x5(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vloxseg6ei16_v_bf16m1x6(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vloxseg7ei16_v_bf16m1x7(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vloxseg8ei16_v_bf16m1x8(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vloxseg2ei16_v_bf16m2x2(const __bf16 *rs1,
                                                vuint16m2_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vloxseg3ei16_v_bf16m2x3(const __bf16 *rs1,
                                                vuint16m2_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vloxseg4ei16_v_bf16m2x4(const __bf16 *rs1,
                                                vuint16m2_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vloxseg2ei16_v_bf16m4x2(const __bf16 *rs1,
                                                vuint16m4_t rs2, size_t vl);
vbfloat16mf4x2_t __riscv_vluxseg2ei16_v_bf16mf4x2(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vluxseg3ei16_v_bf16mf4x3(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vluxseg4ei16_v_bf16mf4x4(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vluxseg5ei16_v_bf16mf4x5(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vluxseg6ei16_v_bf16mf4x6(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vluxseg7ei16_v_bf16mf4x7(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vluxseg8ei16_v_bf16mf4x8(const __bf16 *rs1,
                                                  vuint16mf4_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vluxseg2ei16_v_bf16mf2x2(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vluxseg3ei16_v_bf16mf2x3(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vluxseg4ei16_v_bf16mf2x4(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vluxseg5ei16_v_bf16mf2x5(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vluxseg6ei16_v_bf16mf2x6(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vluxseg7ei16_v_bf16mf2x7(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vluxseg8ei16_v_bf16mf2x8(const __bf16 *rs1,
                                                  vuint16mf2_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vluxseg2ei16_v_bf16m1x2(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vluxseg3ei16_v_bf16m1x3(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vluxseg4ei16_v_bf16m1x4(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vluxseg5ei16_v_bf16m1x5(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vluxseg6ei16_v_bf16m1x6(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vluxseg7ei16_v_bf16m1x7(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vluxseg8ei16_v_bf16m1x8(const __bf16 *rs1,
                                                vuint16m1_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vluxseg2ei16_v_bf16m2x2(const __bf16 *rs1,
                                                vuint16m2_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vluxseg3ei16_v_bf16m2x3(const __bf16 *rs1,
                                                vuint16m2_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vluxseg4ei16_v_bf16m2x4(const __bf16 *rs1,
                                                vuint16m2_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vluxseg2ei16_v_bf16m4x2(const __bf16 *rs1,
                                                vuint16m4_t rs2, size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vloxseg2ei16_v_bf16mf4x2_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x3_t __riscv_vloxseg3ei16_v_bf16mf4x3_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x4_t __riscv_vloxseg4ei16_v_bf16mf4x4_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x5_t __riscv_vloxseg5ei16_v_bf16mf4x5_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x6_t __riscv_vloxseg6ei16_v_bf16mf4x6_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x7_t __riscv_vloxseg7ei16_v_bf16mf4x7_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x8_t __riscv_vloxseg8ei16_v_bf16mf4x8_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf2x2_t __riscv_vloxseg2ei16_v_bf16mf2x2_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x3_t __riscv_vloxseg3ei16_v_bf16mf2x3_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x4_t __riscv_vloxseg4ei16_v_bf16mf2x4_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x5_t __riscv_vloxseg5ei16_v_bf16mf2x5_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x6_t __riscv_vloxseg6ei16_v_bf16mf2x6_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x7_t __riscv_vloxseg7ei16_v_bf16mf2x7_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x8_t __riscv_vloxseg8ei16_v_bf16mf2x8_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16m1x2_t __riscv_vloxseg2ei16_v_bf16m1x2_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vloxseg3ei16_v_bf16m1x3_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vloxseg4ei16_v_bf16m1x4_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vloxseg5ei16_v_bf16m1x5_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vloxseg6ei16_v_bf16m1x6_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vloxseg7ei16_v_bf16m1x7_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vloxseg8ei16_v_bf16m1x8_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vloxseg2ei16_v_bf16m2x2_m(vbool8_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m2_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vloxseg3ei16_v_bf16m2x3_m(vbool8_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m2_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vloxseg4ei16_v_bf16m2x4_m(vbool8_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m2_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vloxseg2ei16_v_bf16m4x2_m(vbool4_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m4_t rs2, size_t vl);
vbfloat16mf4x2_t __riscv_vluxseg2ei16_v_bf16mf4x2_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x3_t __riscv_vluxseg3ei16_v_bf16mf4x3_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x4_t __riscv_vluxseg4ei16_v_bf16mf4x4_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x5_t __riscv_vluxseg5ei16_v_bf16mf4x5_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x6_t __riscv_vluxseg6ei16_v_bf16mf4x6_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x7_t __riscv_vluxseg7ei16_v_bf16mf4x7_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf4x8_t __riscv_vluxseg8ei16_v_bf16mf4x8_m(vbool64_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf4_t rs2,
                                                    size_t vl);
vbfloat16mf2x2_t __riscv_vluxseg2ei16_v_bf16mf2x2_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x3_t __riscv_vluxseg3ei16_v_bf16mf2x3_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x4_t __riscv_vluxseg4ei16_v_bf16mf2x4_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x5_t __riscv_vluxseg5ei16_v_bf16mf2x5_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x6_t __riscv_vluxseg6ei16_v_bf16mf2x6_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x7_t __riscv_vluxseg7ei16_v_bf16mf2x7_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16mf2x8_t __riscv_vluxseg8ei16_v_bf16mf2x8_m(vbool32_t vm,
                                                    const __bf16 *rs1,
                                                    vuint16mf2_t rs2,
                                                    size_t vl);
vbfloat16m1x2_t __riscv_vluxseg2ei16_v_bf16m1x2_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vluxseg3ei16_v_bf16m1x3_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vluxseg4ei16_v_bf16m1x4_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vluxseg5ei16_v_bf16m1x5_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vluxseg6ei16_v_bf16m1x6_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vluxseg7ei16_v_bf16m1x7_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vluxseg8ei16_v_bf16m1x8_m(vbool16_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m1_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vluxseg2ei16_v_bf16m2x2_m(vbool8_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m2_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vluxseg3ei16_v_bf16m2x3_m(vbool8_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m2_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vluxseg4ei16_v_bf16m2x4_m(vbool8_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m2_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vluxseg2ei16_v_bf16m4x2_m(vbool4_t vm,
                                                  const __bf16 *rs1,
                                                  vuint16m4_t rs2, size_t vl);
----

[[vector-indexed-segment-store]]
==== Vector Indexed Segment Store Intrinsics(XAndesVBFHCvt)

[,c]
----
void __riscv_vsoxseg2ei16_v_bf16mf4x2(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vsoxseg3ei16_v_bf16mf4x3(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vsoxseg4ei16_v_bf16mf4x4(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vsoxseg5ei16_v_bf16mf4x5(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vsoxseg6ei16_v_bf16mf4x6(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vsoxseg7ei16_v_bf16mf4x7(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vsoxseg8ei16_v_bf16mf4x8(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vsoxseg2ei16_v_bf16mf2x2(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vsoxseg3ei16_v_bf16mf2x3(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vsoxseg4ei16_v_bf16mf2x4(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vsoxseg5ei16_v_bf16mf2x5(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vsoxseg6ei16_v_bf16mf2x6(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vsoxseg7ei16_v_bf16mf2x7(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vsoxseg8ei16_v_bf16mf2x8(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vsoxseg2ei16_v_bf16m1x2(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vsoxseg3ei16_v_bf16m1x3(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vsoxseg4ei16_v_bf16m1x4(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vsoxseg5ei16_v_bf16m1x5(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vsoxseg6ei16_v_bf16m1x6(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vsoxseg7ei16_v_bf16m1x7(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vsoxseg8ei16_v_bf16m1x8(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vsoxseg2ei16_v_bf16m2x2(__bf16 *rs1, vuint16m2_t vs2,
                                     vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vsoxseg3ei16_v_bf16m2x3(__bf16 *rs1, vuint16m2_t vs2,
                                     vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vsoxseg4ei16_v_bf16m2x4(__bf16 *rs1, vuint16m2_t vs2,
                                     vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vsoxseg2ei16_v_bf16m4x2(__bf16 *rs1, vuint16m4_t vs2,
                                     vbfloat16m4x2_t vs3, size_t vl);
void __riscv_vsuxseg2ei16_v_bf16mf4x2(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x2_t vs3, size_t vl);
void __riscv_vsuxseg3ei16_v_bf16mf4x3(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x3_t vs3, size_t vl);
void __riscv_vsuxseg4ei16_v_bf16mf4x4(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x4_t vs3, size_t vl);
void __riscv_vsuxseg5ei16_v_bf16mf4x5(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x5_t vs3, size_t vl);
void __riscv_vsuxseg6ei16_v_bf16mf4x6(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x6_t vs3, size_t vl);
void __riscv_vsuxseg7ei16_v_bf16mf4x7(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x7_t vs3, size_t vl);
void __riscv_vsuxseg8ei16_v_bf16mf4x8(__bf16 *rs1, vuint16mf4_t vs2,
                                      vbfloat16mf4x8_t vs3, size_t vl);
void __riscv_vsuxseg2ei16_v_bf16mf2x2(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x2_t vs3, size_t vl);
void __riscv_vsuxseg3ei16_v_bf16mf2x3(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x3_t vs3, size_t vl);
void __riscv_vsuxseg4ei16_v_bf16mf2x4(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x4_t vs3, size_t vl);
void __riscv_vsuxseg5ei16_v_bf16mf2x5(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x5_t vs3, size_t vl);
void __riscv_vsuxseg6ei16_v_bf16mf2x6(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x6_t vs3, size_t vl);
void __riscv_vsuxseg7ei16_v_bf16mf2x7(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x7_t vs3, size_t vl);
void __riscv_vsuxseg8ei16_v_bf16mf2x8(__bf16 *rs1, vuint16mf2_t vs2,
                                      vbfloat16mf2x8_t vs3, size_t vl);
void __riscv_vsuxseg2ei16_v_bf16m1x2(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x2_t vs3, size_t vl);
void __riscv_vsuxseg3ei16_v_bf16m1x3(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x3_t vs3, size_t vl);
void __riscv_vsuxseg4ei16_v_bf16m1x4(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x4_t vs3, size_t vl);
void __riscv_vsuxseg5ei16_v_bf16m1x5(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x5_t vs3, size_t vl);
void __riscv_vsuxseg6ei16_v_bf16m1x6(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x6_t vs3, size_t vl);
void __riscv_vsuxseg7ei16_v_bf16m1x7(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x7_t vs3, size_t vl);
void __riscv_vsuxseg8ei16_v_bf16m1x8(__bf16 *rs1, vuint16m1_t vs2,
                                     vbfloat16m1x8_t vs3, size_t vl);
void __riscv_vsuxseg2ei16_v_bf16m2x2(__bf16 *rs1, vuint16m2_t vs2,
                                     vbfloat16m2x2_t vs3, size_t vl);
void __riscv_vsuxseg3ei16_v_bf16m2x3(__bf16 *rs1, vuint16m2_t vs2,
                                     vbfloat16m2x3_t vs3, size_t vl);
void __riscv_vsuxseg4ei16_v_bf16m2x4(__bf16 *rs1, vuint16m2_t vs2,
                                     vbfloat16m2x4_t vs3, size_t vl);
void __riscv_vsuxseg2ei16_v_bf16m4x2(__bf16 *rs1, vuint16m4_t vs2,
                                     vbfloat16m4x2_t vs3, size_t vl);
// masked functions
void __riscv_vsoxseg2ei16_v_bf16mf4x2_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x2_t vs3,
                                        size_t vl);
void __riscv_vsoxseg3ei16_v_bf16mf4x3_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x3_t vs3,
                                        size_t vl);
void __riscv_vsoxseg4ei16_v_bf16mf4x4_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x4_t vs3,
                                        size_t vl);
void __riscv_vsoxseg5ei16_v_bf16mf4x5_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x5_t vs3,
                                        size_t vl);
void __riscv_vsoxseg6ei16_v_bf16mf4x6_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x6_t vs3,
                                        size_t vl);
void __riscv_vsoxseg7ei16_v_bf16mf4x7_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x7_t vs3,
                                        size_t vl);
void __riscv_vsoxseg8ei16_v_bf16mf4x8_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x8_t vs3,
                                        size_t vl);
void __riscv_vsoxseg2ei16_v_bf16mf2x2_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x2_t vs3,
                                        size_t vl);
void __riscv_vsoxseg3ei16_v_bf16mf2x3_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x3_t vs3,
                                        size_t vl);
void __riscv_vsoxseg4ei16_v_bf16mf2x4_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x4_t vs3,
                                        size_t vl);
void __riscv_vsoxseg5ei16_v_bf16mf2x5_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x5_t vs3,
                                        size_t vl);
void __riscv_vsoxseg6ei16_v_bf16mf2x6_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x6_t vs3,
                                        size_t vl);
void __riscv_vsoxseg7ei16_v_bf16mf2x7_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x7_t vs3,
                                        size_t vl);
void __riscv_vsoxseg8ei16_v_bf16mf2x8_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x8_t vs3,
                                        size_t vl);
void __riscv_vsoxseg2ei16_v_bf16m1x2_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x2_t vs3,
                                       size_t vl);
void __riscv_vsoxseg3ei16_v_bf16m1x3_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x3_t vs3,
                                       size_t vl);
void __riscv_vsoxseg4ei16_v_bf16m1x4_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x4_t vs3,
                                       size_t vl);
void __riscv_vsoxseg5ei16_v_bf16m1x5_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x5_t vs3,
                                       size_t vl);
void __riscv_vsoxseg6ei16_v_bf16m1x6_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x6_t vs3,
                                       size_t vl);
void __riscv_vsoxseg7ei16_v_bf16m1x7_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x7_t vs3,
                                       size_t vl);
void __riscv_vsoxseg8ei16_v_bf16m1x8_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x8_t vs3,
                                       size_t vl);
void __riscv_vsoxseg2ei16_v_bf16m2x2_m(vbool8_t vm, __bf16 *rs1,
                                       vuint16m2_t vs2, vbfloat16m2x2_t vs3,
                                       size_t vl);
void __riscv_vsoxseg3ei16_v_bf16m2x3_m(vbool8_t vm, __bf16 *rs1,
                                       vuint16m2_t vs2, vbfloat16m2x3_t vs3,
                                       size_t vl);
void __riscv_vsoxseg4ei16_v_bf16m2x4_m(vbool8_t vm, __bf16 *rs1,
                                       vuint16m2_t vs2, vbfloat16m2x4_t vs3,
                                       size_t vl);
void __riscv_vsoxseg2ei16_v_bf16m4x2_m(vbool4_t vm, __bf16 *rs1,
                                       vuint16m4_t vs2, vbfloat16m4x2_t vs3,
                                       size_t vl);
void __riscv_vsuxseg2ei16_v_bf16mf4x2_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x2_t vs3,
                                        size_t vl);
void __riscv_vsuxseg3ei16_v_bf16mf4x3_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x3_t vs3,
                                        size_t vl);
void __riscv_vsuxseg4ei16_v_bf16mf4x4_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x4_t vs3,
                                        size_t vl);
void __riscv_vsuxseg5ei16_v_bf16mf4x5_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x5_t vs3,
                                        size_t vl);
void __riscv_vsuxseg6ei16_v_bf16mf4x6_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x6_t vs3,
                                        size_t vl);
void __riscv_vsuxseg7ei16_v_bf16mf4x7_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x7_t vs3,
                                        size_t vl);
void __riscv_vsuxseg8ei16_v_bf16mf4x8_m(vbool64_t vm, __bf16 *rs1,
                                        vuint16mf4_t vs2, vbfloat16mf4x8_t vs3,
                                        size_t vl);
void __riscv_vsuxseg2ei16_v_bf16mf2x2_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x2_t vs3,
                                        size_t vl);
void __riscv_vsuxseg3ei16_v_bf16mf2x3_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x3_t vs3,
                                        size_t vl);
void __riscv_vsuxseg4ei16_v_bf16mf2x4_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x4_t vs3,
                                        size_t vl);
void __riscv_vsuxseg5ei16_v_bf16mf2x5_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x5_t vs3,
                                        size_t vl);
void __riscv_vsuxseg6ei16_v_bf16mf2x6_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x6_t vs3,
                                        size_t vl);
void __riscv_vsuxseg7ei16_v_bf16mf2x7_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x7_t vs3,
                                        size_t vl);
void __riscv_vsuxseg8ei16_v_bf16mf2x8_m(vbool32_t vm, __bf16 *rs1,
                                        vuint16mf2_t vs2, vbfloat16mf2x8_t vs3,
                                        size_t vl);
void __riscv_vsuxseg2ei16_v_bf16m1x2_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x2_t vs3,
                                       size_t vl);
void __riscv_vsuxseg3ei16_v_bf16m1x3_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x3_t vs3,
                                       size_t vl);
void __riscv_vsuxseg4ei16_v_bf16m1x4_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x4_t vs3,
                                       size_t vl);
void __riscv_vsuxseg5ei16_v_bf16m1x5_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x5_t vs3,
                                       size_t vl);
void __riscv_vsuxseg6ei16_v_bf16m1x6_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x6_t vs3,
                                       size_t vl);
void __riscv_vsuxseg7ei16_v_bf16m1x7_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x7_t vs3,
                                       size_t vl);
void __riscv_vsuxseg8ei16_v_bf16m1x8_m(vbool16_t vm, __bf16 *rs1,
                                       vuint16m1_t vs2, vbfloat16m1x8_t vs3,
                                       size_t vl);
void __riscv_vsuxseg2ei16_v_bf16m2x2_m(vbool8_t vm, __bf16 *rs1,
                                       vuint16m2_t vs2, vbfloat16m2x2_t vs3,
                                       size_t vl);
void __riscv_vsuxseg3ei16_v_bf16m2x3_m(vbool8_t vm, __bf16 *rs1,
                                       vuint16m2_t vs2, vbfloat16m2x3_t vs3,
                                       size_t vl);
void __riscv_vsuxseg4ei16_v_bf16m2x4_m(vbool8_t vm, __bf16 *rs1,
                                       vuint16m2_t vs2, vbfloat16m2x4_t vs3,
                                       size_t vl);
void __riscv_vsuxseg2ei16_v_bf16m4x2_m(vbool4_t vm, __bf16 *rs1,
                                       vuint16m4_t vs2, vbfloat16m4x2_t vs3,
                                       size_t vl);
----

[[bf16-vector-narrow-convert]]
==== Vector Narrowing Convert Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_nds_vfncvt_bf16_s_bf16mf4(vfloat32mf2_t vs2, size_t vl);
vbfloat16mf2_t __riscv_nds_vfncvt_bf16_s_bf16mf2(vfloat32m1_t vs2, size_t vl);
vbfloat16m1_t __riscv_nds_vfncvt_bf16_s_bf16m1(vfloat32m2_t vs2, size_t vl);
vbfloat16m2_t __riscv_nds_vfncvt_bf16_s_bf16m2(vfloat32m4_t vs2, size_t vl);
vbfloat16m4_t __riscv_nds_vfncvt_bf16_s_bf16m4(vfloat32m8_t vs2, size_t vl);
vbfloat16mf4_t __riscv_nds_vfncvt_bf16_s_bf16mf4_rm(vfloat32mf2_t vs2,
                                                    unsigned int frm,
                                                    size_t vl);
vbfloat16mf2_t __riscv_nds_vfncvt_bf16_s_bf16mf2_rm(vfloat32m1_t vs2,
                                                    unsigned int frm,
                                                    size_t vl);
vbfloat16m1_t __riscv_nds_vfncvt_bf16_s_bf16m1_rm(vfloat32m2_t vs2,
                                                  unsigned int frm, size_t vl);
vbfloat16m2_t __riscv_nds_vfncvt_bf16_s_bf16m2_rm(vfloat32m4_t vs2,
                                                  unsigned int frm, size_t vl);
vbfloat16m4_t __riscv_nds_vfncvt_bf16_s_bf16m4_rm(vfloat32m8_t vs2,
                                                  unsigned int frm, size_t vl);
----

[[bf16-vector-widening-convert]]
==== Vector Widening Convert Intrinsics(XAndesVBFHCvt)

[,c]
----
vfloat32mf2_t __riscv_nds_vfwcvt_s_bf16_f32mf2(vbfloat16mf4_t vs2, size_t vl);
vfloat32m1_t __riscv_nds_vfwcvt_s_bf16_f32m1(vbfloat16mf2_t vs2, size_t vl);
vfloat32m2_t __riscv_nds_vfwcvt_s_bf16_f32m2(vbfloat16m1_t vs2, size_t vl);
vfloat32m4_t __riscv_nds_vfwcvt_s_bf16_f32m4(vbfloat16m2_t vs2, size_t vl);
vfloat32m8_t __riscv_nds_vfwcvt_s_bf16_f32m8(vbfloat16m4_t vs2, size_t vl);
----

[[reinterpret-cast-conversion]]
==== Reinterpret Cast Conversion Intrinsics(XAndesVBFHCvt)

[,c]
----
// Reinterpret between different type under the same SEW/LMUL
vbfloat16mf4_t __riscv_vreinterpret_v_i16mf4_bf16mf4(vint16mf4_t src);
vbfloat16mf2_t __riscv_vreinterpret_v_i16mf2_bf16mf2(vint16mf2_t src);
vbfloat16m1_t __riscv_vreinterpret_v_i16m1_bf16m1(vint16m1_t src);
vbfloat16m2_t __riscv_vreinterpret_v_i16m2_bf16m2(vint16m2_t src);
vbfloat16m4_t __riscv_vreinterpret_v_i16m4_bf16m4(vint16m4_t src);
vbfloat16m8_t __riscv_vreinterpret_v_i16m8_bf16m8(vint16m8_t src);
vbfloat16mf4_t __riscv_vreinterpret_v_u16mf4_bf16mf4(vuint16mf4_t src);
vbfloat16mf2_t __riscv_vreinterpret_v_u16mf2_bf16mf2(vuint16mf2_t src);
vbfloat16m1_t __riscv_vreinterpret_v_u16m1_bf16m1(vuint16m1_t src);
vbfloat16m2_t __riscv_vreinterpret_v_u16m2_bf16m2(vuint16m2_t src);
vbfloat16m4_t __riscv_vreinterpret_v_u16m4_bf16m4(vuint16m4_t src);
vbfloat16m8_t __riscv_vreinterpret_v_u16m8_bf16m8(vuint16m8_t src);
vint16mf4_t __riscv_vreinterpret_v_bf16mf4_i16mf4(vbfloat16mf4_t src);
vint16mf2_t __riscv_vreinterpret_v_bf16mf2_i16mf2(vbfloat16mf2_t src);
vint16m1_t __riscv_vreinterpret_v_bf16m1_i16m1(vbfloat16m1_t src);
vint16m2_t __riscv_vreinterpret_v_bf16m2_i16m2(vbfloat16m2_t src);
vint16m4_t __riscv_vreinterpret_v_bf16m4_i16m4(vbfloat16m4_t src);
vint16m8_t __riscv_vreinterpret_v_bf16m8_i16m8(vbfloat16m8_t src);
vuint16mf4_t __riscv_vreinterpret_v_bf16mf4_u16mf4(vbfloat16mf4_t src);
vuint16mf2_t __riscv_vreinterpret_v_bf16mf2_u16mf2(vbfloat16mf2_t src);
vuint16m1_t __riscv_vreinterpret_v_bf16m1_u16m1(vbfloat16m1_t src);
vuint16m2_t __riscv_vreinterpret_v_bf16m2_u16m2(vbfloat16m2_t src);
vuint16m4_t __riscv_vreinterpret_v_bf16m4_u16m4(vbfloat16m4_t src);
vuint16m8_t __riscv_vreinterpret_v_bf16m8_u16m8(vbfloat16m8_t src);
----

[[vector-lmul-extensionn]]
==== Vector LMUL Extension Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf2_t __riscv_vlmul_ext_v_bf16mf4_bf16mf2(vbfloat16mf4_t value);
vbfloat16m1_t __riscv_vlmul_ext_v_bf16mf4_bf16m1(vbfloat16mf4_t value);
vbfloat16m2_t __riscv_vlmul_ext_v_bf16mf4_bf16m2(vbfloat16mf4_t value);
vbfloat16m4_t __riscv_vlmul_ext_v_bf16mf4_bf16m4(vbfloat16mf4_t value);
vbfloat16m8_t __riscv_vlmul_ext_v_bf16mf4_bf16m8(vbfloat16mf4_t value);
vbfloat16m1_t __riscv_vlmul_ext_v_bf16mf2_bf16m1(vbfloat16mf2_t value);
vbfloat16m2_t __riscv_vlmul_ext_v_bf16mf2_bf16m2(vbfloat16mf2_t value);
vbfloat16m4_t __riscv_vlmul_ext_v_bf16mf2_bf16m4(vbfloat16mf2_t value);
vbfloat16m8_t __riscv_vlmul_ext_v_bf16mf2_bf16m8(vbfloat16mf2_t value);
vbfloat16m2_t __riscv_vlmul_ext_v_bf16m1_bf16m2(vbfloat16m1_t value);
vbfloat16m4_t __riscv_vlmul_ext_v_bf16m1_bf16m4(vbfloat16m1_t value);
vbfloat16m8_t __riscv_vlmul_ext_v_bf16m1_bf16m8(vbfloat16m1_t value);
vbfloat16m4_t __riscv_vlmul_ext_v_bf16m2_bf16m4(vbfloat16m2_t value);
vbfloat16m8_t __riscv_vlmul_ext_v_bf16m2_bf16m8(vbfloat16m2_t value);
vbfloat16m8_t __riscv_vlmul_ext_v_bf16m4_bf16m8(vbfloat16m4_t value);
----

[[vector-lmul-truncation]]
==== Vector LMUL Truncation Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vlmul_trunc_v_bf16mf2_bf16mf4(vbfloat16mf2_t value);
vbfloat16mf4_t __riscv_vlmul_trunc_v_bf16m1_bf16mf4(vbfloat16m1_t value);
vbfloat16mf2_t __riscv_vlmul_trunc_v_bf16m1_bf16mf2(vbfloat16m1_t value);
vbfloat16mf4_t __riscv_vlmul_trunc_v_bf16m2_bf16mf4(vbfloat16m2_t value);
vbfloat16mf2_t __riscv_vlmul_trunc_v_bf16m2_bf16mf2(vbfloat16m2_t value);
vbfloat16m1_t __riscv_vlmul_trunc_v_bf16m2_bf16m1(vbfloat16m2_t value);
vbfloat16mf4_t __riscv_vlmul_trunc_v_bf16m4_bf16mf4(vbfloat16m4_t value);
vbfloat16mf2_t __riscv_vlmul_trunc_v_bf16m4_bf16mf2(vbfloat16m4_t value);
vbfloat16m1_t __riscv_vlmul_trunc_v_bf16m4_bf16m1(vbfloat16m4_t value);
vbfloat16m2_t __riscv_vlmul_trunc_v_bf16m4_bf16m2(vbfloat16m4_t value);
vbfloat16mf4_t __riscv_vlmul_trunc_v_bf16m8_bf16mf4(vbfloat16m8_t value);
vbfloat16mf2_t __riscv_vlmul_trunc_v_bf16m8_bf16mf2(vbfloat16m8_t value);
vbfloat16m1_t __riscv_vlmul_trunc_v_bf16m8_bf16m1(vbfloat16m8_t value);
vbfloat16m2_t __riscv_vlmul_trunc_v_bf16m8_bf16m2(vbfloat16m8_t value);
vbfloat16m4_t __riscv_vlmul_trunc_v_bf16m8_bf16m4(vbfloat16m8_t value);
----

[[vector-initialization]]
==== Vector Initialization Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vundefined_bf16mf4();
vbfloat16mf2_t __riscv_vundefined_bf16mf2();
vbfloat16m1_t __riscv_vundefined_bf16m1();
vbfloat16m2_t __riscv_vundefined_bf16m2();
vbfloat16m4_t __riscv_vundefined_bf16m4();
vbfloat16m8_t __riscv_vundefined_bf16m8();
vbfloat16mf4x2_t __riscv_vundefined_bf16mf4x2();
vbfloat16mf4x3_t __riscv_vundefined_bf16mf4x3();
vbfloat16mf4x4_t __riscv_vundefined_bf16mf4x4();
vbfloat16mf4x5_t __riscv_vundefined_bf16mf4x5();
vbfloat16mf4x6_t __riscv_vundefined_bf16mf4x6();
vbfloat16mf4x7_t __riscv_vundefined_bf16mf4x7();
vbfloat16mf4x8_t __riscv_vundefined_bf16mf4x8();
vbfloat16mf2x2_t __riscv_vundefined_bf16mf2x2();
vbfloat16mf2x3_t __riscv_vundefined_bf16mf2x3();
vbfloat16mf2x4_t __riscv_vundefined_bf16mf2x4();
vbfloat16mf2x5_t __riscv_vundefined_bf16mf2x5();
vbfloat16mf2x6_t __riscv_vundefined_bf16mf2x6();
vbfloat16mf2x7_t __riscv_vundefined_bf16mf2x7();
vbfloat16mf2x8_t __riscv_vundefined_bf16mf2x8();
vbfloat16m1x2_t __riscv_vundefined_bf16m1x2();
vbfloat16m1x3_t __riscv_vundefined_bf16m1x3();
vbfloat16m1x4_t __riscv_vundefined_bf16m1x4();
vbfloat16m1x5_t __riscv_vundefined_bf16m1x5();
vbfloat16m1x6_t __riscv_vundefined_bf16m1x6();
vbfloat16m1x7_t __riscv_vundefined_bf16m1x7();
vbfloat16m1x8_t __riscv_vundefined_bf16m1x8();
vbfloat16m2x2_t __riscv_vundefined_bf16m2x2();
vbfloat16m2x3_t __riscv_vundefined_bf16m2x3();
vbfloat16m2x4_t __riscv_vundefined_bf16m2x4();
vbfloat16m4x2_t __riscv_vundefined_bf16m4x2();
----

[[vector-insertion]]
==== Vector Insertion Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16m2_t __riscv_vset_v_bf16m1_bf16m2(vbfloat16m2_t dest, size_t index,
                                           vbfloat16m1_t value);
vbfloat16m4_t __riscv_vset_v_bf16m1_bf16m4(vbfloat16m4_t dest, size_t index,
                                           vbfloat16m1_t value);
vbfloat16m4_t __riscv_vset_v_bf16m2_bf16m4(vbfloat16m4_t dest, size_t index,
                                           vbfloat16m2_t value);
vbfloat16m8_t __riscv_vset_v_bf16m1_bf16m8(vbfloat16m8_t dest, size_t index,
                                           vbfloat16m1_t value);
vbfloat16m8_t __riscv_vset_v_bf16m2_bf16m8(vbfloat16m8_t dest, size_t index,
                                           vbfloat16m2_t value);
vbfloat16m8_t __riscv_vset_v_bf16m4_bf16m8(vbfloat16m8_t dest, size_t index,
                                           vbfloat16m4_t value);
vbfloat16mf4x2_t __riscv_vset_v_bf16mf4_bf16mf4x2(vbfloat16mf4x2_t dest,
                                                  size_t index,
                                                  vbfloat16mf4_t value);
vbfloat16mf4x3_t __riscv_vset_v_bf16mf4_bf16mf4x3(vbfloat16mf4x3_t dest,
                                                  size_t index,
                                                  vbfloat16mf4_t value);
vbfloat16mf4x4_t __riscv_vset_v_bf16mf4_bf16mf4x4(vbfloat16mf4x4_t dest,
                                                  size_t index,
                                                  vbfloat16mf4_t value);
vbfloat16mf4x5_t __riscv_vset_v_bf16mf4_bf16mf4x5(vbfloat16mf4x5_t dest,
                                                  size_t index,
                                                  vbfloat16mf4_t value);
vbfloat16mf4x6_t __riscv_vset_v_bf16mf4_bf16mf4x6(vbfloat16mf4x6_t dest,
                                                  size_t index,
                                                  vbfloat16mf4_t value);
vbfloat16mf4x7_t __riscv_vset_v_bf16mf4_bf16mf4x7(vbfloat16mf4x7_t dest,
                                                  size_t index,
                                                  vbfloat16mf4_t value);
vbfloat16mf4x8_t __riscv_vset_v_bf16mf4_bf16mf4x8(vbfloat16mf4x8_t dest,
                                                  size_t index,
                                                  vbfloat16mf4_t value);
vbfloat16mf2x2_t __riscv_vset_v_bf16mf2_bf16mf2x2(vbfloat16mf2x2_t dest,
                                                  size_t index,
                                                  vbfloat16mf2_t value);
vbfloat16mf2x3_t __riscv_vset_v_bf16mf2_bf16mf2x3(vbfloat16mf2x3_t dest,
                                                  size_t index,
                                                  vbfloat16mf2_t value);
vbfloat16mf2x4_t __riscv_vset_v_bf16mf2_bf16mf2x4(vbfloat16mf2x4_t dest,
                                                  size_t index,
                                                  vbfloat16mf2_t value);
vbfloat16mf2x5_t __riscv_vset_v_bf16mf2_bf16mf2x5(vbfloat16mf2x5_t dest,
                                                  size_t index,
                                                  vbfloat16mf2_t value);
vbfloat16mf2x6_t __riscv_vset_v_bf16mf2_bf16mf2x6(vbfloat16mf2x6_t dest,
                                                  size_t index,
                                                  vbfloat16mf2_t value);
vbfloat16mf2x7_t __riscv_vset_v_bf16mf2_bf16mf2x7(vbfloat16mf2x7_t dest,
                                                  size_t index,
                                                  vbfloat16mf2_t value);
vbfloat16mf2x8_t __riscv_vset_v_bf16mf2_bf16mf2x8(vbfloat16mf2x8_t dest,
                                                  size_t index,
                                                  vbfloat16mf2_t value);
vbfloat16m1x2_t __riscv_vset_v_bf16m1_bf16m1x2(vbfloat16m1x2_t dest,
                                               size_t index,
                                               vbfloat16m1_t value);
vbfloat16m1x3_t __riscv_vset_v_bf16m1_bf16m1x3(vbfloat16m1x3_t dest,
                                               size_t index,
                                               vbfloat16m1_t value);
vbfloat16m1x4_t __riscv_vset_v_bf16m1_bf16m1x4(vbfloat16m1x4_t dest,
                                               size_t index,
                                               vbfloat16m1_t value);
vbfloat16m1x5_t __riscv_vset_v_bf16m1_bf16m1x5(vbfloat16m1x5_t dest,
                                               size_t index,
                                               vbfloat16m1_t value);
vbfloat16m1x6_t __riscv_vset_v_bf16m1_bf16m1x6(vbfloat16m1x6_t dest,
                                               size_t index,
                                               vbfloat16m1_t value);
vbfloat16m1x7_t __riscv_vset_v_bf16m1_bf16m1x7(vbfloat16m1x7_t dest,
                                               size_t index,
                                               vbfloat16m1_t value);
vbfloat16m1x8_t __riscv_vset_v_bf16m1_bf16m1x8(vbfloat16m1x8_t dest,
                                               size_t index,
                                               vbfloat16m1_t value);
vbfloat16m2x2_t __riscv_vset_v_bf16m2_bf16m2x2(vbfloat16m2x2_t dest,
                                               size_t index,
                                               vbfloat16m2_t value);
vbfloat16m2x3_t __riscv_vset_v_bf16m2_bf16m2x3(vbfloat16m2x3_t dest,
                                               size_t index,
                                               vbfloat16m2_t value);
vbfloat16m2x4_t __riscv_vset_v_bf16m2_bf16m2x4(vbfloat16m2x4_t dest,
                                               size_t index,
                                               vbfloat16m2_t value);
vbfloat16m4x2_t __riscv_vset_v_bf16m4_bf16m4x2(vbfloat16m4x2_t dest,
                                               size_t index,
                                               vbfloat16m4_t value);
----

[[vector-extraction]]
==== Vector Extraction Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16m1_t __riscv_vget_v_bf16m2_bf16m1(vbfloat16m2_t src, size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m4_bf16m1(vbfloat16m4_t src, size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m8_bf16m1(vbfloat16m8_t src, size_t index);
vbfloat16m2_t __riscv_vget_v_bf16m4_bf16m2(vbfloat16m4_t src, size_t index);
vbfloat16m2_t __riscv_vget_v_bf16m8_bf16m2(vbfloat16m8_t src, size_t index);
vbfloat16m4_t __riscv_vget_v_bf16m8_bf16m4(vbfloat16m8_t src, size_t index);
vbfloat16mf4_t __riscv_vget_v_bf16mf4x2_bf16mf4(vbfloat16mf4x2_t src,
                                                size_t index);
vbfloat16mf4_t __riscv_vget_v_bf16mf4x3_bf16mf4(vbfloat16mf4x3_t src,
                                                size_t index);
vbfloat16mf4_t __riscv_vget_v_bf16mf4x4_bf16mf4(vbfloat16mf4x4_t src,
                                                size_t index);
vbfloat16mf4_t __riscv_vget_v_bf16mf4x5_bf16mf4(vbfloat16mf4x5_t src,
                                                size_t index);
vbfloat16mf4_t __riscv_vget_v_bf16mf4x6_bf16mf4(vbfloat16mf4x6_t src,
                                                size_t index);
vbfloat16mf4_t __riscv_vget_v_bf16mf4x7_bf16mf4(vbfloat16mf4x7_t src,
                                                size_t index);
vbfloat16mf4_t __riscv_vget_v_bf16mf4x8_bf16mf4(vbfloat16mf4x8_t src,
                                                size_t index);
vbfloat16mf2_t __riscv_vget_v_bf16mf2x2_bf16mf2(vbfloat16mf2x2_t src,
                                                size_t index);
vbfloat16mf2_t __riscv_vget_v_bf16mf2x3_bf16mf2(vbfloat16mf2x3_t src,
                                                size_t index);
vbfloat16mf2_t __riscv_vget_v_bf16mf2x4_bf16mf2(vbfloat16mf2x4_t src,
                                                size_t index);
vbfloat16mf2_t __riscv_vget_v_bf16mf2x5_bf16mf2(vbfloat16mf2x5_t src,
                                                size_t index);
vbfloat16mf2_t __riscv_vget_v_bf16mf2x6_bf16mf2(vbfloat16mf2x6_t src,
                                                size_t index);
vbfloat16mf2_t __riscv_vget_v_bf16mf2x7_bf16mf2(vbfloat16mf2x7_t src,
                                                size_t index);
vbfloat16mf2_t __riscv_vget_v_bf16mf2x8_bf16mf2(vbfloat16mf2x8_t src,
                                                size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m1x2_bf16m1(vbfloat16m1x2_t src, size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m1x3_bf16m1(vbfloat16m1x3_t src, size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m1x4_bf16m1(vbfloat16m1x4_t src, size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m1x5_bf16m1(vbfloat16m1x5_t src, size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m1x6_bf16m1(vbfloat16m1x6_t src, size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m1x7_bf16m1(vbfloat16m1x7_t src, size_t index);
vbfloat16m1_t __riscv_vget_v_bf16m1x8_bf16m1(vbfloat16m1x8_t src, size_t index);
vbfloat16m2_t __riscv_vget_v_bf16m2x2_bf16m2(vbfloat16m2x2_t src, size_t index);
vbfloat16m2_t __riscv_vget_v_bf16m2x3_bf16m2(vbfloat16m2x3_t src, size_t index);
vbfloat16m2_t __riscv_vget_v_bf16m2x4_bf16m2(vbfloat16m2x4_t src, size_t index);
vbfloat16m4_t __riscv_vget_v_bf16m4x2_bf16m4(vbfloat16m4x2_t src, size_t index);
----

[[vector-creation]]
==== Vector Creation Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16m2_t __riscv_vcreate_v_bf16m1_bf16m2(vbfloat16m1_t v0,
                                              vbfloat16m1_t v1);
vbfloat16m4_t __riscv_vcreate_v_bf16m1_bf16m4(vbfloat16m1_t v0,
                                              vbfloat16m1_t v1,
                                              vbfloat16m1_t v2,
                                              vbfloat16m1_t v3);
vbfloat16m8_t __riscv_vcreate_v_bf16m1_bf16m8(
    vbfloat16m1_t v0, vbfloat16m1_t v1, vbfloat16m1_t v2, vbfloat16m1_t v3,
    vbfloat16m1_t v4, vbfloat16m1_t v5, vbfloat16m1_t v6, vbfloat16m1_t v7);
vbfloat16m4_t __riscv_vcreate_v_bf16m2_bf16m4(vbfloat16m2_t v0,
                                              vbfloat16m2_t v1);
vbfloat16m8_t __riscv_vcreate_v_bf16m2_bf16m8(vbfloat16m2_t v0,
                                              vbfloat16m2_t v1,
                                              vbfloat16m2_t v2,
                                              vbfloat16m2_t v3);
vbfloat16m8_t __riscv_vcreate_v_bf16m4_bf16m8(vbfloat16m4_t v0,
                                              vbfloat16m4_t v1);
vbfloat16mf4x2_t __riscv_vcreate_v_bf16mf4x2(vbfloat16mf4_t v0,
                                             vbfloat16mf4_t v1);
vbfloat16mf4x3_t __riscv_vcreate_v_bf16mf4x3(vbfloat16mf4_t v0,
                                             vbfloat16mf4_t v1,
                                             vbfloat16mf4_t v2);
vbfloat16mf4x4_t __riscv_vcreate_v_bf16mf4x4(vbfloat16mf4_t v0,
                                             vbfloat16mf4_t v1,
                                             vbfloat16mf4_t v2,
                                             vbfloat16mf4_t v3);
vbfloat16mf4x5_t __riscv_vcreate_v_bf16mf4x5(vbfloat16mf4_t v0,
                                             vbfloat16mf4_t v1,
                                             vbfloat16mf4_t v2,
                                             vbfloat16mf4_t v3,
                                             vbfloat16mf4_t v4);
vbfloat16mf4x6_t
__riscv_vcreate_v_bf16mf4x6(vbfloat16mf4_t v0, vbfloat16mf4_t v1,
                            vbfloat16mf4_t v2, vbfloat16mf4_t v3,
                            vbfloat16mf4_t v4, vbfloat16mf4_t v5);
vbfloat16mf4x7_t __riscv_vcreate_v_bf16mf4x7(
    vbfloat16mf4_t v0, vbfloat16mf4_t v1, vbfloat16mf4_t v2, vbfloat16mf4_t v3,
    vbfloat16mf4_t v4, vbfloat16mf4_t v5, vbfloat16mf4_t v6);
vbfloat16mf4x8_t __riscv_vcreate_v_bf16mf4x8(
    vbfloat16mf4_t v0, vbfloat16mf4_t v1, vbfloat16mf4_t v2, vbfloat16mf4_t v3,
    vbfloat16mf4_t v4, vbfloat16mf4_t v5, vbfloat16mf4_t v6, vbfloat16mf4_t v7);
vbfloat16mf2x2_t __riscv_vcreate_v_bf16mf2x2(vbfloat16mf2_t v0,
                                             vbfloat16mf2_t v1);
vbfloat16mf2x3_t __riscv_vcreate_v_bf16mf2x3(vbfloat16mf2_t v0,
                                             vbfloat16mf2_t v1,
                                             vbfloat16mf2_t v2);
vbfloat16mf2x4_t __riscv_vcreate_v_bf16mf2x4(vbfloat16mf2_t v0,
                                             vbfloat16mf2_t v1,
                                             vbfloat16mf2_t v2,
                                             vbfloat16mf2_t v3);
vbfloat16mf2x5_t __riscv_vcreate_v_bf16mf2x5(vbfloat16mf2_t v0,
                                             vbfloat16mf2_t v1,
                                             vbfloat16mf2_t v2,
                                             vbfloat16mf2_t v3,
                                             vbfloat16mf2_t v4);
vbfloat16mf2x6_t
__riscv_vcreate_v_bf16mf2x6(vbfloat16mf2_t v0, vbfloat16mf2_t v1,
                            vbfloat16mf2_t v2, vbfloat16mf2_t v3,
                            vbfloat16mf2_t v4, vbfloat16mf2_t v5);
vbfloat16mf2x7_t __riscv_vcreate_v_bf16mf2x7(
    vbfloat16mf2_t v0, vbfloat16mf2_t v1, vbfloat16mf2_t v2, vbfloat16mf2_t v3,
    vbfloat16mf2_t v4, vbfloat16mf2_t v5, vbfloat16mf2_t v6);
vbfloat16mf2x8_t __riscv_vcreate_v_bf16mf2x8(
    vbfloat16mf2_t v0, vbfloat16mf2_t v1, vbfloat16mf2_t v2, vbfloat16mf2_t v3,
    vbfloat16mf2_t v4, vbfloat16mf2_t v5, vbfloat16mf2_t v6, vbfloat16mf2_t v7);
vbfloat16m1x2_t __riscv_vcreate_v_bf16m1x2(vbfloat16m1_t v0, vbfloat16m1_t v1);
vbfloat16m1x3_t __riscv_vcreate_v_bf16m1x3(vbfloat16m1_t v0, vbfloat16m1_t v1,
                                           vbfloat16m1_t v2);
vbfloat16m1x4_t __riscv_vcreate_v_bf16m1x4(vbfloat16m1_t v0, vbfloat16m1_t v1,
                                           vbfloat16m1_t v2, vbfloat16m1_t v3);
vbfloat16m1x5_t __riscv_vcreate_v_bf16m1x5(vbfloat16m1_t v0, vbfloat16m1_t v1,
                                           vbfloat16m1_t v2, vbfloat16m1_t v3,
                                           vbfloat16m1_t v4);
vbfloat16m1x6_t __riscv_vcreate_v_bf16m1x6(vbfloat16m1_t v0, vbfloat16m1_t v1,
                                           vbfloat16m1_t v2, vbfloat16m1_t v3,
                                           vbfloat16m1_t v4, vbfloat16m1_t v5);
vbfloat16m1x7_t __riscv_vcreate_v_bf16m1x7(vbfloat16m1_t v0, vbfloat16m1_t v1,
                                           vbfloat16m1_t v2, vbfloat16m1_t v3,
                                           vbfloat16m1_t v4, vbfloat16m1_t v5,
                                           vbfloat16m1_t v6);
vbfloat16m1x8_t __riscv_vcreate_v_bf16m1x8(vbfloat16m1_t v0, vbfloat16m1_t v1,
                                           vbfloat16m1_t v2, vbfloat16m1_t v3,
                                           vbfloat16m1_t v4, vbfloat16m1_t v5,
                                           vbfloat16m1_t v6, vbfloat16m1_t v7);
vbfloat16m2x2_t __riscv_vcreate_v_bf16m2x2(vbfloat16m2_t v0, vbfloat16m2_t v1);
vbfloat16m2x3_t __riscv_vcreate_v_bf16m2x3(vbfloat16m2_t v0, vbfloat16m2_t v1,
                                           vbfloat16m2_t v2);
vbfloat16m2x4_t __riscv_vcreate_v_bf16m2x4(vbfloat16m2_t v0, vbfloat16m2_t v1,
                                           vbfloat16m2_t v2, vbfloat16m2_t v3);
vbfloat16m4x2_t __riscv_vcreate_v_bf16m4x2(vbfloat16m4_t v0, vbfloat16m4_t v1);
----

=== Andes Vector Quad-Widening Integer Multiply-Add Extension(XAndesVQMac)

[[vector-quad-widening-integer-multiply-add-operations]]
==== Vector Quad-Widening Integer Multiply-Add Intrinsics(XAndesVQMac)

[,c]
----
vint32mf2_t __riscv_nds_vqmacc_vv_i32mf2(vint32mf2_t vd, vint8mf8_t vs1,
                                         vint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmacc_vx_i32mf2(vint32mf2_t vd, int8_t rs1,
                                         vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc_vv_i32m1(vint32m1_t vd, vint8mf4_t vs1,
                                       vint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc_vx_i32m1(vint32m1_t vd, int8_t rs1,
                                       vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_vv_i32m2(vint32m2_t vd, vint8mf2_t vs1,
                                       vint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_vx_i32m2(vint32m2_t vd, int8_t rs1,
                                       vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_vv_i32m4(vint32m4_t vd, vint8m1_t vs1,
                                       vint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_vx_i32m4(vint32m4_t vd, int8_t rs1, vint8m1_t vs2,
                                       size_t vl);
vint32m8_t __riscv_nds_vqmacc_vv_i32m8(vint32m8_t vd, vint8m2_t vs1,
                                       vint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc_vx_i32m8(vint32m8_t vd, int8_t rs1, vint8m2_t vs2,
                                       size_t vl);
vint64m1_t __riscv_nds_vqmacc_vv_i64m1(vint64m1_t vd, vint16mf4_t vs1,
                                       vint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_vx_i64m1(vint64m1_t vd, int16_t rs1,
                                       vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_vv_i64m2(vint64m2_t vd, vint16mf2_t vs1,
                                       vint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_vx_i64m2(vint64m2_t vd, int16_t rs1,
                                       vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc_vv_i64m4(vint64m4_t vd, vint16m1_t vs1,
                                       vint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc_vx_i64m4(vint64m4_t vd, int16_t rs1,
                                       vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc_vv_i64m8(vint64m8_t vd, vint16m2_t vs1,
                                       vint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc_vx_i64m8(vint64m8_t vd, int16_t rs1,
                                       vint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_vv_i32mf2(vint32mf2_t vd, vint8mf8_t vs1,
                                           vuint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_vx_i32mf2(vint32mf2_t vd, int8_t rs1,
                                           vuint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_vv_i32m1(vint32m1_t vd, vint8mf4_t vs1,
                                         vuint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_vx_i32m1(vint32m1_t vd, int8_t rs1,
                                         vuint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_vv_i32m2(vint32m2_t vd, vint8mf2_t vs1,
                                         vuint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_vx_i32m2(vint32m2_t vd, int8_t rs1,
                                         vuint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_vv_i32m4(vint32m4_t vd, vint8m1_t vs1,
                                         vuint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_vx_i32m4(vint32m4_t vd, int8_t rs1,
                                         vuint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_vv_i32m8(vint32m8_t vd, vint8m2_t vs1,
                                         vuint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_vx_i32m8(vint32m8_t vd, int8_t rs1,
                                         vuint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_vv_i64m1(vint64m1_t vd, vint16mf4_t vs1,
                                         vuint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_vx_i64m1(vint64m1_t vd, int16_t rs1,
                                         vuint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_vv_i64m2(vint64m2_t vd, vint16mf2_t vs1,
                                         vuint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_vx_i64m2(vint64m2_t vd, int16_t rs1,
                                         vuint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_vv_i64m4(vint64m4_t vd, vint16m1_t vs1,
                                         vuint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_vx_i64m4(vint64m4_t vd, int16_t rs1,
                                         vuint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_vv_i64m8(vint64m8_t vd, vint16m2_t vs1,
                                         vuint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_vx_i64m8(vint64m8_t vd, int16_t rs1,
                                         vuint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccus_vx_i32mf2(vint32mf2_t vd, uint8_t rs1,
                                           vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccus_vx_i32m1(vint32m1_t vd, uint8_t rs1,
                                         vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccus_vx_i32m2(vint32m2_t vd, uint8_t rs1,
                                         vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccus_vx_i32m4(vint32m4_t vd, uint8_t rs1,
                                         vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccus_vx_i32m8(vint32m8_t vd, uint8_t rs1,
                                         vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccus_vx_i64m1(vint64m1_t vd, uint16_t rs1,
                                         vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccus_vx_i64m2(vint64m2_t vd, uint16_t rs1,
                                         vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccus_vx_i64m4(vint64m4_t vd, uint16_t rs1,
                                         vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccus_vx_i64m8(vint64m8_t vd, uint16_t rs1,
                                         vint16m2_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_vv_u32mf2(vuint32mf2_t vd, vuint8mf8_t vs1,
                                           vuint8mf8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_vx_u32mf2(vuint32mf2_t vd, uint8_t rs1,
                                           vuint8mf8_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_vv_u32m1(vuint32m1_t vd, vuint8mf4_t vs1,
                                         vuint8mf4_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_vx_u32m1(vuint32m1_t vd, uint8_t rs1,
                                         vuint8mf4_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_vv_u32m2(vuint32m2_t vd, vuint8mf2_t vs1,
                                         vuint8mf2_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_vx_u32m2(vuint32m2_t vd, uint8_t rs1,
                                         vuint8mf2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_vv_u32m4(vuint32m4_t vd, vuint8m1_t vs1,
                                         vuint8m1_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_vx_u32m4(vuint32m4_t vd, uint8_t rs1,
                                         vuint8m1_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_vv_u32m8(vuint32m8_t vd, vuint8m2_t vs1,
                                         vuint8m2_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_vx_u32m8(vuint32m8_t vd, uint8_t rs1,
                                         vuint8m2_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_vv_u64m1(vuint64m1_t vd, vuint16mf4_t vs1,
                                         vuint16mf4_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_vx_u64m1(vuint64m1_t vd, uint16_t rs1,
                                         vuint16mf4_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_vv_u64m2(vuint64m2_t vd, vuint16mf2_t vs1,
                                         vuint16mf2_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_vx_u64m2(vuint64m2_t vd, uint16_t rs1,
                                         vuint16mf2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_vv_u64m4(vuint64m4_t vd, vuint16m1_t vs1,
                                         vuint16m1_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_vx_u64m4(vuint64m4_t vd, uint16_t rs1,
                                         vuint16m1_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_vv_u64m8(vuint64m8_t vd, vuint16m2_t vs1,
                                         vuint16m2_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_vx_u64m8(vuint64m8_t vd, uint16_t rs1,
                                         vuint16m2_t vs2, size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vqmacc_vv_i32mf2_m(vbool64_t vm, vint32mf2_t vd,
                                           vint8mf8_t vs1, vint8mf8_t vs2,
                                           size_t vl);
vint32mf2_t __riscv_nds_vqmacc_vx_i32mf2_m(vbool64_t vm, vint32mf2_t vd,
                                           int8_t rs1, vint8mf8_t vs2,
                                           size_t vl);
vint32m1_t __riscv_nds_vqmacc_vv_i32m1_m(vbool32_t vm, vint32m1_t vd,
                                         vint8mf4_t vs1, vint8mf4_t vs2,
                                         size_t vl);
vint32m1_t __riscv_nds_vqmacc_vx_i32m1_m(vbool32_t vm, vint32m1_t vd,
                                         int8_t rs1, vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_vv_i32m2_m(vbool16_t vm, vint32m2_t vd,
                                         vint8mf2_t vs1, vint8mf2_t vs2,
                                         size_t vl);
vint32m2_t __riscv_nds_vqmacc_vx_i32m2_m(vbool16_t vm, vint32m2_t vd,
                                         int8_t rs1, vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_vv_i32m4_m(vbool8_t vm, vint32m4_t vd,
                                         vint8m1_t vs1, vint8m1_t vs2,
                                         size_t vl);
vint32m4_t __riscv_nds_vqmacc_vx_i32m4_m(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                                         vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc_vv_i32m8_m(vbool4_t vm, vint32m8_t vd,
                                         vint8m2_t vs1, vint8m2_t vs2,
                                         size_t vl);
vint32m8_t __riscv_nds_vqmacc_vx_i32m8_m(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                                         vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_vv_i64m1_m(vbool64_t vm, vint64m1_t vd,
                                         vint16mf4_t vs1, vint16mf4_t vs2,
                                         size_t vl);
vint64m1_t __riscv_nds_vqmacc_vx_i64m1_m(vbool64_t vm, vint64m1_t vd,
                                         int16_t rs1, vint16mf4_t vs2,
                                         size_t vl);
vint64m2_t __riscv_nds_vqmacc_vv_i64m2_m(vbool32_t vm, vint64m2_t vd,
                                         vint16mf2_t vs1, vint16mf2_t vs2,
                                         size_t vl);
vint64m2_t __riscv_nds_vqmacc_vx_i64m2_m(vbool32_t vm, vint64m2_t vd,
                                         int16_t rs1, vint16mf2_t vs2,
                                         size_t vl);
vint64m4_t __riscv_nds_vqmacc_vv_i64m4_m(vbool16_t vm, vint64m4_t vd,
                                         vint16m1_t vs1, vint16m1_t vs2,
                                         size_t vl);
vint64m4_t __riscv_nds_vqmacc_vx_i64m4_m(vbool16_t vm, vint64m4_t vd,
                                         int16_t rs1, vint16m1_t vs2,
                                         size_t vl);
vint64m8_t __riscv_nds_vqmacc_vv_i64m8_m(vbool8_t vm, vint64m8_t vd,
                                         vint16m2_t vs1, vint16m2_t vs2,
                                         size_t vl);
vint64m8_t __riscv_nds_vqmacc_vx_i64m8_m(vbool8_t vm, vint64m8_t vd,
                                         int16_t rs1, vint16m2_t vs2,
                                         size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_vv_i32mf2_m(vbool64_t vm, vint32mf2_t vd,
                                             vint8mf8_t vs1, vuint8mf8_t vs2,
                                             size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_vx_i32mf2_m(vbool64_t vm, vint32mf2_t vd,
                                             int8_t rs1, vuint8mf8_t vs2,
                                             size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_vv_i32m1_m(vbool32_t vm, vint32m1_t vd,
                                           vint8mf4_t vs1, vuint8mf4_t vs2,
                                           size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_vx_i32m1_m(vbool32_t vm, vint32m1_t vd,
                                           int8_t rs1, vuint8mf4_t vs2,
                                           size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_vv_i32m2_m(vbool16_t vm, vint32m2_t vd,
                                           vint8mf2_t vs1, vuint8mf2_t vs2,
                                           size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_vx_i32m2_m(vbool16_t vm, vint32m2_t vd,
                                           int8_t rs1, vuint8mf2_t vs2,
                                           size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_vv_i32m4_m(vbool8_t vm, vint32m4_t vd,
                                           vint8m1_t vs1, vuint8m1_t vs2,
                                           size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_vx_i32m4_m(vbool8_t vm, vint32m4_t vd,
                                           int8_t rs1, vuint8m1_t vs2,
                                           size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_vv_i32m8_m(vbool4_t vm, vint32m8_t vd,
                                           vint8m2_t vs1, vuint8m2_t vs2,
                                           size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_vx_i32m8_m(vbool4_t vm, vint32m8_t vd,
                                           int8_t rs1, vuint8m2_t vs2,
                                           size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_vv_i64m1_m(vbool64_t vm, vint64m1_t vd,
                                           vint16mf4_t vs1, vuint16mf4_t vs2,
                                           size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_vx_i64m1_m(vbool64_t vm, vint64m1_t vd,
                                           int16_t rs1, vuint16mf4_t vs2,
                                           size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_vv_i64m2_m(vbool32_t vm, vint64m2_t vd,
                                           vint16mf2_t vs1, vuint16mf2_t vs2,
                                           size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_vx_i64m2_m(vbool32_t vm, vint64m2_t vd,
                                           int16_t rs1, vuint16mf2_t vs2,
                                           size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_vv_i64m4_m(vbool16_t vm, vint64m4_t vd,
                                           vint16m1_t vs1, vuint16m1_t vs2,
                                           size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_vx_i64m4_m(vbool16_t vm, vint64m4_t vd,
                                           int16_t rs1, vuint16m1_t vs2,
                                           size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_vv_i64m8_m(vbool8_t vm, vint64m8_t vd,
                                           vint16m2_t vs1, vuint16m2_t vs2,
                                           size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_vx_i64m8_m(vbool8_t vm, vint64m8_t vd,
                                           int16_t rs1, vuint16m2_t vs2,
                                           size_t vl);
vint32mf2_t __riscv_nds_vqmaccus_vx_i32mf2_m(vbool64_t vm, vint32mf2_t vd,
                                             uint8_t rs1, vint8mf8_t vs2,
                                             size_t vl);
vint32m1_t __riscv_nds_vqmaccus_vx_i32m1_m(vbool32_t vm, vint32m1_t vd,
                                           uint8_t rs1, vint8mf4_t vs2,
                                           size_t vl);
vint32m2_t __riscv_nds_vqmaccus_vx_i32m2_m(vbool16_t vm, vint32m2_t vd,
                                           uint8_t rs1, vint8mf2_t vs2,
                                           size_t vl);
vint32m4_t __riscv_nds_vqmaccus_vx_i32m4_m(vbool8_t vm, vint32m4_t vd,
                                           uint8_t rs1, vint8m1_t vs2,
                                           size_t vl);
vint32m8_t __riscv_nds_vqmaccus_vx_i32m8_m(vbool4_t vm, vint32m8_t vd,
                                           uint8_t rs1, vint8m2_t vs2,
                                           size_t vl);
vint64m1_t __riscv_nds_vqmaccus_vx_i64m1_m(vbool64_t vm, vint64m1_t vd,
                                           uint16_t rs1, vint16mf4_t vs2,
                                           size_t vl);
vint64m2_t __riscv_nds_vqmaccus_vx_i64m2_m(vbool32_t vm, vint64m2_t vd,
                                           uint16_t rs1, vint16mf2_t vs2,
                                           size_t vl);
vint64m4_t __riscv_nds_vqmaccus_vx_i64m4_m(vbool16_t vm, vint64m4_t vd,
                                           uint16_t rs1, vint16m1_t vs2,
                                           size_t vl);
vint64m8_t __riscv_nds_vqmaccus_vx_i64m8_m(vbool8_t vm, vint64m8_t vd,
                                           uint16_t rs1, vint16m2_t vs2,
                                           size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_vv_u32mf2_m(vbool64_t vm, vuint32mf2_t vd,
                                             vuint8mf8_t vs1, vuint8mf8_t vs2,
                                             size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_vx_u32mf2_m(vbool64_t vm, vuint32mf2_t vd,
                                             uint8_t rs1, vuint8mf8_t vs2,
                                             size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_vv_u32m1_m(vbool32_t vm, vuint32m1_t vd,
                                           vuint8mf4_t vs1, vuint8mf4_t vs2,
                                           size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_vx_u32m1_m(vbool32_t vm, vuint32m1_t vd,
                                           uint8_t rs1, vuint8mf4_t vs2,
                                           size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_vv_u32m2_m(vbool16_t vm, vuint32m2_t vd,
                                           vuint8mf2_t vs1, vuint8mf2_t vs2,
                                           size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_vx_u32m2_m(vbool16_t vm, vuint32m2_t vd,
                                           uint8_t rs1, vuint8mf2_t vs2,
                                           size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_vv_u32m4_m(vbool8_t vm, vuint32m4_t vd,
                                           vuint8m1_t vs1, vuint8m1_t vs2,
                                           size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_vx_u32m4_m(vbool8_t vm, vuint32m4_t vd,
                                           uint8_t rs1, vuint8m1_t vs2,
                                           size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_vv_u32m8_m(vbool4_t vm, vuint32m8_t vd,
                                           vuint8m2_t vs1, vuint8m2_t vs2,
                                           size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_vx_u32m8_m(vbool4_t vm, vuint32m8_t vd,
                                           uint8_t rs1, vuint8m2_t vs2,
                                           size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_vv_u64m1_m(vbool64_t vm, vuint64m1_t vd,
                                           vuint16mf4_t vs1, vuint16mf4_t vs2,
                                           size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_vx_u64m1_m(vbool64_t vm, vuint64m1_t vd,
                                           uint16_t rs1, vuint16mf4_t vs2,
                                           size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_vv_u64m2_m(vbool32_t vm, vuint64m2_t vd,
                                           vuint16mf2_t vs1, vuint16mf2_t vs2,
                                           size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_vx_u64m2_m(vbool32_t vm, vuint64m2_t vd,
                                           uint16_t rs1, vuint16mf2_t vs2,
                                           size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_vv_u64m4_m(vbool16_t vm, vuint64m4_t vd,
                                           vuint16m1_t vs1, vuint16m1_t vs2,
                                           size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_vx_u64m4_m(vbool16_t vm, vuint64m4_t vd,
                                           uint16_t rs1, vuint16m1_t vs2,
                                           size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_vv_u64m8_m(vbool8_t vm, vuint64m8_t vd,
                                           vuint16m2_t vs1, vuint16m2_t vs2,
                                           size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_vx_u64m8_m(vbool8_t vm, vuint64m8_t vd,
                                           uint16_t rs1, vuint16m2_t vs2,
                                           size_t vl);
----

=== Andes Vector Dot Product Extension(XAndesVDot)

[[vector-dot-product-operations]]
==== Vector Dot Product Intrinsics(XAndesVDot)

[,c]
----
vint32mf2_t __riscv_nds_vd4dots_vv_i32mf2(vint32mf2_t vd, vint8mf2_t vs1,
                                          vint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dots_vv_i32m1(vint32m1_t vd, vint8m1_t vs1,
                                        vint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dots_vv_i32m2(vint32m2_t vd, vint8m2_t vs1,
                                        vint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dots_vv_i32m4(vint32m4_t vd, vint8m4_t vs1,
                                        vint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dots_vv_i32m8(vint32m8_t vd, vint8m8_t vs1,
                                        vint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dots_vv_i64m1(vint64m1_t vd, vint16m1_t vs1,
                                        vint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dots_vv_i64m2(vint64m2_t vd, vint16m2_t vs1,
                                        vint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dots_vv_i64m4(vint64m4_t vd, vint16m4_t vs1,
                                        vint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dots_vv_i64m8(vint64m8_t vd, vint16m8_t vs1,
                                        vint16m8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vd4dotsu_vv_i32mf2(vint32mf2_t vd, vint8mf2_t vs1,
                                           vuint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dotsu_vv_i32m1(vint32m1_t vd, vint8m1_t vs1,
                                         vuint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dotsu_vv_i32m2(vint32m2_t vd, vint8m2_t vs1,
                                         vuint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dotsu_vv_i32m4(vint32m4_t vd, vint8m4_t vs1,
                                         vuint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dotsu_vv_i32m8(vint32m8_t vd, vint8m8_t vs1,
                                         vuint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dotsu_vv_i64m1(vint64m1_t vd, vint16m1_t vs1,
                                         vuint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dotsu_vv_i64m2(vint64m2_t vd, vint16m2_t vs1,
                                         vuint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dotsu_vv_i64m4(vint64m4_t vd, vint16m4_t vs1,
                                         vuint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dotsu_vv_i64m8(vint64m8_t vd, vint16m8_t vs1,
                                         vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vd4dotu_vv_u32mf2(vuint32mf2_t vd, vuint8mf2_t vs1,
                                           vuint8mf2_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vd4dotu_vv_u32m1(vuint32m1_t vd, vuint8m1_t vs1,
                                         vuint8m1_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vd4dotu_vv_u32m2(vuint32m2_t vd, vuint8m2_t vs1,
                                         vuint8m2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vd4dotu_vv_u32m4(vuint32m4_t vd, vuint8m4_t vs1,
                                         vuint8m4_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vd4dotu_vv_u32m8(vuint32m8_t vd, vuint8m8_t vs1,
                                         vuint8m8_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vd4dotu_vv_u64m1(vuint64m1_t vd, vuint16m1_t vs1,
                                         vuint16m1_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vd4dotu_vv_u64m2(vuint64m2_t vd, vuint16m2_t vs1,
                                         vuint16m2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vd4dotu_vv_u64m4(vuint64m4_t vd, vuint16m4_t vs1,
                                         vuint16m4_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vd4dotu_vv_u64m8(vuint64m8_t vd, vuint16m8_t vs1,
                                         vuint16m8_t vs2, size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vd4dots_vv_i32mf2_m(vbool64_t vm, vint32mf2_t vd,
                                            vint8mf2_t vs1, vint8mf2_t vs2,
                                            size_t vl);
vint32m1_t __riscv_nds_vd4dots_vv_i32m1_m(vbool32_t vm, vint32m1_t vd,
                                          vint8m1_t vs1, vint8m1_t vs2,
                                          size_t vl);
vint32m2_t __riscv_nds_vd4dots_vv_i32m2_m(vbool16_t vm, vint32m2_t vd,
                                          vint8m2_t vs1, vint8m2_t vs2,
                                          size_t vl);
vint32m4_t __riscv_nds_vd4dots_vv_i32m4_m(vbool8_t vm, vint32m4_t vd,
                                          vint8m4_t vs1, vint8m4_t vs2,
                                          size_t vl);
vint32m8_t __riscv_nds_vd4dots_vv_i32m8_m(vbool4_t vm, vint32m8_t vd,
                                          vint8m8_t vs1, vint8m8_t vs2,
                                          size_t vl);
vint64m1_t __riscv_nds_vd4dots_vv_i64m1_m(vbool64_t vm, vint64m1_t vd,
                                          vint16m1_t vs1, vint16m1_t vs2,
                                          size_t vl);
vint64m2_t __riscv_nds_vd4dots_vv_i64m2_m(vbool32_t vm, vint64m2_t vd,
                                          vint16m2_t vs1, vint16m2_t vs2,
                                          size_t vl);
vint64m4_t __riscv_nds_vd4dots_vv_i64m4_m(vbool16_t vm, vint64m4_t vd,
                                          vint16m4_t vs1, vint16m4_t vs2,
                                          size_t vl);
vint64m8_t __riscv_nds_vd4dots_vv_i64m8_m(vbool8_t vm, vint64m8_t vd,
                                          vint16m8_t vs1, vint16m8_t vs2,
                                          size_t vl);
vint32mf2_t __riscv_nds_vd4dotsu_vv_i32mf2_m(vbool64_t vm, vint32mf2_t vd,
                                             vint8mf2_t vs1, vuint8mf2_t vs2,
                                             size_t vl);
vint32m1_t __riscv_nds_vd4dotsu_vv_i32m1_m(vbool32_t vm, vint32m1_t vd,
                                           vint8m1_t vs1, vuint8m1_t vs2,
                                           size_t vl);
vint32m2_t __riscv_nds_vd4dotsu_vv_i32m2_m(vbool16_t vm, vint32m2_t vd,
                                           vint8m2_t vs1, vuint8m2_t vs2,
                                           size_t vl);
vint32m4_t __riscv_nds_vd4dotsu_vv_i32m4_m(vbool8_t vm, vint32m4_t vd,
                                           vint8m4_t vs1, vuint8m4_t vs2,
                                           size_t vl);
vint32m8_t __riscv_nds_vd4dotsu_vv_i32m8_m(vbool4_t vm, vint32m8_t vd,
                                           vint8m8_t vs1, vuint8m8_t vs2,
                                           size_t vl);
vint64m1_t __riscv_nds_vd4dotsu_vv_i64m1_m(vbool64_t vm, vint64m1_t vd,
                                           vint16m1_t vs1, vuint16m1_t vs2,
                                           size_t vl);
vint64m2_t __riscv_nds_vd4dotsu_vv_i64m2_m(vbool32_t vm, vint64m2_t vd,
                                           vint16m2_t vs1, vuint16m2_t vs2,
                                           size_t vl);
vint64m4_t __riscv_nds_vd4dotsu_vv_i64m4_m(vbool16_t vm, vint64m4_t vd,
                                           vint16m4_t vs1, vuint16m4_t vs2,
                                           size_t vl);
vint64m8_t __riscv_nds_vd4dotsu_vv_i64m8_m(vbool8_t vm, vint64m8_t vd,
                                           vint16m8_t vs1, vuint16m8_t vs2,
                                           size_t vl);
vuint32mf2_t __riscv_nds_vd4dotu_vv_u32mf2_m(vbool64_t vm, vuint32mf2_t vd,
                                             vuint8mf2_t vs1, vuint8mf2_t vs2,
                                             size_t vl);
vuint32m1_t __riscv_nds_vd4dotu_vv_u32m1_m(vbool32_t vm, vuint32m1_t vd,
                                           vuint8m1_t vs1, vuint8m1_t vs2,
                                           size_t vl);
vuint32m2_t __riscv_nds_vd4dotu_vv_u32m2_m(vbool16_t vm, vuint32m2_t vd,
                                           vuint8m2_t vs1, vuint8m2_t vs2,
                                           size_t vl);
vuint32m4_t __riscv_nds_vd4dotu_vv_u32m4_m(vbool8_t vm, vuint32m4_t vd,
                                           vuint8m4_t vs1, vuint8m4_t vs2,
                                           size_t vl);
vuint32m8_t __riscv_nds_vd4dotu_vv_u32m8_m(vbool4_t vm, vuint32m8_t vd,
                                           vuint8m8_t vs1, vuint8m8_t vs2,
                                           size_t vl);
vuint64m1_t __riscv_nds_vd4dotu_vv_u64m1_m(vbool64_t vm, vuint64m1_t vd,
                                           vuint16m1_t vs1, vuint16m1_t vs2,
                                           size_t vl);
vuint64m2_t __riscv_nds_vd4dotu_vv_u64m2_m(vbool32_t vm, vuint64m2_t vd,
                                           vuint16m2_t vs1, vuint16m2_t vs2,
                                           size_t vl);
vuint64m4_t __riscv_nds_vd4dotu_vv_u64m4_m(vbool16_t vm, vuint64m4_t vd,
                                           vuint16m4_t vs1, vuint16m4_t vs2,
                                           size_t vl);
vuint64m8_t __riscv_nds_vd4dotu_vv_u64m8_m(vbool8_t vm, vuint64m8_t vd,
                                           vuint16m8_t vs1, vuint16m8_t vs2,
                                           size_t vl);
----

=== Andes Vector Packed FP16 Extension(XAndesVPackFPH)

[[vector-packed-fp16-operations]]
==== Vector Packed FP16 Intrinsics(XAndesVPackFPH)

[,c]
----
vfloat16mf4_t __riscv_nds_vfpmadt_vf_f16mf4(vfloat16mf4_t vs2, float32_t rs1,
                                            size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_vf_f16mf2(vfloat16mf2_t vs2, float32_t rs1,
                                            size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_vf_f16m1(vfloat16m1_t vs2, float32_t rs1,
                                          size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_vf_f16m2(vfloat16m2_t vs2, float32_t rs1,
                                          size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_vf_f16m4(vfloat16m4_t vs2, float32_t rs1,
                                          size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_vf_f16m8(vfloat16m8_t vs2, float32_t rs1,
                                          size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_vf_f16mf4(vfloat16mf4_t vs2, float32_t rs1,
                                            size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_vf_f16mf2(vfloat16mf2_t vs2, float32_t rs1,
                                            size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_vf_f16m1(vfloat16m1_t vs2, float32_t rs1,
                                          size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_vf_f16m2(vfloat16m2_t vs2, float32_t rs1,
                                          size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_vf_f16m4(vfloat16m4_t vs2, float32_t rs1,
                                          size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_vf_f16m8(vfloat16m8_t vs2, float32_t rs1,
                                          size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt_vf_f16mf4_m(vbool64_t vm, vfloat16mf4_t vs2,
                                              float32_t rs1, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_vf_f16mf2_m(vbool32_t vm, vfloat16mf2_t vs2,
                                              float32_t rs1, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_vf_f16m1_m(vbool16_t vm, vfloat16m1_t vs2,
                                            float32_t rs1, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_vf_f16m2_m(vbool8_t vm, vfloat16m2_t vs2,
                                            float32_t rs1, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_vf_f16m4_m(vbool4_t vm, vfloat16m4_t vs2,
                                            float32_t rs1, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_vf_f16m8_m(vbool2_t vm, vfloat16m8_t vs2,
                                            float32_t rs1, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_vf_f16mf4_m(vbool64_t vm, vfloat16mf4_t vs2,
                                              float32_t rs1, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_vf_f16mf2_m(vbool32_t vm, vfloat16mf2_t vs2,
                                              float32_t rs1, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_vf_f16m1_m(vbool16_t vm, vfloat16m1_t vs2,
                                            float32_t rs1, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_vf_f16m2_m(vbool8_t vm, vfloat16m2_t vs2,
                                            float32_t rs1, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_vf_f16m4_m(vbool4_t vm, vfloat16m4_t vs2,
                                            float32_t rs1, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_vf_f16m8_m(vbool2_t vm, vfloat16m8_t vs2,
                                            float32_t rs1, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadt_vf_f16mf4_rm(vfloat16mf4_t vs2, float32_t rs1,
                                               unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_vf_f16mf2_rm(vfloat16mf2_t vs2, float32_t rs1,
                                               unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_vf_f16m1_rm(vfloat16m1_t vs2, float32_t rs1,
                                             unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_vf_f16m2_rm(vfloat16m2_t vs2, float32_t rs1,
                                             unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_vf_f16m4_rm(vfloat16m4_t vs2, float32_t rs1,
                                             unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_vf_f16m8_rm(vfloat16m8_t vs2, float32_t rs1,
                                             unsigned int frm, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_vf_f16mf4_rm(vfloat16mf4_t vs2, float32_t rs1,
                                               unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_vf_f16mf2_rm(vfloat16mf2_t vs2, float32_t rs1,
                                               unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_vf_f16m1_rm(vfloat16m1_t vs2, float32_t rs1,
                                             unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_vf_f16m2_rm(vfloat16m2_t vs2, float32_t rs1,
                                             unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_vf_f16m4_rm(vfloat16m4_t vs2, float32_t rs1,
                                             unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_vf_f16m8_rm(vfloat16m8_t vs2, float32_t rs1,
                                             unsigned int frm, size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt_vf_f16mf4_rm_m(vbool64_t vm,
                                                 vfloat16mf4_t vs2,
                                                 float32_t rs1,
                                                 unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_vf_f16mf2_rm_m(vbool32_t vm,
                                                 vfloat16mf2_t vs2,
                                                 float32_t rs1,
                                                 unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_vf_f16m1_rm_m(vbool16_t vm, vfloat16m1_t vs2,
                                               float32_t rs1, unsigned int frm,
                                               size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_vf_f16m2_rm_m(vbool8_t vm, vfloat16m2_t vs2,
                                               float32_t rs1, unsigned int frm,
                                               size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_vf_f16m4_rm_m(vbool4_t vm, vfloat16m4_t vs2,
                                               float32_t rs1, unsigned int frm,
                                               size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_vf_f16m8_rm_m(vbool2_t vm, vfloat16m8_t vs2,
                                               float32_t rs1, unsigned int frm,
                                               size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_vf_f16mf4_rm_m(vbool64_t vm,
                                                 vfloat16mf4_t vs2,
                                                 float32_t rs1,
                                                 unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_vf_f16mf2_rm_m(vbool32_t vm,
                                                 vfloat16mf2_t vs2,
                                                 float32_t rs1,
                                                 unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_vf_f16m1_rm_m(vbool16_t vm, vfloat16m1_t vs2,
                                               float32_t rs1, unsigned int frm,
                                               size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_vf_f16m2_rm_m(vbool8_t vm, vfloat16m2_t vs2,
                                               float32_t rs1, unsigned int frm,
                                               size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_vf_f16m4_rm_m(vbool4_t vm, vfloat16m4_t vs2,
                                               float32_t rs1, unsigned int frm,
                                               size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_vf_f16m8_rm_m(vbool2_t vm, vfloat16m8_t vs2,
                                               float32_t rs1, unsigned int frm,
                                               size_t vl);
----

=== Andes Vector INT4 Load Extension(XAndesVSIntLoad)

[[vector-int4-load-operations]]
==== Andes Vector INT4 Load Intrinsics(XAndesVSIntLoad)

[,c]
----
vint8mf8_t __riscv_nds_vln8_v_i8mf8(const void *rs1, size_t vl);
vint8mf4_t __riscv_nds_vln8_v_i8mf4(const void *rs1, size_t vl);
vint8mf2_t __riscv_nds_vln8_v_i8mf2(const void *rs1, size_t vl);
vint8m1_t __riscv_nds_vln8_v_i8m1(const void *rs1, size_t vl);
vint8m2_t __riscv_nds_vln8_v_i8m2(const void *rs1, size_t vl);
vint8m4_t __riscv_nds_vln8_v_i8m4(const void *rs1, size_t vl);
vint8m8_t __riscv_nds_vln8_v_i8m8(const void *rs1, size_t vl);
vuint8mf8_t __riscv_nds_vlnu8_v_u8mf8(const void *rs1, size_t vl);
vuint8mf4_t __riscv_nds_vlnu8_v_u8mf4(const void *rs1, size_t vl);
vuint8mf2_t __riscv_nds_vlnu8_v_u8mf2(const void *rs1, size_t vl);
vuint8m1_t __riscv_nds_vlnu8_v_u8m1(const void *rs1, size_t vl);
vuint8m2_t __riscv_nds_vlnu8_v_u8m2(const void *rs1, size_t vl);
vuint8m4_t __riscv_nds_vlnu8_v_u8m4(const void *rs1, size_t vl);
vuint8m8_t __riscv_nds_vlnu8_v_u8m8(const void *rs1, size_t vl);
// masked functions
vint8mf8_t __riscv_nds_vln8_v_i8mf8_m(vbool64_t vm, const void *rs1, size_t vl);
vint8mf4_t __riscv_nds_vln8_v_i8mf4_m(vbool32_t vm, const void *rs1, size_t vl);
vint8mf2_t __riscv_nds_vln8_v_i8mf2_m(vbool16_t vm, const void *rs1, size_t vl);
vint8m1_t __riscv_nds_vln8_v_i8m1_m(vbool8_t vm, const void *rs1, size_t vl);
vint8m2_t __riscv_nds_vln8_v_i8m2_m(vbool4_t vm, const void *rs1, size_t vl);
vint8m4_t __riscv_nds_vln8_v_i8m4_m(vbool2_t vm, const void *rs1, size_t vl);
vint8m8_t __riscv_nds_vln8_v_i8m8_m(vbool1_t vm, const void *rs1, size_t vl);
vuint8mf8_t __riscv_nds_vlnu8_v_u8mf8_m(vbool64_t vm, const void *rs1,
                                        size_t vl);
vuint8mf4_t __riscv_nds_vlnu8_v_u8mf4_m(vbool32_t vm, const void *rs1,
                                        size_t vl);
vuint8mf2_t __riscv_nds_vlnu8_v_u8mf2_m(vbool16_t vm, const void *rs1,
                                        size_t vl);
vuint8m1_t __riscv_nds_vlnu8_v_u8m1_m(vbool8_t vm, const void *rs1, size_t vl);
vuint8m2_t __riscv_nds_vlnu8_v_u8m2_m(vbool4_t vm, const void *rs1, size_t vl);
vuint8m4_t __riscv_nds_vlnu8_v_u8m4_m(vbool2_t vm, const void *rs1, size_t vl);
vuint8m8_t __riscv_nds_vlnu8_v_u8m8_m(vbool1_t vm, const void *rs1, size_t vl);
----
