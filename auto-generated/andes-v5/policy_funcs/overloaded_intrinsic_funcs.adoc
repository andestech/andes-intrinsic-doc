
=== Andes Vector BFLOAT16 Conversion Extension(XAndesVBFHCvt)

[[policy-variant-overloadedbf16-vector-unit-stride-load]]
==== Vector Unit-Stride Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vle16_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                size_t vl);
vbfloat16mf2_t __riscv_vle16_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                size_t vl);
vbfloat16m1_t __riscv_vle16_tu(vbfloat16m1_t vd, const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_tu(vbfloat16m2_t vd, const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16_tu(vbfloat16m4_t vd, const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16_tu(vbfloat16m8_t vd, const __bf16 *rs1, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16_tum(vbool16_t vm, vbfloat16m1_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_tum(vbool8_t vm, vbfloat16m2_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16_tum(vbool4_t vm, vbfloat16m4_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16_tum(vbool2_t vm, vbfloat16m8_t vd,
                                const __bf16 *rs1, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                  const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                  const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16m4_t __riscv_vle16_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                 const __bf16 *rs1, size_t vl);
vbfloat16m8_t __riscv_vle16_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                 const __bf16 *rs1, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16mf2_t __riscv_vle16_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                const __bf16 *rs1, size_t vl);
vbfloat16m1_t __riscv_vle16_mu(vbool16_t vm, vbfloat16m1_t vd,
                               const __bf16 *rs1, size_t vl);
vbfloat16m2_t __riscv_vle16_mu(vbool8_t vm, vbfloat16m2_t vd, const __bf16 *rs1,
                               size_t vl);
vbfloat16m4_t __riscv_vle16_mu(vbool4_t vm, vbfloat16m4_t vd, const __bf16 *rs1,
                               size_t vl);
vbfloat16m8_t __riscv_vle16_mu(vbool2_t vm, vbfloat16m8_t vd, const __bf16 *rs1,
                               size_t vl);
----

[[policy-variant-overloadedbf16-vector-unit-stride-store]]
==== Vector Unit-Stride Store Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-strided-load]]
==== Vector Strided Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vlse16_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                 ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                 ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_tu(vbfloat16m1_t vd, const __bf16 *rs1,
                                ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_tu(vbfloat16m2_t vd, const __bf16 *rs1,
                                ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_tu(vbfloat16m4_t vd, const __bf16 *rs1,
                                ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_tu(vbfloat16m8_t vd, const __bf16 *rs1,
                                ptrdiff_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vlse16_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_tum(vbool16_t vm, vbfloat16m1_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_tum(vbool8_t vm, vbfloat16m2_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_tum(vbool4_t vm, vbfloat16m4_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_tum(vbool2_t vm, vbfloat16m8_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vlse16_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                   const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                   const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                  const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vlse16_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vlse16_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                 const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m1_t __riscv_vlse16_mu(vbool16_t vm, vbfloat16m1_t vd,
                                const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m2_t __riscv_vlse16_mu(vbool8_t vm, vbfloat16m2_t vd,
                                const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m4_t __riscv_vlse16_mu(vbool4_t vm, vbfloat16m4_t vd,
                                const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
vbfloat16m8_t __riscv_vlse16_mu(vbool2_t vm, vbfloat16m8_t vd,
                                const __bf16 *rs1, ptrdiff_t rs2, size_t vl);
----

[[policy-variant-overloadedvector-strided-store]]
==== Vector Strided Store Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-indexed-load]]
==== Vector Indexed Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vloxei16_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                   vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vloxei16_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                   vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vloxei16_tu(vbfloat16m1_t vd, const __bf16 *rs1,
                                  vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vloxei16_tu(vbfloat16m2_t vd, const __bf16 *rs1,
                                  vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vloxei16_tu(vbfloat16m4_t vd, const __bf16 *rs1,
                                  vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vloxei16_tu(vbfloat16m8_t vd, const __bf16 *rs1,
                                  vuint16m8_t rs2, size_t vl);
vbfloat16mf4_t __riscv_vluxei16_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                   vuint16mf4_t rs2, size_t vl);
vbfloat16mf2_t __riscv_vluxei16_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                   vuint16mf2_t rs2, size_t vl);
vbfloat16m1_t __riscv_vluxei16_tu(vbfloat16m1_t vd, const __bf16 *rs1,
                                  vuint16m1_t rs2, size_t vl);
vbfloat16m2_t __riscv_vluxei16_tu(vbfloat16m2_t vd, const __bf16 *rs1,
                                  vuint16m2_t rs2, size_t vl);
vbfloat16m4_t __riscv_vluxei16_tu(vbfloat16m4_t vd, const __bf16 *rs1,
                                  vuint16m4_t rs2, size_t vl);
vbfloat16m8_t __riscv_vluxei16_tu(vbfloat16m8_t vd, const __bf16 *rs1,
                                  vuint16m8_t rs2, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                    const __bf16 *rs1, vuint16mf4_t rs2,
                                    size_t vl);
vbfloat16mf2_t __riscv_vloxei16_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                    const __bf16 *rs1, vuint16mf2_t rs2,
                                    size_t vl);
vbfloat16m1_t __riscv_vloxei16_tum(vbool16_t vm, vbfloat16m1_t vd,
                                   const __bf16 *rs1, vuint16m1_t rs2,
                                   size_t vl);
vbfloat16m2_t __riscv_vloxei16_tum(vbool8_t vm, vbfloat16m2_t vd,
                                   const __bf16 *rs1, vuint16m2_t rs2,
                                   size_t vl);
vbfloat16m4_t __riscv_vloxei16_tum(vbool4_t vm, vbfloat16m4_t vd,
                                   const __bf16 *rs1, vuint16m4_t rs2,
                                   size_t vl);
vbfloat16m8_t __riscv_vloxei16_tum(vbool2_t vm, vbfloat16m8_t vd,
                                   const __bf16 *rs1, vuint16m8_t rs2,
                                   size_t vl);
vbfloat16mf4_t __riscv_vluxei16_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                    const __bf16 *rs1, vuint16mf4_t rs2,
                                    size_t vl);
vbfloat16mf2_t __riscv_vluxei16_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                    const __bf16 *rs1, vuint16mf2_t rs2,
                                    size_t vl);
vbfloat16m1_t __riscv_vluxei16_tum(vbool16_t vm, vbfloat16m1_t vd,
                                   const __bf16 *rs1, vuint16m1_t rs2,
                                   size_t vl);
vbfloat16m2_t __riscv_vluxei16_tum(vbool8_t vm, vbfloat16m2_t vd,
                                   const __bf16 *rs1, vuint16m2_t rs2,
                                   size_t vl);
vbfloat16m4_t __riscv_vluxei16_tum(vbool4_t vm, vbfloat16m4_t vd,
                                   const __bf16 *rs1, vuint16m4_t rs2,
                                   size_t vl);
vbfloat16m8_t __riscv_vluxei16_tum(vbool2_t vm, vbfloat16m8_t vd,
                                   const __bf16 *rs1, vuint16m8_t rs2,
                                   size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                     const __bf16 *rs1, vuint16mf4_t rs2,
                                     size_t vl);
vbfloat16mf2_t __riscv_vloxei16_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                     const __bf16 *rs1, vuint16mf2_t rs2,
                                     size_t vl);
vbfloat16m1_t __riscv_vloxei16_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                    const __bf16 *rs1, vuint16m1_t rs2,
                                    size_t vl);
vbfloat16m2_t __riscv_vloxei16_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                    const __bf16 *rs1, vuint16m2_t rs2,
                                    size_t vl);
vbfloat16m4_t __riscv_vloxei16_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                    const __bf16 *rs1, vuint16m4_t rs2,
                                    size_t vl);
vbfloat16m8_t __riscv_vloxei16_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                    const __bf16 *rs1, vuint16m8_t rs2,
                                    size_t vl);
vbfloat16mf4_t __riscv_vluxei16_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                     const __bf16 *rs1, vuint16mf4_t rs2,
                                     size_t vl);
vbfloat16mf2_t __riscv_vluxei16_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                     const __bf16 *rs1, vuint16mf2_t rs2,
                                     size_t vl);
vbfloat16m1_t __riscv_vluxei16_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                    const __bf16 *rs1, vuint16m1_t rs2,
                                    size_t vl);
vbfloat16m2_t __riscv_vluxei16_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                    const __bf16 *rs1, vuint16m2_t rs2,
                                    size_t vl);
vbfloat16m4_t __riscv_vluxei16_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                    const __bf16 *rs1, vuint16m4_t rs2,
                                    size_t vl);
vbfloat16m8_t __riscv_vluxei16_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                    const __bf16 *rs1, vuint16m8_t rs2,
                                    size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vloxei16_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                   const __bf16 *rs1, vuint16mf4_t rs2,
                                   size_t vl);
vbfloat16mf2_t __riscv_vloxei16_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                   const __bf16 *rs1, vuint16mf2_t rs2,
                                   size_t vl);
vbfloat16m1_t __riscv_vloxei16_mu(vbool16_t vm, vbfloat16m1_t vd,
                                  const __bf16 *rs1, vuint16m1_t rs2,
                                  size_t vl);
vbfloat16m2_t __riscv_vloxei16_mu(vbool8_t vm, vbfloat16m2_t vd,
                                  const __bf16 *rs1, vuint16m2_t rs2,
                                  size_t vl);
vbfloat16m4_t __riscv_vloxei16_mu(vbool4_t vm, vbfloat16m4_t vd,
                                  const __bf16 *rs1, vuint16m4_t rs2,
                                  size_t vl);
vbfloat16m8_t __riscv_vloxei16_mu(vbool2_t vm, vbfloat16m8_t vd,
                                  const __bf16 *rs1, vuint16m8_t rs2,
                                  size_t vl);
vbfloat16mf4_t __riscv_vluxei16_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                   const __bf16 *rs1, vuint16mf4_t rs2,
                                   size_t vl);
vbfloat16mf2_t __riscv_vluxei16_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                   const __bf16 *rs1, vuint16mf2_t rs2,
                                   size_t vl);
vbfloat16m1_t __riscv_vluxei16_mu(vbool16_t vm, vbfloat16m1_t vd,
                                  const __bf16 *rs1, vuint16m1_t rs2,
                                  size_t vl);
vbfloat16m2_t __riscv_vluxei16_mu(vbool8_t vm, vbfloat16m2_t vd,
                                  const __bf16 *rs1, vuint16m2_t rs2,
                                  size_t vl);
vbfloat16m4_t __riscv_vluxei16_mu(vbool4_t vm, vbfloat16m4_t vd,
                                  const __bf16 *rs1, vuint16m4_t rs2,
                                  size_t vl);
vbfloat16m8_t __riscv_vluxei16_mu(vbool2_t vm, vbfloat16m8_t vd,
                                  const __bf16 *rs1, vuint16m8_t rs2,
                                  size_t vl);
----

[[policy-variant-overloadedvector-indexed-store]]
==== Vector Indexed Store Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedunit-stride-fault-only-first-loads]]
==== Unit-stride Fault-Only-First Loads Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_vle16ff_tu(vbfloat16mf4_t vd, const __bf16 *rs1,
                                  size_t *new_vl, size_t vl);
vbfloat16mf2_t __riscv_vle16ff_tu(vbfloat16mf2_t vd, const __bf16 *rs1,
                                  size_t *new_vl, size_t vl);
vbfloat16m1_t __riscv_vle16ff_tu(vbfloat16m1_t vd, const __bf16 *rs1,
                                 size_t *new_vl, size_t vl);
vbfloat16m2_t __riscv_vle16ff_tu(vbfloat16m2_t vd, const __bf16 *rs1,
                                 size_t *new_vl, size_t vl);
vbfloat16m4_t __riscv_vle16ff_tu(vbfloat16m4_t vd, const __bf16 *rs1,
                                 size_t *new_vl, size_t vl);
vbfloat16m8_t __riscv_vle16ff_tu(vbfloat16m8_t vd, const __bf16 *rs1,
                                 size_t *new_vl, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16ff_tum(vbool64_t vm, vbfloat16mf4_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16mf2_t __riscv_vle16ff_tum(vbool32_t vm, vbfloat16mf2_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16m1_t __riscv_vle16ff_tum(vbool16_t vm, vbfloat16m1_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m2_t __riscv_vle16ff_tum(vbool8_t vm, vbfloat16m2_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m4_t __riscv_vle16ff_tum(vbool4_t vm, vbfloat16m4_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m8_t __riscv_vle16ff_tum(vbool2_t vm, vbfloat16m8_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16ff_tumu(vbool64_t vm, vbfloat16mf4_t vd,
                                    const __bf16 *rs1, size_t *new_vl,
                                    size_t vl);
vbfloat16mf2_t __riscv_vle16ff_tumu(vbool32_t vm, vbfloat16mf2_t vd,
                                    const __bf16 *rs1, size_t *new_vl,
                                    size_t vl);
vbfloat16m1_t __riscv_vle16ff_tumu(vbool16_t vm, vbfloat16m1_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16m2_t __riscv_vle16ff_tumu(vbool8_t vm, vbfloat16m2_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16m4_t __riscv_vle16ff_tumu(vbool4_t vm, vbfloat16m4_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
vbfloat16m8_t __riscv_vle16ff_tumu(vbool2_t vm, vbfloat16m8_t vd,
                                   const __bf16 *rs1, size_t *new_vl,
                                   size_t vl);
// masked functions
vbfloat16mf4_t __riscv_vle16ff_mu(vbool64_t vm, vbfloat16mf4_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16mf2_t __riscv_vle16ff_mu(vbool32_t vm, vbfloat16mf2_t vd,
                                  const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m1_t __riscv_vle16ff_mu(vbool16_t vm, vbfloat16m1_t vd,
                                 const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m2_t __riscv_vle16ff_mu(vbool8_t vm, vbfloat16m2_t vd,
                                 const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m4_t __riscv_vle16ff_mu(vbool4_t vm, vbfloat16m4_t vd,
                                 const __bf16 *rs1, size_t *new_vl, size_t vl);
vbfloat16m8_t __riscv_vle16ff_mu(vbool2_t vm, vbfloat16m8_t vd,
                                 const __bf16 *rs1, size_t *new_vl, size_t vl);
----

[[policy-variant-overloadedvector-unit-stride-segment-load]]
==== Vector Unit-Stride Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4x2_t __riscv_vlseg2e16_tu(vbfloat16mf4x2_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16_tu(vbfloat16mf4x3_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16_tu(vbfloat16mf4x4_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16_tu(vbfloat16mf4x5_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16_tu(vbfloat16mf4x6_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16_tu(vbfloat16mf4x7_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16_tu(vbfloat16mf4x8_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16_tu(vbfloat16mf2x2_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16_tu(vbfloat16mf2x3_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16_tu(vbfloat16mf2x4_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16_tu(vbfloat16mf2x5_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16_tu(vbfloat16mf2x6_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16_tu(vbfloat16mf2x7_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16_tu(vbfloat16mf2x8_t vd, const __bf16 *rs1,
                                      size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16_tu(vbfloat16m1x2_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16_tu(vbfloat16m1x3_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16_tu(vbfloat16m1x4_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16_tu(vbfloat16m1x5_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16_tu(vbfloat16m1x6_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16_tu(vbfloat16m1x7_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16_tu(vbfloat16m1x8_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16_tu(vbfloat16m2x2_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16_tu(vbfloat16m2x3_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16_tu(vbfloat16m2x4_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16_tu(vbfloat16m4x2_t vd, const __bf16 *rs1,
                                     size_t vl);
vbfloat16mf4x2_t __riscv_vlseg2e16ff_tu(vbfloat16mf4x2_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16ff_tu(vbfloat16mf4x3_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16ff_tu(vbfloat16mf4x4_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16ff_tu(vbfloat16mf4x5_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16ff_tu(vbfloat16mf4x6_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16ff_tu(vbfloat16mf4x7_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16ff_tu(vbfloat16mf4x8_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16ff_tu(vbfloat16mf2x2_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16ff_tu(vbfloat16mf2x3_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16ff_tu(vbfloat16mf2x4_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16ff_tu(vbfloat16mf2x5_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16ff_tu(vbfloat16mf2x6_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16ff_tu(vbfloat16mf2x7_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16ff_tu(vbfloat16mf2x8_t vd, const __bf16 *rs1,
                                        size_t *new_vl, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16ff_tu(vbfloat16m1x2_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16ff_tu(vbfloat16m1x3_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16ff_tu(vbfloat16m1x4_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16ff_tu(vbfloat16m1x5_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16ff_tu(vbfloat16m1x6_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16ff_tu(vbfloat16m1x7_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16ff_tu(vbfloat16m1x8_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16ff_tu(vbfloat16m2x2_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16ff_tu(vbfloat16m2x3_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16ff_tu(vbfloat16m2x4_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16ff_tu(vbfloat16m4x2_t vd, const __bf16 *rs1,
                                       size_t *new_vl, size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vlseg2e16_tum(vbool64_t vm, vbfloat16mf4x2_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16_tum(vbool64_t vm, vbfloat16mf4x3_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16_tum(vbool64_t vm, vbfloat16mf4x4_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16_tum(vbool64_t vm, vbfloat16mf4x5_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16_tum(vbool64_t vm, vbfloat16mf4x6_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16_tum(vbool64_t vm, vbfloat16mf4x7_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16_tum(vbool64_t vm, vbfloat16mf4x8_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16_tum(vbool32_t vm, vbfloat16mf2x2_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16_tum(vbool32_t vm, vbfloat16mf2x3_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16_tum(vbool32_t vm, vbfloat16mf2x4_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16_tum(vbool32_t vm, vbfloat16mf2x5_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16_tum(vbool32_t vm, vbfloat16mf2x6_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16_tum(vbool32_t vm, vbfloat16mf2x7_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16_tum(vbool32_t vm, vbfloat16mf2x8_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16_tum(vbool16_t vm, vbfloat16m1x2_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16_tum(vbool16_t vm, vbfloat16m1x3_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16_tum(vbool16_t vm, vbfloat16m1x4_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16_tum(vbool16_t vm, vbfloat16m1x5_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16_tum(vbool16_t vm, vbfloat16m1x6_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16_tum(vbool16_t vm, vbfloat16m1x7_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16_tum(vbool16_t vm, vbfloat16m1x8_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16_tum(vbool8_t vm, vbfloat16m2x2_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16_tum(vbool8_t vm, vbfloat16m2x3_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16_tum(vbool8_t vm, vbfloat16m2x4_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16_tum(vbool4_t vm, vbfloat16m4x2_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf4x2_t __riscv_vlseg2e16ff_tum(vbool64_t vm, vbfloat16mf4x2_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16ff_tum(vbool64_t vm, vbfloat16mf4x3_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16ff_tum(vbool64_t vm, vbfloat16mf4x4_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16ff_tum(vbool64_t vm, vbfloat16mf4x5_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16ff_tum(vbool64_t vm, vbfloat16mf4x6_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16ff_tum(vbool64_t vm, vbfloat16mf4x7_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16ff_tum(vbool64_t vm, vbfloat16mf4x8_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16ff_tum(vbool32_t vm, vbfloat16mf2x2_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16ff_tum(vbool32_t vm, vbfloat16mf2x3_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16ff_tum(vbool32_t vm, vbfloat16mf2x4_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16ff_tum(vbool32_t vm, vbfloat16mf2x5_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16ff_tum(vbool32_t vm, vbfloat16mf2x6_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16ff_tum(vbool32_t vm, vbfloat16mf2x7_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16ff_tum(vbool32_t vm, vbfloat16mf2x8_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16ff_tum(vbool16_t vm, vbfloat16m1x2_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16ff_tum(vbool16_t vm, vbfloat16m1x3_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16ff_tum(vbool16_t vm, vbfloat16m1x4_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16ff_tum(vbool16_t vm, vbfloat16m1x5_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16ff_tum(vbool16_t vm, vbfloat16m1x6_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16ff_tum(vbool16_t vm, vbfloat16m1x7_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16ff_tum(vbool16_t vm, vbfloat16m1x8_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16ff_tum(vbool8_t vm, vbfloat16m2x2_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16ff_tum(vbool8_t vm, vbfloat16m2x3_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16ff_tum(vbool8_t vm, vbfloat16m2x4_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16ff_tum(vbool4_t vm, vbfloat16m4x2_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vlseg2e16_tumu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16_tumu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16_tumu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16_tumu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16_tumu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16_tumu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16_tumu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16_tumu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16_tumu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16_tumu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16_tumu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16_tumu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16_tumu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16_tumu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                        const __bf16 *rs1, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16_tumu(vbool16_t vm, vbfloat16m1x2_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16_tumu(vbool16_t vm, vbfloat16m1x3_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16_tumu(vbool16_t vm, vbfloat16m1x4_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16_tumu(vbool16_t vm, vbfloat16m1x5_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16_tumu(vbool16_t vm, vbfloat16m1x6_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16_tumu(vbool16_t vm, vbfloat16m1x7_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16_tumu(vbool16_t vm, vbfloat16m1x8_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16_tumu(vbool8_t vm, vbfloat16m2x2_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16_tumu(vbool8_t vm, vbfloat16m2x3_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16_tumu(vbool8_t vm, vbfloat16m2x4_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16_tumu(vbool4_t vm, vbfloat16m4x2_t vd,
                                       const __bf16 *rs1, size_t vl);
vbfloat16mf4x2_t __riscv_vlseg2e16ff_tumu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16ff_tumu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16ff_tumu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16ff_tumu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16ff_tumu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16ff_tumu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16ff_tumu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16ff_tumu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16ff_tumu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16ff_tumu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16ff_tumu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16ff_tumu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16ff_tumu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16ff_tumu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                          const __bf16 *rs1, size_t *new_vl,
                                          size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16ff_tumu(vbool16_t vm, vbfloat16m1x2_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16ff_tumu(vbool16_t vm, vbfloat16m1x3_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16ff_tumu(vbool16_t vm, vbfloat16m1x4_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16ff_tumu(vbool16_t vm, vbfloat16m1x5_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16ff_tumu(vbool16_t vm, vbfloat16m1x6_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16ff_tumu(vbool16_t vm, vbfloat16m1x7_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16ff_tumu(vbool16_t vm, vbfloat16m1x8_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16ff_tumu(vbool8_t vm, vbfloat16m2x2_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16ff_tumu(vbool8_t vm, vbfloat16m2x3_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16ff_tumu(vbool8_t vm, vbfloat16m2x4_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16ff_tumu(vbool4_t vm, vbfloat16m4x2_t vd,
                                         const __bf16 *rs1, size_t *new_vl,
                                         size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vlseg2e16_mu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16_mu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16_mu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16_mu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16_mu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16_mu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16_mu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16_mu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16_mu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16_mu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16_mu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16_mu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16_mu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16_mu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                      const __bf16 *rs1, size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16_mu(vbool16_t vm, vbfloat16m1x2_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16_mu(vbool16_t vm, vbfloat16m1x3_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16_mu(vbool16_t vm, vbfloat16m1x4_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16_mu(vbool16_t vm, vbfloat16m1x5_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16_mu(vbool16_t vm, vbfloat16m1x6_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16_mu(vbool16_t vm, vbfloat16m1x7_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16_mu(vbool16_t vm, vbfloat16m1x8_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16_mu(vbool8_t vm, vbfloat16m2x2_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16_mu(vbool8_t vm, vbfloat16m2x3_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16_mu(vbool8_t vm, vbfloat16m2x4_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16_mu(vbool4_t vm, vbfloat16m4x2_t vd,
                                     const __bf16 *rs1, size_t vl);
vbfloat16mf4x2_t __riscv_vlseg2e16ff_mu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf4x3_t __riscv_vlseg3e16ff_mu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf4x4_t __riscv_vlseg4e16ff_mu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf4x5_t __riscv_vlseg5e16ff_mu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf4x6_t __riscv_vlseg6e16ff_mu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf4x7_t __riscv_vlseg7e16ff_mu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf4x8_t __riscv_vlseg8e16ff_mu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf2x2_t __riscv_vlseg2e16ff_mu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf2x3_t __riscv_vlseg3e16ff_mu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf2x4_t __riscv_vlseg4e16ff_mu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf2x5_t __riscv_vlseg5e16ff_mu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf2x6_t __riscv_vlseg6e16ff_mu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf2x7_t __riscv_vlseg7e16ff_mu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16mf2x8_t __riscv_vlseg8e16ff_mu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                        const __bf16 *rs1, size_t *new_vl,
                                        size_t vl);
vbfloat16m1x2_t __riscv_vlseg2e16ff_mu(vbool16_t vm, vbfloat16m1x2_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m1x3_t __riscv_vlseg3e16ff_mu(vbool16_t vm, vbfloat16m1x3_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m1x4_t __riscv_vlseg4e16ff_mu(vbool16_t vm, vbfloat16m1x4_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m1x5_t __riscv_vlseg5e16ff_mu(vbool16_t vm, vbfloat16m1x5_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m1x6_t __riscv_vlseg6e16ff_mu(vbool16_t vm, vbfloat16m1x6_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m1x7_t __riscv_vlseg7e16ff_mu(vbool16_t vm, vbfloat16m1x7_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m1x8_t __riscv_vlseg8e16ff_mu(vbool16_t vm, vbfloat16m1x8_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m2x2_t __riscv_vlseg2e16ff_mu(vbool8_t vm, vbfloat16m2x2_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m2x3_t __riscv_vlseg3e16ff_mu(vbool8_t vm, vbfloat16m2x3_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m2x4_t __riscv_vlseg4e16ff_mu(vbool8_t vm, vbfloat16m2x4_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
vbfloat16m4x2_t __riscv_vlseg2e16ff_mu(vbool4_t vm, vbfloat16m4x2_t vd,
                                       const __bf16 *rs1, size_t *new_vl,
                                       size_t vl);
----

[[policy-variant-overloadedvecrtor-unit-stride-segment-store]]
==== Vector Unit-Stride Segment Store Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-strided-segment-load]]
==== Vector Strided Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4x2_t __riscv_vlsseg2e16_tu(vbfloat16mf4x2_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vlsseg3e16_tu(vbfloat16mf4x3_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vlsseg4e16_tu(vbfloat16mf4x4_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vlsseg5e16_tu(vbfloat16mf4x5_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vlsseg6e16_tu(vbfloat16mf4x6_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vlsseg7e16_tu(vbfloat16mf4x7_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vlsseg8e16_tu(vbfloat16mf4x8_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vlsseg2e16_tu(vbfloat16mf2x2_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vlsseg3e16_tu(vbfloat16mf2x3_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vlsseg4e16_tu(vbfloat16mf2x4_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vlsseg5e16_tu(vbfloat16mf2x5_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vlsseg6e16_tu(vbfloat16mf2x6_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vlsseg7e16_tu(vbfloat16mf2x7_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vlsseg8e16_tu(vbfloat16mf2x8_t vd, const __bf16 *rs1,
                                       ptrdiff_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vlsseg2e16_tu(vbfloat16m1x2_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vlsseg3e16_tu(vbfloat16m1x3_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vlsseg4e16_tu(vbfloat16m1x4_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vlsseg5e16_tu(vbfloat16m1x5_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vlsseg6e16_tu(vbfloat16m1x6_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vlsseg7e16_tu(vbfloat16m1x7_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vlsseg8e16_tu(vbfloat16m1x8_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vlsseg2e16_tu(vbfloat16m2x2_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vlsseg3e16_tu(vbfloat16m2x3_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vlsseg4e16_tu(vbfloat16m2x4_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vlsseg2e16_tu(vbfloat16m4x2_t vd, const __bf16 *rs1,
                                      ptrdiff_t rs2, size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vlsseg2e16_tum(vbool64_t vm, vbfloat16mf4x2_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf4x3_t __riscv_vlsseg3e16_tum(vbool64_t vm, vbfloat16mf4x3_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf4x4_t __riscv_vlsseg4e16_tum(vbool64_t vm, vbfloat16mf4x4_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf4x5_t __riscv_vlsseg5e16_tum(vbool64_t vm, vbfloat16mf4x5_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf4x6_t __riscv_vlsseg6e16_tum(vbool64_t vm, vbfloat16mf4x6_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf4x7_t __riscv_vlsseg7e16_tum(vbool64_t vm, vbfloat16mf4x7_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf4x8_t __riscv_vlsseg8e16_tum(vbool64_t vm, vbfloat16mf4x8_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2x2_t __riscv_vlsseg2e16_tum(vbool32_t vm, vbfloat16mf2x2_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2x3_t __riscv_vlsseg3e16_tum(vbool32_t vm, vbfloat16mf2x3_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2x4_t __riscv_vlsseg4e16_tum(vbool32_t vm, vbfloat16mf2x4_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2x5_t __riscv_vlsseg5e16_tum(vbool32_t vm, vbfloat16mf2x5_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2x6_t __riscv_vlsseg6e16_tum(vbool32_t vm, vbfloat16mf2x6_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2x7_t __riscv_vlsseg7e16_tum(vbool32_t vm, vbfloat16mf2x7_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16mf2x8_t __riscv_vlsseg8e16_tum(vbool32_t vm, vbfloat16mf2x8_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1x2_t __riscv_vlsseg2e16_tum(vbool16_t vm, vbfloat16m1x2_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m1x3_t __riscv_vlsseg3e16_tum(vbool16_t vm, vbfloat16m1x3_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m1x4_t __riscv_vlsseg4e16_tum(vbool16_t vm, vbfloat16m1x4_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m1x5_t __riscv_vlsseg5e16_tum(vbool16_t vm, vbfloat16m1x5_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m1x6_t __riscv_vlsseg6e16_tum(vbool16_t vm, vbfloat16m1x6_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m1x7_t __riscv_vlsseg7e16_tum(vbool16_t vm, vbfloat16m1x7_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m1x8_t __riscv_vlsseg8e16_tum(vbool16_t vm, vbfloat16m1x8_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m2x2_t __riscv_vlsseg2e16_tum(vbool8_t vm, vbfloat16m2x2_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m2x3_t __riscv_vlsseg3e16_tum(vbool8_t vm, vbfloat16m2x3_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m2x4_t __riscv_vlsseg4e16_tum(vbool8_t vm, vbfloat16m2x4_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m4x2_t __riscv_vlsseg2e16_tum(vbool4_t vm, vbfloat16m4x2_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vlsseg2e16_tumu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf4x3_t __riscv_vlsseg3e16_tumu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf4x4_t __riscv_vlsseg4e16_tumu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf4x5_t __riscv_vlsseg5e16_tumu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf4x6_t __riscv_vlsseg6e16_tumu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf4x7_t __riscv_vlsseg7e16_tumu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf4x8_t __riscv_vlsseg8e16_tumu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf2x2_t __riscv_vlsseg2e16_tumu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf2x3_t __riscv_vlsseg3e16_tumu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf2x4_t __riscv_vlsseg4e16_tumu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf2x5_t __riscv_vlsseg5e16_tumu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf2x6_t __riscv_vlsseg6e16_tumu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf2x7_t __riscv_vlsseg7e16_tumu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16mf2x8_t __riscv_vlsseg8e16_tumu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                         const __bf16 *rs1, ptrdiff_t rs2,
                                         size_t vl);
vbfloat16m1x2_t __riscv_vlsseg2e16_tumu(vbool16_t vm, vbfloat16m1x2_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1x3_t __riscv_vlsseg3e16_tumu(vbool16_t vm, vbfloat16m1x3_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1x4_t __riscv_vlsseg4e16_tumu(vbool16_t vm, vbfloat16m1x4_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1x5_t __riscv_vlsseg5e16_tumu(vbool16_t vm, vbfloat16m1x5_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1x6_t __riscv_vlsseg6e16_tumu(vbool16_t vm, vbfloat16m1x6_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1x7_t __riscv_vlsseg7e16_tumu(vbool16_t vm, vbfloat16m1x7_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m1x8_t __riscv_vlsseg8e16_tumu(vbool16_t vm, vbfloat16m1x8_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m2x2_t __riscv_vlsseg2e16_tumu(vbool8_t vm, vbfloat16m2x2_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m2x3_t __riscv_vlsseg3e16_tumu(vbool8_t vm, vbfloat16m2x3_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m2x4_t __riscv_vlsseg4e16_tumu(vbool8_t vm, vbfloat16m2x4_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
vbfloat16m4x2_t __riscv_vlsseg2e16_tumu(vbool4_t vm, vbfloat16m4x2_t vd,
                                        const __bf16 *rs1, ptrdiff_t rs2,
                                        size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vlsseg2e16_mu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf4x3_t __riscv_vlsseg3e16_mu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf4x4_t __riscv_vlsseg4e16_mu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf4x5_t __riscv_vlsseg5e16_mu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf4x6_t __riscv_vlsseg6e16_mu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf4x7_t __riscv_vlsseg7e16_mu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf4x8_t __riscv_vlsseg8e16_mu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf2x2_t __riscv_vlsseg2e16_mu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf2x3_t __riscv_vlsseg3e16_mu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf2x4_t __riscv_vlsseg4e16_mu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf2x5_t __riscv_vlsseg5e16_mu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf2x6_t __riscv_vlsseg6e16_mu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf2x7_t __riscv_vlsseg7e16_mu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16mf2x8_t __riscv_vlsseg8e16_mu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                       const __bf16 *rs1, ptrdiff_t rs2,
                                       size_t vl);
vbfloat16m1x2_t __riscv_vlsseg2e16_mu(vbool16_t vm, vbfloat16m1x2_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m1x3_t __riscv_vlsseg3e16_mu(vbool16_t vm, vbfloat16m1x3_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m1x4_t __riscv_vlsseg4e16_mu(vbool16_t vm, vbfloat16m1x4_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m1x5_t __riscv_vlsseg5e16_mu(vbool16_t vm, vbfloat16m1x5_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m1x6_t __riscv_vlsseg6e16_mu(vbool16_t vm, vbfloat16m1x6_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m1x7_t __riscv_vlsseg7e16_mu(vbool16_t vm, vbfloat16m1x7_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m1x8_t __riscv_vlsseg8e16_mu(vbool16_t vm, vbfloat16m1x8_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m2x2_t __riscv_vlsseg2e16_mu(vbool8_t vm, vbfloat16m2x2_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m2x3_t __riscv_vlsseg3e16_mu(vbool8_t vm, vbfloat16m2x3_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m2x4_t __riscv_vlsseg4e16_mu(vbool8_t vm, vbfloat16m2x4_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
vbfloat16m4x2_t __riscv_vlsseg2e16_mu(vbool4_t vm, vbfloat16m4x2_t vd,
                                      const __bf16 *rs1, ptrdiff_t rs2,
                                      size_t vl);
----

[[policy-variant-overloadedvector-strided-segment-store]]
==== Vector Strided Segment Store Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-indexed-segment-load]]
==== Vector Indexed Segment Load Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4x2_t __riscv_vloxseg2ei16_tu(vbfloat16mf4x2_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vloxseg3ei16_tu(vbfloat16mf4x3_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vloxseg4ei16_tu(vbfloat16mf4x4_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vloxseg5ei16_tu(vbfloat16mf4x5_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vloxseg6ei16_tu(vbfloat16mf4x6_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vloxseg7ei16_tu(vbfloat16mf4x7_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vloxseg8ei16_tu(vbfloat16mf4x8_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vloxseg2ei16_tu(vbfloat16mf2x2_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vloxseg3ei16_tu(vbfloat16mf2x3_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vloxseg4ei16_tu(vbfloat16mf2x4_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vloxseg5ei16_tu(vbfloat16mf2x5_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vloxseg6ei16_tu(vbfloat16mf2x6_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vloxseg7ei16_tu(vbfloat16mf2x7_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vloxseg8ei16_tu(vbfloat16mf2x8_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vloxseg2ei16_tu(vbfloat16m1x2_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vloxseg3ei16_tu(vbfloat16m1x3_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vloxseg4ei16_tu(vbfloat16m1x4_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vloxseg5ei16_tu(vbfloat16m1x5_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vloxseg6ei16_tu(vbfloat16m1x6_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vloxseg7ei16_tu(vbfloat16m1x7_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vloxseg8ei16_tu(vbfloat16m1x8_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vloxseg2ei16_tu(vbfloat16m2x2_t vd, const __bf16 *rs1,
                                        vuint16m2_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vloxseg3ei16_tu(vbfloat16m2x3_t vd, const __bf16 *rs1,
                                        vuint16m2_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vloxseg4ei16_tu(vbfloat16m2x4_t vd, const __bf16 *rs1,
                                        vuint16m2_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vloxseg2ei16_tu(vbfloat16m4x2_t vd, const __bf16 *rs1,
                                        vuint16m4_t rs2, size_t vl);
vbfloat16mf4x2_t __riscv_vluxseg2ei16_tu(vbfloat16mf4x2_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x3_t __riscv_vluxseg3ei16_tu(vbfloat16mf4x3_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x4_t __riscv_vluxseg4ei16_tu(vbfloat16mf4x4_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x5_t __riscv_vluxseg5ei16_tu(vbfloat16mf4x5_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x6_t __riscv_vluxseg6ei16_tu(vbfloat16mf4x6_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x7_t __riscv_vluxseg7ei16_tu(vbfloat16mf4x7_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf4x8_t __riscv_vluxseg8ei16_tu(vbfloat16mf4x8_t vd, const __bf16 *rs1,
                                         vuint16mf4_t rs2, size_t vl);
vbfloat16mf2x2_t __riscv_vluxseg2ei16_tu(vbfloat16mf2x2_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x3_t __riscv_vluxseg3ei16_tu(vbfloat16mf2x3_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x4_t __riscv_vluxseg4ei16_tu(vbfloat16mf2x4_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x5_t __riscv_vluxseg5ei16_tu(vbfloat16mf2x5_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x6_t __riscv_vluxseg6ei16_tu(vbfloat16mf2x6_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x7_t __riscv_vluxseg7ei16_tu(vbfloat16mf2x7_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16mf2x8_t __riscv_vluxseg8ei16_tu(vbfloat16mf2x8_t vd, const __bf16 *rs1,
                                         vuint16mf2_t rs2, size_t vl);
vbfloat16m1x2_t __riscv_vluxseg2ei16_tu(vbfloat16m1x2_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x3_t __riscv_vluxseg3ei16_tu(vbfloat16m1x3_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x4_t __riscv_vluxseg4ei16_tu(vbfloat16m1x4_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x5_t __riscv_vluxseg5ei16_tu(vbfloat16m1x5_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x6_t __riscv_vluxseg6ei16_tu(vbfloat16m1x6_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x7_t __riscv_vluxseg7ei16_tu(vbfloat16m1x7_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m1x8_t __riscv_vluxseg8ei16_tu(vbfloat16m1x8_t vd, const __bf16 *rs1,
                                        vuint16m1_t rs2, size_t vl);
vbfloat16m2x2_t __riscv_vluxseg2ei16_tu(vbfloat16m2x2_t vd, const __bf16 *rs1,
                                        vuint16m2_t rs2, size_t vl);
vbfloat16m2x3_t __riscv_vluxseg3ei16_tu(vbfloat16m2x3_t vd, const __bf16 *rs1,
                                        vuint16m2_t rs2, size_t vl);
vbfloat16m2x4_t __riscv_vluxseg4ei16_tu(vbfloat16m2x4_t vd, const __bf16 *rs1,
                                        vuint16m2_t rs2, size_t vl);
vbfloat16m4x2_t __riscv_vluxseg2ei16_tu(vbfloat16m4x2_t vd, const __bf16 *rs1,
                                        vuint16m4_t rs2, size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vloxseg2ei16_tum(vbool64_t vm, vbfloat16mf4x2_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x3_t __riscv_vloxseg3ei16_tum(vbool64_t vm, vbfloat16mf4x3_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x4_t __riscv_vloxseg4ei16_tum(vbool64_t vm, vbfloat16mf4x4_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x5_t __riscv_vloxseg5ei16_tum(vbool64_t vm, vbfloat16mf4x5_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x6_t __riscv_vloxseg6ei16_tum(vbool64_t vm, vbfloat16mf4x6_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x7_t __riscv_vloxseg7ei16_tum(vbool64_t vm, vbfloat16mf4x7_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x8_t __riscv_vloxseg8ei16_tum(vbool64_t vm, vbfloat16mf4x8_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf2x2_t __riscv_vloxseg2ei16_tum(vbool32_t vm, vbfloat16mf2x2_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x3_t __riscv_vloxseg3ei16_tum(vbool32_t vm, vbfloat16mf2x3_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x4_t __riscv_vloxseg4ei16_tum(vbool32_t vm, vbfloat16mf2x4_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x5_t __riscv_vloxseg5ei16_tum(vbool32_t vm, vbfloat16mf2x5_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x6_t __riscv_vloxseg6ei16_tum(vbool32_t vm, vbfloat16mf2x6_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x7_t __riscv_vloxseg7ei16_tum(vbool32_t vm, vbfloat16mf2x7_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x8_t __riscv_vloxseg8ei16_tum(vbool32_t vm, vbfloat16mf2x8_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16m1x2_t __riscv_vloxseg2ei16_tum(vbool16_t vm, vbfloat16m1x2_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x3_t __riscv_vloxseg3ei16_tum(vbool16_t vm, vbfloat16m1x3_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x4_t __riscv_vloxseg4ei16_tum(vbool16_t vm, vbfloat16m1x4_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x5_t __riscv_vloxseg5ei16_tum(vbool16_t vm, vbfloat16m1x5_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x6_t __riscv_vloxseg6ei16_tum(vbool16_t vm, vbfloat16m1x6_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x7_t __riscv_vloxseg7ei16_tum(vbool16_t vm, vbfloat16m1x7_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x8_t __riscv_vloxseg8ei16_tum(vbool16_t vm, vbfloat16m1x8_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m2x2_t __riscv_vloxseg2ei16_tum(vbool8_t vm, vbfloat16m2x2_t vd,
                                         const __bf16 *rs1, vuint16m2_t rs2,
                                         size_t vl);
vbfloat16m2x3_t __riscv_vloxseg3ei16_tum(vbool8_t vm, vbfloat16m2x3_t vd,
                                         const __bf16 *rs1, vuint16m2_t rs2,
                                         size_t vl);
vbfloat16m2x4_t __riscv_vloxseg4ei16_tum(vbool8_t vm, vbfloat16m2x4_t vd,
                                         const __bf16 *rs1, vuint16m2_t rs2,
                                         size_t vl);
vbfloat16m4x2_t __riscv_vloxseg2ei16_tum(vbool4_t vm, vbfloat16m4x2_t vd,
                                         const __bf16 *rs1, vuint16m4_t rs2,
                                         size_t vl);
vbfloat16mf4x2_t __riscv_vluxseg2ei16_tum(vbool64_t vm, vbfloat16mf4x2_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x3_t __riscv_vluxseg3ei16_tum(vbool64_t vm, vbfloat16mf4x3_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x4_t __riscv_vluxseg4ei16_tum(vbool64_t vm, vbfloat16mf4x4_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x5_t __riscv_vluxseg5ei16_tum(vbool64_t vm, vbfloat16mf4x5_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x6_t __riscv_vluxseg6ei16_tum(vbool64_t vm, vbfloat16mf4x6_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x7_t __riscv_vluxseg7ei16_tum(vbool64_t vm, vbfloat16mf4x7_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf4x8_t __riscv_vluxseg8ei16_tum(vbool64_t vm, vbfloat16mf4x8_t vd,
                                          const __bf16 *rs1, vuint16mf4_t rs2,
                                          size_t vl);
vbfloat16mf2x2_t __riscv_vluxseg2ei16_tum(vbool32_t vm, vbfloat16mf2x2_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x3_t __riscv_vluxseg3ei16_tum(vbool32_t vm, vbfloat16mf2x3_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x4_t __riscv_vluxseg4ei16_tum(vbool32_t vm, vbfloat16mf2x4_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x5_t __riscv_vluxseg5ei16_tum(vbool32_t vm, vbfloat16mf2x5_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x6_t __riscv_vluxseg6ei16_tum(vbool32_t vm, vbfloat16mf2x6_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x7_t __riscv_vluxseg7ei16_tum(vbool32_t vm, vbfloat16mf2x7_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16mf2x8_t __riscv_vluxseg8ei16_tum(vbool32_t vm, vbfloat16mf2x8_t vd,
                                          const __bf16 *rs1, vuint16mf2_t rs2,
                                          size_t vl);
vbfloat16m1x2_t __riscv_vluxseg2ei16_tum(vbool16_t vm, vbfloat16m1x2_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x3_t __riscv_vluxseg3ei16_tum(vbool16_t vm, vbfloat16m1x3_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x4_t __riscv_vluxseg4ei16_tum(vbool16_t vm, vbfloat16m1x4_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x5_t __riscv_vluxseg5ei16_tum(vbool16_t vm, vbfloat16m1x5_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x6_t __riscv_vluxseg6ei16_tum(vbool16_t vm, vbfloat16m1x6_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x7_t __riscv_vluxseg7ei16_tum(vbool16_t vm, vbfloat16m1x7_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m1x8_t __riscv_vluxseg8ei16_tum(vbool16_t vm, vbfloat16m1x8_t vd,
                                         const __bf16 *rs1, vuint16m1_t rs2,
                                         size_t vl);
vbfloat16m2x2_t __riscv_vluxseg2ei16_tum(vbool8_t vm, vbfloat16m2x2_t vd,
                                         const __bf16 *rs1, vuint16m2_t rs2,
                                         size_t vl);
vbfloat16m2x3_t __riscv_vluxseg3ei16_tum(vbool8_t vm, vbfloat16m2x3_t vd,
                                         const __bf16 *rs1, vuint16m2_t rs2,
                                         size_t vl);
vbfloat16m2x4_t __riscv_vluxseg4ei16_tum(vbool8_t vm, vbfloat16m2x4_t vd,
                                         const __bf16 *rs1, vuint16m2_t rs2,
                                         size_t vl);
vbfloat16m4x2_t __riscv_vluxseg2ei16_tum(vbool4_t vm, vbfloat16m4x2_t vd,
                                         const __bf16 *rs1, vuint16m4_t rs2,
                                         size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vloxseg2ei16_tumu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x3_t __riscv_vloxseg3ei16_tumu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x4_t __riscv_vloxseg4ei16_tumu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x5_t __riscv_vloxseg5ei16_tumu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x6_t __riscv_vloxseg6ei16_tumu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x7_t __riscv_vloxseg7ei16_tumu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x8_t __riscv_vloxseg8ei16_tumu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf2x2_t __riscv_vloxseg2ei16_tumu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x3_t __riscv_vloxseg3ei16_tumu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x4_t __riscv_vloxseg4ei16_tumu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x5_t __riscv_vloxseg5ei16_tumu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x6_t __riscv_vloxseg6ei16_tumu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x7_t __riscv_vloxseg7ei16_tumu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x8_t __riscv_vloxseg8ei16_tumu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16m1x2_t __riscv_vloxseg2ei16_tumu(vbool16_t vm, vbfloat16m1x2_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x3_t __riscv_vloxseg3ei16_tumu(vbool16_t vm, vbfloat16m1x3_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x4_t __riscv_vloxseg4ei16_tumu(vbool16_t vm, vbfloat16m1x4_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x5_t __riscv_vloxseg5ei16_tumu(vbool16_t vm, vbfloat16m1x5_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x6_t __riscv_vloxseg6ei16_tumu(vbool16_t vm, vbfloat16m1x6_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x7_t __riscv_vloxseg7ei16_tumu(vbool16_t vm, vbfloat16m1x7_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x8_t __riscv_vloxseg8ei16_tumu(vbool16_t vm, vbfloat16m1x8_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m2x2_t __riscv_vloxseg2ei16_tumu(vbool8_t vm, vbfloat16m2x2_t vd,
                                          const __bf16 *rs1, vuint16m2_t rs2,
                                          size_t vl);
vbfloat16m2x3_t __riscv_vloxseg3ei16_tumu(vbool8_t vm, vbfloat16m2x3_t vd,
                                          const __bf16 *rs1, vuint16m2_t rs2,
                                          size_t vl);
vbfloat16m2x4_t __riscv_vloxseg4ei16_tumu(vbool8_t vm, vbfloat16m2x4_t vd,
                                          const __bf16 *rs1, vuint16m2_t rs2,
                                          size_t vl);
vbfloat16m4x2_t __riscv_vloxseg2ei16_tumu(vbool4_t vm, vbfloat16m4x2_t vd,
                                          const __bf16 *rs1, vuint16m4_t rs2,
                                          size_t vl);
vbfloat16mf4x2_t __riscv_vluxseg2ei16_tumu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x3_t __riscv_vluxseg3ei16_tumu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x4_t __riscv_vluxseg4ei16_tumu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x5_t __riscv_vluxseg5ei16_tumu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x6_t __riscv_vluxseg6ei16_tumu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x7_t __riscv_vluxseg7ei16_tumu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf4x8_t __riscv_vluxseg8ei16_tumu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                           const __bf16 *rs1, vuint16mf4_t rs2,
                                           size_t vl);
vbfloat16mf2x2_t __riscv_vluxseg2ei16_tumu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x3_t __riscv_vluxseg3ei16_tumu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x4_t __riscv_vluxseg4ei16_tumu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x5_t __riscv_vluxseg5ei16_tumu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x6_t __riscv_vluxseg6ei16_tumu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x7_t __riscv_vluxseg7ei16_tumu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16mf2x8_t __riscv_vluxseg8ei16_tumu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                           const __bf16 *rs1, vuint16mf2_t rs2,
                                           size_t vl);
vbfloat16m1x2_t __riscv_vluxseg2ei16_tumu(vbool16_t vm, vbfloat16m1x2_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x3_t __riscv_vluxseg3ei16_tumu(vbool16_t vm, vbfloat16m1x3_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x4_t __riscv_vluxseg4ei16_tumu(vbool16_t vm, vbfloat16m1x4_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x5_t __riscv_vluxseg5ei16_tumu(vbool16_t vm, vbfloat16m1x5_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x6_t __riscv_vluxseg6ei16_tumu(vbool16_t vm, vbfloat16m1x6_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x7_t __riscv_vluxseg7ei16_tumu(vbool16_t vm, vbfloat16m1x7_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m1x8_t __riscv_vluxseg8ei16_tumu(vbool16_t vm, vbfloat16m1x8_t vd,
                                          const __bf16 *rs1, vuint16m1_t rs2,
                                          size_t vl);
vbfloat16m2x2_t __riscv_vluxseg2ei16_tumu(vbool8_t vm, vbfloat16m2x2_t vd,
                                          const __bf16 *rs1, vuint16m2_t rs2,
                                          size_t vl);
vbfloat16m2x3_t __riscv_vluxseg3ei16_tumu(vbool8_t vm, vbfloat16m2x3_t vd,
                                          const __bf16 *rs1, vuint16m2_t rs2,
                                          size_t vl);
vbfloat16m2x4_t __riscv_vluxseg4ei16_tumu(vbool8_t vm, vbfloat16m2x4_t vd,
                                          const __bf16 *rs1, vuint16m2_t rs2,
                                          size_t vl);
vbfloat16m4x2_t __riscv_vluxseg2ei16_tumu(vbool4_t vm, vbfloat16m4x2_t vd,
                                          const __bf16 *rs1, vuint16m4_t rs2,
                                          size_t vl);
// masked functions
vbfloat16mf4x2_t __riscv_vloxseg2ei16_mu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x3_t __riscv_vloxseg3ei16_mu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x4_t __riscv_vloxseg4ei16_mu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x5_t __riscv_vloxseg5ei16_mu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x6_t __riscv_vloxseg6ei16_mu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x7_t __riscv_vloxseg7ei16_mu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x8_t __riscv_vloxseg8ei16_mu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf2x2_t __riscv_vloxseg2ei16_mu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x3_t __riscv_vloxseg3ei16_mu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x4_t __riscv_vloxseg4ei16_mu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x5_t __riscv_vloxseg5ei16_mu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x6_t __riscv_vloxseg6ei16_mu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x7_t __riscv_vloxseg7ei16_mu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x8_t __riscv_vloxseg8ei16_mu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16m1x2_t __riscv_vloxseg2ei16_mu(vbool16_t vm, vbfloat16m1x2_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x3_t __riscv_vloxseg3ei16_mu(vbool16_t vm, vbfloat16m1x3_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x4_t __riscv_vloxseg4ei16_mu(vbool16_t vm, vbfloat16m1x4_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x5_t __riscv_vloxseg5ei16_mu(vbool16_t vm, vbfloat16m1x5_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x6_t __riscv_vloxseg6ei16_mu(vbool16_t vm, vbfloat16m1x6_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x7_t __riscv_vloxseg7ei16_mu(vbool16_t vm, vbfloat16m1x7_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x8_t __riscv_vloxseg8ei16_mu(vbool16_t vm, vbfloat16m1x8_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m2x2_t __riscv_vloxseg2ei16_mu(vbool8_t vm, vbfloat16m2x2_t vd,
                                        const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m2x3_t __riscv_vloxseg3ei16_mu(vbool8_t vm, vbfloat16m2x3_t vd,
                                        const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m2x4_t __riscv_vloxseg4ei16_mu(vbool8_t vm, vbfloat16m2x4_t vd,
                                        const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m4x2_t __riscv_vloxseg2ei16_mu(vbool4_t vm, vbfloat16m4x2_t vd,
                                        const __bf16 *rs1, vuint16m4_t rs2,
                                        size_t vl);
vbfloat16mf4x2_t __riscv_vluxseg2ei16_mu(vbool64_t vm, vbfloat16mf4x2_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x3_t __riscv_vluxseg3ei16_mu(vbool64_t vm, vbfloat16mf4x3_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x4_t __riscv_vluxseg4ei16_mu(vbool64_t vm, vbfloat16mf4x4_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x5_t __riscv_vluxseg5ei16_mu(vbool64_t vm, vbfloat16mf4x5_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x6_t __riscv_vluxseg6ei16_mu(vbool64_t vm, vbfloat16mf4x6_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x7_t __riscv_vluxseg7ei16_mu(vbool64_t vm, vbfloat16mf4x7_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf4x8_t __riscv_vluxseg8ei16_mu(vbool64_t vm, vbfloat16mf4x8_t vd,
                                         const __bf16 *rs1, vuint16mf4_t rs2,
                                         size_t vl);
vbfloat16mf2x2_t __riscv_vluxseg2ei16_mu(vbool32_t vm, vbfloat16mf2x2_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x3_t __riscv_vluxseg3ei16_mu(vbool32_t vm, vbfloat16mf2x3_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x4_t __riscv_vluxseg4ei16_mu(vbool32_t vm, vbfloat16mf2x4_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x5_t __riscv_vluxseg5ei16_mu(vbool32_t vm, vbfloat16mf2x5_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x6_t __riscv_vluxseg6ei16_mu(vbool32_t vm, vbfloat16mf2x6_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x7_t __riscv_vluxseg7ei16_mu(vbool32_t vm, vbfloat16mf2x7_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16mf2x8_t __riscv_vluxseg8ei16_mu(vbool32_t vm, vbfloat16mf2x8_t vd,
                                         const __bf16 *rs1, vuint16mf2_t rs2,
                                         size_t vl);
vbfloat16m1x2_t __riscv_vluxseg2ei16_mu(vbool16_t vm, vbfloat16m1x2_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x3_t __riscv_vluxseg3ei16_mu(vbool16_t vm, vbfloat16m1x3_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x4_t __riscv_vluxseg4ei16_mu(vbool16_t vm, vbfloat16m1x4_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x5_t __riscv_vluxseg5ei16_mu(vbool16_t vm, vbfloat16m1x5_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x6_t __riscv_vluxseg6ei16_mu(vbool16_t vm, vbfloat16m1x6_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x7_t __riscv_vluxseg7ei16_mu(vbool16_t vm, vbfloat16m1x7_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m1x8_t __riscv_vluxseg8ei16_mu(vbool16_t vm, vbfloat16m1x8_t vd,
                                        const __bf16 *rs1, vuint16m1_t rs2,
                                        size_t vl);
vbfloat16m2x2_t __riscv_vluxseg2ei16_mu(vbool8_t vm, vbfloat16m2x2_t vd,
                                        const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m2x3_t __riscv_vluxseg3ei16_mu(vbool8_t vm, vbfloat16m2x3_t vd,
                                        const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m2x4_t __riscv_vluxseg4ei16_mu(vbool8_t vm, vbfloat16m2x4_t vd,
                                        const __bf16 *rs1, vuint16m2_t rs2,
                                        size_t vl);
vbfloat16m4x2_t __riscv_vluxseg2ei16_mu(vbool4_t vm, vbfloat16m4x2_t vd,
                                        const __bf16 *rs1, vuint16m4_t rs2,
                                        size_t vl);
----

[[policy-variant-overloadedvector-indexed-segment-store]]
==== Vector Indexed Segment Store Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedbf16-vector-narrow-convert]]
==== Vector Narrowing Convert Intrinsics(XAndesVBFHCvt)

[,c]
----
vbfloat16mf4_t __riscv_nds_vfncvt_bf16_tu(vbfloat16mf4_t vd, vfloat32mf2_t vs2,
                                          size_t vl);
vbfloat16mf2_t __riscv_nds_vfncvt_bf16_tu(vbfloat16mf2_t vd, vfloat32m1_t vs2,
                                          size_t vl);
vbfloat16m1_t __riscv_nds_vfncvt_bf16_tu(vbfloat16m1_t vd, vfloat32m2_t vs2,
                                         size_t vl);
vbfloat16m2_t __riscv_nds_vfncvt_bf16_tu(vbfloat16m2_t vd, vfloat32m4_t vs2,
                                         size_t vl);
vbfloat16m4_t __riscv_nds_vfncvt_bf16_tu(vbfloat16m4_t vd, vfloat32m8_t vs2,
                                         size_t vl);
vbfloat16mf4_t __riscv_nds_vfncvt_bf16_tu(vbfloat16mf4_t vd, vfloat32mf2_t vs2,
                                          unsigned int frm, size_t vl);
vbfloat16mf2_t __riscv_nds_vfncvt_bf16_tu(vbfloat16mf2_t vd, vfloat32m1_t vs2,
                                          unsigned int frm, size_t vl);
vbfloat16m1_t __riscv_nds_vfncvt_bf16_tu(vbfloat16m1_t vd, vfloat32m2_t vs2,
                                         unsigned int frm, size_t vl);
vbfloat16m2_t __riscv_nds_vfncvt_bf16_tu(vbfloat16m2_t vd, vfloat32m4_t vs2,
                                         unsigned int frm, size_t vl);
vbfloat16m4_t __riscv_nds_vfncvt_bf16_tu(vbfloat16m4_t vd, vfloat32m8_t vs2,
                                         unsigned int frm, size_t vl);
----

[[policy-variant-overloadedbf16-vector-widening-convert]]
==== Vector Widening Convert Intrinsics(XAndesVBFHCvt)

[,c]
----
vfloat32mf2_t __riscv_nds_vfwcvt_s_tu(vfloat32mf2_t vd, vbfloat16mf4_t vs2,
                                      size_t vl);
vfloat32m1_t __riscv_nds_vfwcvt_s_tu(vfloat32m1_t vd, vbfloat16mf2_t vs2,
                                     size_t vl);
vfloat32m2_t __riscv_nds_vfwcvt_s_tu(vfloat32m2_t vd, vbfloat16m1_t vs2,
                                     size_t vl);
vfloat32m4_t __riscv_nds_vfwcvt_s_tu(vfloat32m4_t vd, vbfloat16m2_t vs2,
                                     size_t vl);
vfloat32m8_t __riscv_nds_vfwcvt_s_tu(vfloat32m8_t vd, vbfloat16m4_t vs2,
                                     size_t vl);
----

[[policy-variant-overloadedreinterpret-cast-conversion]]
==== Reinterpret Cast Conversion Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-lmul-extensionn]]
==== Vector LMUL Extension Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-lmul-truncation]]
==== Vector LMUL Truncation Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-initialization]]
==== Vector Initialization Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-insertion]]
==== Vector Insertion Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-extraction]]
==== Vector Extraction Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

[[policy-variant-overloadedvector-creation]]
==== Vector Creation Intrinsics(XAndesVBFHCvt)
Intrinsics here don't have a policy variant.

=== Andes Vector Quad-Widening Integer Multiply-Add Extension(XAndesVQMac)

[[policy-variant-overloadedvector-quad-widening-integer-multiply-add-operations]]
==== Vector Quad-Widening Integer Multiply-Add Intrinsics(XAndesVQMac)

[,c]
----
vint32mf2_t __riscv_nds_vqmacc_tu(vint32mf2_t vd, vint8mf8_t vs1,
                                  vint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmacc_tu(vint32mf2_t vd, int8_t rs1, vint8mf8_t vs2,
                                  size_t vl);
vint32m1_t __riscv_nds_vqmacc_tu(vint32m1_t vd, vint8mf4_t vs1, vint8mf4_t vs2,
                                 size_t vl);
vint32m1_t __riscv_nds_vqmacc_tu(vint32m1_t vd, int8_t rs1, vint8mf4_t vs2,
                                 size_t vl);
vint32m2_t __riscv_nds_vqmacc_tu(vint32m2_t vd, vint8mf2_t vs1, vint8mf2_t vs2,
                                 size_t vl);
vint32m2_t __riscv_nds_vqmacc_tu(vint32m2_t vd, int8_t rs1, vint8mf2_t vs2,
                                 size_t vl);
vint32m4_t __riscv_nds_vqmacc_tu(vint32m4_t vd, vint8m1_t vs1, vint8m1_t vs2,
                                 size_t vl);
vint32m4_t __riscv_nds_vqmacc_tu(vint32m4_t vd, int8_t rs1, vint8m1_t vs2,
                                 size_t vl);
vint32m8_t __riscv_nds_vqmacc_tu(vint32m8_t vd, vint8m2_t vs1, vint8m2_t vs2,
                                 size_t vl);
vint32m8_t __riscv_nds_vqmacc_tu(vint32m8_t vd, int8_t rs1, vint8m2_t vs2,
                                 size_t vl);
vint64m1_t __riscv_nds_vqmacc_tu(vint64m1_t vd, vint16mf4_t vs1,
                                 vint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_tu(vint64m1_t vd, int16_t rs1, vint16mf4_t vs2,
                                 size_t vl);
vint64m2_t __riscv_nds_vqmacc_tu(vint64m2_t vd, vint16mf2_t vs1,
                                 vint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_tu(vint64m2_t vd, int16_t rs1, vint16mf2_t vs2,
                                 size_t vl);
vint64m4_t __riscv_nds_vqmacc_tu(vint64m4_t vd, vint16m1_t vs1, vint16m1_t vs2,
                                 size_t vl);
vint64m4_t __riscv_nds_vqmacc_tu(vint64m4_t vd, int16_t rs1, vint16m1_t vs2,
                                 size_t vl);
vint64m8_t __riscv_nds_vqmacc_tu(vint64m8_t vd, vint16m2_t vs1, vint16m2_t vs2,
                                 size_t vl);
vint64m8_t __riscv_nds_vqmacc_tu(vint64m8_t vd, int16_t rs1, vint16m2_t vs2,
                                 size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_tu(vint32mf2_t vd, vint8mf8_t vs1,
                                    vuint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_tu(vint32mf2_t vd, int8_t rs1, vuint8mf8_t vs2,
                                    size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_tu(vint32m1_t vd, vint8mf4_t vs1,
                                   vuint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_tu(vint32m1_t vd, int8_t rs1, vuint8mf4_t vs2,
                                   size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_tu(vint32m2_t vd, vint8mf2_t vs1,
                                   vuint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_tu(vint32m2_t vd, int8_t rs1, vuint8mf2_t vs2,
                                   size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_tu(vint32m4_t vd, vint8m1_t vs1, vuint8m1_t vs2,
                                   size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_tu(vint32m4_t vd, int8_t rs1, vuint8m1_t vs2,
                                   size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_tu(vint32m8_t vd, vint8m2_t vs1, vuint8m2_t vs2,
                                   size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_tu(vint32m8_t vd, int8_t rs1, vuint8m2_t vs2,
                                   size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_tu(vint64m1_t vd, vint16mf4_t vs1,
                                   vuint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_tu(vint64m1_t vd, int16_t rs1, vuint16mf4_t vs2,
                                   size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_tu(vint64m2_t vd, vint16mf2_t vs1,
                                   vuint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_tu(vint64m2_t vd, int16_t rs1, vuint16mf2_t vs2,
                                   size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_tu(vint64m4_t vd, vint16m1_t vs1,
                                   vuint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_tu(vint64m4_t vd, int16_t rs1, vuint16m1_t vs2,
                                   size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_tu(vint64m8_t vd, vint16m2_t vs1,
                                   vuint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_tu(vint64m8_t vd, int16_t rs1, vuint16m2_t vs2,
                                   size_t vl);
vint32mf2_t __riscv_nds_vqmaccus_tu(vint32mf2_t vd, uint8_t rs1, vint8mf8_t vs2,
                                    size_t vl);
vint32m1_t __riscv_nds_vqmaccus_tu(vint32m1_t vd, uint8_t rs1, vint8mf4_t vs2,
                                   size_t vl);
vint32m2_t __riscv_nds_vqmaccus_tu(vint32m2_t vd, uint8_t rs1, vint8mf2_t vs2,
                                   size_t vl);
vint32m4_t __riscv_nds_vqmaccus_tu(vint32m4_t vd, uint8_t rs1, vint8m1_t vs2,
                                   size_t vl);
vint32m8_t __riscv_nds_vqmaccus_tu(vint32m8_t vd, uint8_t rs1, vint8m2_t vs2,
                                   size_t vl);
vint64m1_t __riscv_nds_vqmaccus_tu(vint64m1_t vd, uint16_t rs1, vint16mf4_t vs2,
                                   size_t vl);
vint64m2_t __riscv_nds_vqmaccus_tu(vint64m2_t vd, uint16_t rs1, vint16mf2_t vs2,
                                   size_t vl);
vint64m4_t __riscv_nds_vqmaccus_tu(vint64m4_t vd, uint16_t rs1, vint16m1_t vs2,
                                   size_t vl);
vint64m8_t __riscv_nds_vqmaccus_tu(vint64m8_t vd, uint16_t rs1, vint16m2_t vs2,
                                   size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_tu(vuint32mf2_t vd, vuint8mf8_t vs1,
                                    vuint8mf8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_tu(vuint32mf2_t vd, uint8_t rs1,
                                    vuint8mf8_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_tu(vuint32m1_t vd, vuint8mf4_t vs1,
                                   vuint8mf4_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_tu(vuint32m1_t vd, uint8_t rs1, vuint8mf4_t vs2,
                                   size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_tu(vuint32m2_t vd, vuint8mf2_t vs1,
                                   vuint8mf2_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_tu(vuint32m2_t vd, uint8_t rs1, vuint8mf2_t vs2,
                                   size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_tu(vuint32m4_t vd, vuint8m1_t vs1,
                                   vuint8m1_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_tu(vuint32m4_t vd, uint8_t rs1, vuint8m1_t vs2,
                                   size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_tu(vuint32m8_t vd, vuint8m2_t vs1,
                                   vuint8m2_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_tu(vuint32m8_t vd, uint8_t rs1, vuint8m2_t vs2,
                                   size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_tu(vuint64m1_t vd, vuint16mf4_t vs1,
                                   vuint16mf4_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_tu(vuint64m1_t vd, uint16_t rs1,
                                   vuint16mf4_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_tu(vuint64m2_t vd, vuint16mf2_t vs1,
                                   vuint16mf2_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_tu(vuint64m2_t vd, uint16_t rs1,
                                   vuint16mf2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_tu(vuint64m4_t vd, vuint16m1_t vs1,
                                   vuint16m1_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_tu(vuint64m4_t vd, uint16_t rs1,
                                   vuint16m1_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_tu(vuint64m8_t vd, vuint16m2_t vs1,
                                   vuint16m2_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_tu(vuint64m8_t vd, uint16_t rs1,
                                   vuint16m2_t vs2, size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vqmacc_tum(vbool64_t vm, vint32mf2_t vd, vint8mf8_t vs1,
                                   vint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmacc_tum(vbool64_t vm, vint32mf2_t vd, int8_t rs1,
                                   vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc_tum(vbool32_t vm, vint32m1_t vd, vint8mf4_t vs1,
                                  vint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc_tum(vbool32_t vm, vint32m1_t vd, int8_t rs1,
                                  vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_tum(vbool16_t vm, vint32m2_t vd, vint8mf2_t vs1,
                                  vint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_tum(vbool16_t vm, vint32m2_t vd, int8_t rs1,
                                  vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_tum(vbool8_t vm, vint32m4_t vd, vint8m1_t vs1,
                                  vint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_tum(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                                  vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc_tum(vbool4_t vm, vint32m8_t vd, vint8m2_t vs1,
                                  vint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc_tum(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                                  vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_tum(vbool64_t vm, vint64m1_t vd, vint16mf4_t vs1,
                                  vint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_tum(vbool64_t vm, vint64m1_t vd, int16_t rs1,
                                  vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_tum(vbool32_t vm, vint64m2_t vd, vint16mf2_t vs1,
                                  vint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_tum(vbool32_t vm, vint64m2_t vd, int16_t rs1,
                                  vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc_tum(vbool16_t vm, vint64m4_t vd, vint16m1_t vs1,
                                  vint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc_tum(vbool16_t vm, vint64m4_t vd, int16_t rs1,
                                  vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc_tum(vbool8_t vm, vint64m8_t vd, vint16m2_t vs1,
                                  vint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc_tum(vbool8_t vm, vint64m8_t vd, int16_t rs1,
                                  vint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_tum(vbool64_t vm, vint32mf2_t vd,
                                     vint8mf8_t vs1, vuint8mf8_t vs2,
                                     size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_tum(vbool64_t vm, vint32mf2_t vd, int8_t rs1,
                                     vuint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_tum(vbool32_t vm, vint32m1_t vd, vint8mf4_t vs1,
                                    vuint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_tum(vbool32_t vm, vint32m1_t vd, int8_t rs1,
                                    vuint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_tum(vbool16_t vm, vint32m2_t vd, vint8mf2_t vs1,
                                    vuint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_tum(vbool16_t vm, vint32m2_t vd, int8_t rs1,
                                    vuint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_tum(vbool8_t vm, vint32m4_t vd, vint8m1_t vs1,
                                    vuint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_tum(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                                    vuint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_tum(vbool4_t vm, vint32m8_t vd, vint8m2_t vs1,
                                    vuint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_tum(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                                    vuint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_tum(vbool64_t vm, vint64m1_t vd,
                                    vint16mf4_t vs1, vuint16mf4_t vs2,
                                    size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_tum(vbool64_t vm, vint64m1_t vd, int16_t rs1,
                                    vuint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_tum(vbool32_t vm, vint64m2_t vd,
                                    vint16mf2_t vs1, vuint16mf2_t vs2,
                                    size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_tum(vbool32_t vm, vint64m2_t vd, int16_t rs1,
                                    vuint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_tum(vbool16_t vm, vint64m4_t vd, vint16m1_t vs1,
                                    vuint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_tum(vbool16_t vm, vint64m4_t vd, int16_t rs1,
                                    vuint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_tum(vbool8_t vm, vint64m8_t vd, vint16m2_t vs1,
                                    vuint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_tum(vbool8_t vm, vint64m8_t vd, int16_t rs1,
                                    vuint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccus_tum(vbool64_t vm, vint32mf2_t vd, uint8_t rs1,
                                     vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccus_tum(vbool32_t vm, vint32m1_t vd, uint8_t rs1,
                                    vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccus_tum(vbool16_t vm, vint32m2_t vd, uint8_t rs1,
                                    vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccus_tum(vbool8_t vm, vint32m4_t vd, uint8_t rs1,
                                    vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccus_tum(vbool4_t vm, vint32m8_t vd, uint8_t rs1,
                                    vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccus_tum(vbool64_t vm, vint64m1_t vd, uint16_t rs1,
                                    vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccus_tum(vbool32_t vm, vint64m2_t vd, uint16_t rs1,
                                    vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccus_tum(vbool16_t vm, vint64m4_t vd, uint16_t rs1,
                                    vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccus_tum(vbool8_t vm, vint64m8_t vd, uint16_t rs1,
                                    vint16m2_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_tum(vbool64_t vm, vuint32mf2_t vd,
                                     vuint8mf8_t vs1, vuint8mf8_t vs2,
                                     size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_tum(vbool64_t vm, vuint32mf2_t vd, uint8_t rs1,
                                     vuint8mf8_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_tum(vbool32_t vm, vuint32m1_t vd,
                                    vuint8mf4_t vs1, vuint8mf4_t vs2,
                                    size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_tum(vbool32_t vm, vuint32m1_t vd, uint8_t rs1,
                                    vuint8mf4_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_tum(vbool16_t vm, vuint32m2_t vd,
                                    vuint8mf2_t vs1, vuint8mf2_t vs2,
                                    size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_tum(vbool16_t vm, vuint32m2_t vd, uint8_t rs1,
                                    vuint8mf2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_tum(vbool8_t vm, vuint32m4_t vd, vuint8m1_t vs1,
                                    vuint8m1_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_tum(vbool8_t vm, vuint32m4_t vd, uint8_t rs1,
                                    vuint8m1_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_tum(vbool4_t vm, vuint32m8_t vd, vuint8m2_t vs1,
                                    vuint8m2_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_tum(vbool4_t vm, vuint32m8_t vd, uint8_t rs1,
                                    vuint8m2_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_tum(vbool64_t vm, vuint64m1_t vd,
                                    vuint16mf4_t vs1, vuint16mf4_t vs2,
                                    size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_tum(vbool64_t vm, vuint64m1_t vd, uint16_t rs1,
                                    vuint16mf4_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_tum(vbool32_t vm, vuint64m2_t vd,
                                    vuint16mf2_t vs1, vuint16mf2_t vs2,
                                    size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_tum(vbool32_t vm, vuint64m2_t vd, uint16_t rs1,
                                    vuint16mf2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_tum(vbool16_t vm, vuint64m4_t vd,
                                    vuint16m1_t vs1, vuint16m1_t vs2,
                                    size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_tum(vbool16_t vm, vuint64m4_t vd, uint16_t rs1,
                                    vuint16m1_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_tum(vbool8_t vm, vuint64m8_t vd,
                                    vuint16m2_t vs1, vuint16m2_t vs2,
                                    size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_tum(vbool8_t vm, vuint64m8_t vd, uint16_t rs1,
                                    vuint16m2_t vs2, size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vqmacc_tumu(vbool64_t vm, vint32mf2_t vd,
                                    vint8mf8_t vs1, vint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmacc_tumu(vbool64_t vm, vint32mf2_t vd, int8_t rs1,
                                    vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc_tumu(vbool32_t vm, vint32m1_t vd, vint8mf4_t vs1,
                                   vint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc_tumu(vbool32_t vm, vint32m1_t vd, int8_t rs1,
                                   vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_tumu(vbool16_t vm, vint32m2_t vd, vint8mf2_t vs1,
                                   vint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_tumu(vbool16_t vm, vint32m2_t vd, int8_t rs1,
                                   vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_tumu(vbool8_t vm, vint32m4_t vd, vint8m1_t vs1,
                                   vint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_tumu(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                                   vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc_tumu(vbool4_t vm, vint32m8_t vd, vint8m2_t vs1,
                                   vint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc_tumu(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                                   vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_tumu(vbool64_t vm, vint64m1_t vd, vint16mf4_t vs1,
                                   vint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_tumu(vbool64_t vm, vint64m1_t vd, int16_t rs1,
                                   vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_tumu(vbool32_t vm, vint64m2_t vd, vint16mf2_t vs1,
                                   vint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_tumu(vbool32_t vm, vint64m2_t vd, int16_t rs1,
                                   vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc_tumu(vbool16_t vm, vint64m4_t vd, vint16m1_t vs1,
                                   vint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc_tumu(vbool16_t vm, vint64m4_t vd, int16_t rs1,
                                   vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc_tumu(vbool8_t vm, vint64m8_t vd, vint16m2_t vs1,
                                   vint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc_tumu(vbool8_t vm, vint64m8_t vd, int16_t rs1,
                                   vint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_tumu(vbool64_t vm, vint32mf2_t vd,
                                      vint8mf8_t vs1, vuint8mf8_t vs2,
                                      size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_tumu(vbool64_t vm, vint32mf2_t vd, int8_t rs1,
                                      vuint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_tumu(vbool32_t vm, vint32m1_t vd,
                                     vint8mf4_t vs1, vuint8mf4_t vs2,
                                     size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_tumu(vbool32_t vm, vint32m1_t vd, int8_t rs1,
                                     vuint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_tumu(vbool16_t vm, vint32m2_t vd,
                                     vint8mf2_t vs1, vuint8mf2_t vs2,
                                     size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_tumu(vbool16_t vm, vint32m2_t vd, int8_t rs1,
                                     vuint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_tumu(vbool8_t vm, vint32m4_t vd, vint8m1_t vs1,
                                     vuint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_tumu(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                                     vuint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_tumu(vbool4_t vm, vint32m8_t vd, vint8m2_t vs1,
                                     vuint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_tumu(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                                     vuint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_tumu(vbool64_t vm, vint64m1_t vd,
                                     vint16mf4_t vs1, vuint16mf4_t vs2,
                                     size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_tumu(vbool64_t vm, vint64m1_t vd, int16_t rs1,
                                     vuint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_tumu(vbool32_t vm, vint64m2_t vd,
                                     vint16mf2_t vs1, vuint16mf2_t vs2,
                                     size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_tumu(vbool32_t vm, vint64m2_t vd, int16_t rs1,
                                     vuint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_tumu(vbool16_t vm, vint64m4_t vd,
                                     vint16m1_t vs1, vuint16m1_t vs2,
                                     size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_tumu(vbool16_t vm, vint64m4_t vd, int16_t rs1,
                                     vuint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_tumu(vbool8_t vm, vint64m8_t vd, vint16m2_t vs1,
                                     vuint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_tumu(vbool8_t vm, vint64m8_t vd, int16_t rs1,
                                     vuint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccus_tumu(vbool64_t vm, vint32mf2_t vd, uint8_t rs1,
                                      vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccus_tumu(vbool32_t vm, vint32m1_t vd, uint8_t rs1,
                                     vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccus_tumu(vbool16_t vm, vint32m2_t vd, uint8_t rs1,
                                     vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccus_tumu(vbool8_t vm, vint32m4_t vd, uint8_t rs1,
                                     vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccus_tumu(vbool4_t vm, vint32m8_t vd, uint8_t rs1,
                                     vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccus_tumu(vbool64_t vm, vint64m1_t vd, uint16_t rs1,
                                     vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccus_tumu(vbool32_t vm, vint64m2_t vd, uint16_t rs1,
                                     vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccus_tumu(vbool16_t vm, vint64m4_t vd, uint16_t rs1,
                                     vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccus_tumu(vbool8_t vm, vint64m8_t vd, uint16_t rs1,
                                     vint16m2_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_tumu(vbool64_t vm, vuint32mf2_t vd,
                                      vuint8mf8_t vs1, vuint8mf8_t vs2,
                                      size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_tumu(vbool64_t vm, vuint32mf2_t vd,
                                      uint8_t rs1, vuint8mf8_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_tumu(vbool32_t vm, vuint32m1_t vd,
                                     vuint8mf4_t vs1, vuint8mf4_t vs2,
                                     size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_tumu(vbool32_t vm, vuint32m1_t vd, uint8_t rs1,
                                     vuint8mf4_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_tumu(vbool16_t vm, vuint32m2_t vd,
                                     vuint8mf2_t vs1, vuint8mf2_t vs2,
                                     size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_tumu(vbool16_t vm, vuint32m2_t vd, uint8_t rs1,
                                     vuint8mf2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_tumu(vbool8_t vm, vuint32m4_t vd,
                                     vuint8m1_t vs1, vuint8m1_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_tumu(vbool8_t vm, vuint32m4_t vd, uint8_t rs1,
                                     vuint8m1_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_tumu(vbool4_t vm, vuint32m8_t vd,
                                     vuint8m2_t vs1, vuint8m2_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_tumu(vbool4_t vm, vuint32m8_t vd, uint8_t rs1,
                                     vuint8m2_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_tumu(vbool64_t vm, vuint64m1_t vd,
                                     vuint16mf4_t vs1, vuint16mf4_t vs2,
                                     size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_tumu(vbool64_t vm, vuint64m1_t vd, uint16_t rs1,
                                     vuint16mf4_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_tumu(vbool32_t vm, vuint64m2_t vd,
                                     vuint16mf2_t vs1, vuint16mf2_t vs2,
                                     size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_tumu(vbool32_t vm, vuint64m2_t vd, uint16_t rs1,
                                     vuint16mf2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_tumu(vbool16_t vm, vuint64m4_t vd,
                                     vuint16m1_t vs1, vuint16m1_t vs2,
                                     size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_tumu(vbool16_t vm, vuint64m4_t vd, uint16_t rs1,
                                     vuint16m1_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_tumu(vbool8_t vm, vuint64m8_t vd,
                                     vuint16m2_t vs1, vuint16m2_t vs2,
                                     size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_tumu(vbool8_t vm, vuint64m8_t vd, uint16_t rs1,
                                     vuint16m2_t vs2, size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vqmacc_mu(vbool64_t vm, vint32mf2_t vd, vint8mf8_t vs1,
                                  vint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmacc_mu(vbool64_t vm, vint32mf2_t vd, int8_t rs1,
                                  vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc_mu(vbool32_t vm, vint32m1_t vd, vint8mf4_t vs1,
                                 vint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmacc_mu(vbool32_t vm, vint32m1_t vd, int8_t rs1,
                                 vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_mu(vbool16_t vm, vint32m2_t vd, vint8mf2_t vs1,
                                 vint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmacc_mu(vbool16_t vm, vint32m2_t vd, int8_t rs1,
                                 vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_mu(vbool8_t vm, vint32m4_t vd, vint8m1_t vs1,
                                 vint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmacc_mu(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                                 vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc_mu(vbool4_t vm, vint32m8_t vd, vint8m2_t vs1,
                                 vint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmacc_mu(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                                 vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_mu(vbool64_t vm, vint64m1_t vd, vint16mf4_t vs1,
                                 vint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmacc_mu(vbool64_t vm, vint64m1_t vd, int16_t rs1,
                                 vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_mu(vbool32_t vm, vint64m2_t vd, vint16mf2_t vs1,
                                 vint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmacc_mu(vbool32_t vm, vint64m2_t vd, int16_t rs1,
                                 vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc_mu(vbool16_t vm, vint64m4_t vd, vint16m1_t vs1,
                                 vint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmacc_mu(vbool16_t vm, vint64m4_t vd, int16_t rs1,
                                 vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc_mu(vbool8_t vm, vint64m8_t vd, vint16m2_t vs1,
                                 vint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmacc_mu(vbool8_t vm, vint64m8_t vd, int16_t rs1,
                                 vint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_mu(vbool64_t vm, vint32mf2_t vd,
                                    vint8mf8_t vs1, vuint8mf8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccsu_mu(vbool64_t vm, vint32mf2_t vd, int8_t rs1,
                                    vuint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_mu(vbool32_t vm, vint32m1_t vd, vint8mf4_t vs1,
                                   vuint8mf4_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccsu_mu(vbool32_t vm, vint32m1_t vd, int8_t rs1,
                                   vuint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_mu(vbool16_t vm, vint32m2_t vd, vint8mf2_t vs1,
                                   vuint8mf2_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccsu_mu(vbool16_t vm, vint32m2_t vd, int8_t rs1,
                                   vuint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_mu(vbool8_t vm, vint32m4_t vd, vint8m1_t vs1,
                                   vuint8m1_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccsu_mu(vbool8_t vm, vint32m4_t vd, int8_t rs1,
                                   vuint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_mu(vbool4_t vm, vint32m8_t vd, vint8m2_t vs1,
                                   vuint8m2_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccsu_mu(vbool4_t vm, vint32m8_t vd, int8_t rs1,
                                   vuint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_mu(vbool64_t vm, vint64m1_t vd, vint16mf4_t vs1,
                                   vuint16mf4_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccsu_mu(vbool64_t vm, vint64m1_t vd, int16_t rs1,
                                   vuint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_mu(vbool32_t vm, vint64m2_t vd, vint16mf2_t vs1,
                                   vuint16mf2_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccsu_mu(vbool32_t vm, vint64m2_t vd, int16_t rs1,
                                   vuint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_mu(vbool16_t vm, vint64m4_t vd, vint16m1_t vs1,
                                   vuint16m1_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccsu_mu(vbool16_t vm, vint64m4_t vd, int16_t rs1,
                                   vuint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_mu(vbool8_t vm, vint64m8_t vd, vint16m2_t vs1,
                                   vuint16m2_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccsu_mu(vbool8_t vm, vint64m8_t vd, int16_t rs1,
                                   vuint16m2_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vqmaccus_mu(vbool64_t vm, vint32mf2_t vd, uint8_t rs1,
                                    vint8mf8_t vs2, size_t vl);
vint32m1_t __riscv_nds_vqmaccus_mu(vbool32_t vm, vint32m1_t vd, uint8_t rs1,
                                   vint8mf4_t vs2, size_t vl);
vint32m2_t __riscv_nds_vqmaccus_mu(vbool16_t vm, vint32m2_t vd, uint8_t rs1,
                                   vint8mf2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vqmaccus_mu(vbool8_t vm, vint32m4_t vd, uint8_t rs1,
                                   vint8m1_t vs2, size_t vl);
vint32m8_t __riscv_nds_vqmaccus_mu(vbool4_t vm, vint32m8_t vd, uint8_t rs1,
                                   vint8m2_t vs2, size_t vl);
vint64m1_t __riscv_nds_vqmaccus_mu(vbool64_t vm, vint64m1_t vd, uint16_t rs1,
                                   vint16mf4_t vs2, size_t vl);
vint64m2_t __riscv_nds_vqmaccus_mu(vbool32_t vm, vint64m2_t vd, uint16_t rs1,
                                   vint16mf2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vqmaccus_mu(vbool16_t vm, vint64m4_t vd, uint16_t rs1,
                                   vint16m1_t vs2, size_t vl);
vint64m8_t __riscv_nds_vqmaccus_mu(vbool8_t vm, vint64m8_t vd, uint16_t rs1,
                                   vint16m2_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_mu(vbool64_t vm, vuint32mf2_t vd,
                                    vuint8mf8_t vs1, vuint8mf8_t vs2,
                                    size_t vl);
vuint32mf2_t __riscv_nds_vqmaccu_mu(vbool64_t vm, vuint32mf2_t vd, uint8_t rs1,
                                    vuint8mf8_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_mu(vbool32_t vm, vuint32m1_t vd,
                                   vuint8mf4_t vs1, vuint8mf4_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vqmaccu_mu(vbool32_t vm, vuint32m1_t vd, uint8_t rs1,
                                   vuint8mf4_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_mu(vbool16_t vm, vuint32m2_t vd,
                                   vuint8mf2_t vs1, vuint8mf2_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vqmaccu_mu(vbool16_t vm, vuint32m2_t vd, uint8_t rs1,
                                   vuint8mf2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_mu(vbool8_t vm, vuint32m4_t vd, vuint8m1_t vs1,
                                   vuint8m1_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vqmaccu_mu(vbool8_t vm, vuint32m4_t vd, uint8_t rs1,
                                   vuint8m1_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_mu(vbool4_t vm, vuint32m8_t vd, vuint8m2_t vs1,
                                   vuint8m2_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vqmaccu_mu(vbool4_t vm, vuint32m8_t vd, uint8_t rs1,
                                   vuint8m2_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_mu(vbool64_t vm, vuint64m1_t vd,
                                   vuint16mf4_t vs1, vuint16mf4_t vs2,
                                   size_t vl);
vuint64m1_t __riscv_nds_vqmaccu_mu(vbool64_t vm, vuint64m1_t vd, uint16_t rs1,
                                   vuint16mf4_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_mu(vbool32_t vm, vuint64m2_t vd,
                                   vuint16mf2_t vs1, vuint16mf2_t vs2,
                                   size_t vl);
vuint64m2_t __riscv_nds_vqmaccu_mu(vbool32_t vm, vuint64m2_t vd, uint16_t rs1,
                                   vuint16mf2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_mu(vbool16_t vm, vuint64m4_t vd,
                                   vuint16m1_t vs1, vuint16m1_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vqmaccu_mu(vbool16_t vm, vuint64m4_t vd, uint16_t rs1,
                                   vuint16m1_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_mu(vbool8_t vm, vuint64m8_t vd, vuint16m2_t vs1,
                                   vuint16m2_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vqmaccu_mu(vbool8_t vm, vuint64m8_t vd, uint16_t rs1,
                                   vuint16m2_t vs2, size_t vl);
----

=== Andes Vector Dot Product Extension(XAndesVDot)

[[policy-variant-overloadedvector-dot-product-operations]]
==== Vector Dot Product Intrinsics(XAndesVDot)

[,c]
----
vint32mf2_t __riscv_nds_vd4dots_tu(vint32mf2_t vd, vint8mf2_t vs1,
                                   vint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dots_tu(vint32m1_t vd, vint8m1_t vs1, vint8m1_t vs2,
                                  size_t vl);
vint32m2_t __riscv_nds_vd4dots_tu(vint32m2_t vd, vint8m2_t vs1, vint8m2_t vs2,
                                  size_t vl);
vint32m4_t __riscv_nds_vd4dots_tu(vint32m4_t vd, vint8m4_t vs1, vint8m4_t vs2,
                                  size_t vl);
vint32m8_t __riscv_nds_vd4dots_tu(vint32m8_t vd, vint8m8_t vs1, vint8m8_t vs2,
                                  size_t vl);
vint64m1_t __riscv_nds_vd4dots_tu(vint64m1_t vd, vint16m1_t vs1, vint16m1_t vs2,
                                  size_t vl);
vint64m2_t __riscv_nds_vd4dots_tu(vint64m2_t vd, vint16m2_t vs1, vint16m2_t vs2,
                                  size_t vl);
vint64m4_t __riscv_nds_vd4dots_tu(vint64m4_t vd, vint16m4_t vs1, vint16m4_t vs2,
                                  size_t vl);
vint64m8_t __riscv_nds_vd4dots_tu(vint64m8_t vd, vint16m8_t vs1, vint16m8_t vs2,
                                  size_t vl);
vint32mf2_t __riscv_nds_vd4dotsu_tu(vint32mf2_t vd, vint8mf2_t vs1,
                                    vuint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dotsu_tu(vint32m1_t vd, vint8m1_t vs1, vuint8m1_t vs2,
                                   size_t vl);
vint32m2_t __riscv_nds_vd4dotsu_tu(vint32m2_t vd, vint8m2_t vs1, vuint8m2_t vs2,
                                   size_t vl);
vint32m4_t __riscv_nds_vd4dotsu_tu(vint32m4_t vd, vint8m4_t vs1, vuint8m4_t vs2,
                                   size_t vl);
vint32m8_t __riscv_nds_vd4dotsu_tu(vint32m8_t vd, vint8m8_t vs1, vuint8m8_t vs2,
                                   size_t vl);
vint64m1_t __riscv_nds_vd4dotsu_tu(vint64m1_t vd, vint16m1_t vs1,
                                   vuint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dotsu_tu(vint64m2_t vd, vint16m2_t vs1,
                                   vuint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dotsu_tu(vint64m4_t vd, vint16m4_t vs1,
                                   vuint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dotsu_tu(vint64m8_t vd, vint16m8_t vs1,
                                   vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vd4dotu_tu(vuint32mf2_t vd, vuint8mf2_t vs1,
                                    vuint8mf2_t vs2, size_t vl);
vuint32m1_t __riscv_nds_vd4dotu_tu(vuint32m1_t vd, vuint8m1_t vs1,
                                   vuint8m1_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vd4dotu_tu(vuint32m2_t vd, vuint8m2_t vs1,
                                   vuint8m2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vd4dotu_tu(vuint32m4_t vd, vuint8m4_t vs1,
                                   vuint8m4_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vd4dotu_tu(vuint32m8_t vd, vuint8m8_t vs1,
                                   vuint8m8_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vd4dotu_tu(vuint64m1_t vd, vuint16m1_t vs1,
                                   vuint16m1_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vd4dotu_tu(vuint64m2_t vd, vuint16m2_t vs1,
                                   vuint16m2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vd4dotu_tu(vuint64m4_t vd, vuint16m4_t vs1,
                                   vuint16m4_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vd4dotu_tu(vuint64m8_t vd, vuint16m8_t vs1,
                                   vuint16m8_t vs2, size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vd4dots_tum(vbool64_t vm, vint32mf2_t vd,
                                    vint8mf2_t vs1, vint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dots_tum(vbool32_t vm, vint32m1_t vd, vint8m1_t vs1,
                                   vint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dots_tum(vbool16_t vm, vint32m2_t vd, vint8m2_t vs1,
                                   vint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dots_tum(vbool8_t vm, vint32m4_t vd, vint8m4_t vs1,
                                   vint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dots_tum(vbool4_t vm, vint32m8_t vd, vint8m8_t vs1,
                                   vint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dots_tum(vbool64_t vm, vint64m1_t vd, vint16m1_t vs1,
                                   vint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dots_tum(vbool32_t vm, vint64m2_t vd, vint16m2_t vs1,
                                   vint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dots_tum(vbool16_t vm, vint64m4_t vd, vint16m4_t vs1,
                                   vint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dots_tum(vbool8_t vm, vint64m8_t vd, vint16m8_t vs1,
                                   vint16m8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vd4dotsu_tum(vbool64_t vm, vint32mf2_t vd,
                                     vint8mf2_t vs1, vuint8mf2_t vs2,
                                     size_t vl);
vint32m1_t __riscv_nds_vd4dotsu_tum(vbool32_t vm, vint32m1_t vd, vint8m1_t vs1,
                                    vuint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dotsu_tum(vbool16_t vm, vint32m2_t vd, vint8m2_t vs1,
                                    vuint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dotsu_tum(vbool8_t vm, vint32m4_t vd, vint8m4_t vs1,
                                    vuint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dotsu_tum(vbool4_t vm, vint32m8_t vd, vint8m8_t vs1,
                                    vuint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dotsu_tum(vbool64_t vm, vint64m1_t vd, vint16m1_t vs1,
                                    vuint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dotsu_tum(vbool32_t vm, vint64m2_t vd, vint16m2_t vs1,
                                    vuint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dotsu_tum(vbool16_t vm, vint64m4_t vd, vint16m4_t vs1,
                                    vuint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dotsu_tum(vbool8_t vm, vint64m8_t vd, vint16m8_t vs1,
                                    vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vd4dotu_tum(vbool64_t vm, vuint32mf2_t vd,
                                     vuint8mf2_t vs1, vuint8mf2_t vs2,
                                     size_t vl);
vuint32m1_t __riscv_nds_vd4dotu_tum(vbool32_t vm, vuint32m1_t vd,
                                    vuint8m1_t vs1, vuint8m1_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vd4dotu_tum(vbool16_t vm, vuint32m2_t vd,
                                    vuint8m2_t vs1, vuint8m2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vd4dotu_tum(vbool8_t vm, vuint32m4_t vd, vuint8m4_t vs1,
                                    vuint8m4_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vd4dotu_tum(vbool4_t vm, vuint32m8_t vd, vuint8m8_t vs1,
                                    vuint8m8_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vd4dotu_tum(vbool64_t vm, vuint64m1_t vd,
                                    vuint16m1_t vs1, vuint16m1_t vs2,
                                    size_t vl);
vuint64m2_t __riscv_nds_vd4dotu_tum(vbool32_t vm, vuint64m2_t vd,
                                    vuint16m2_t vs1, vuint16m2_t vs2,
                                    size_t vl);
vuint64m4_t __riscv_nds_vd4dotu_tum(vbool16_t vm, vuint64m4_t vd,
                                    vuint16m4_t vs1, vuint16m4_t vs2,
                                    size_t vl);
vuint64m8_t __riscv_nds_vd4dotu_tum(vbool8_t vm, vuint64m8_t vd,
                                    vuint16m8_t vs1, vuint16m8_t vs2,
                                    size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vd4dots_tumu(vbool64_t vm, vint32mf2_t vd,
                                     vint8mf2_t vs1, vint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dots_tumu(vbool32_t vm, vint32m1_t vd, vint8m1_t vs1,
                                    vint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dots_tumu(vbool16_t vm, vint32m2_t vd, vint8m2_t vs1,
                                    vint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dots_tumu(vbool8_t vm, vint32m4_t vd, vint8m4_t vs1,
                                    vint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dots_tumu(vbool4_t vm, vint32m8_t vd, vint8m8_t vs1,
                                    vint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dots_tumu(vbool64_t vm, vint64m1_t vd, vint16m1_t vs1,
                                    vint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dots_tumu(vbool32_t vm, vint64m2_t vd, vint16m2_t vs1,
                                    vint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dots_tumu(vbool16_t vm, vint64m4_t vd, vint16m4_t vs1,
                                    vint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dots_tumu(vbool8_t vm, vint64m8_t vd, vint16m8_t vs1,
                                    vint16m8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vd4dotsu_tumu(vbool64_t vm, vint32mf2_t vd,
                                      vint8mf2_t vs1, vuint8mf2_t vs2,
                                      size_t vl);
vint32m1_t __riscv_nds_vd4dotsu_tumu(vbool32_t vm, vint32m1_t vd, vint8m1_t vs1,
                                     vuint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dotsu_tumu(vbool16_t vm, vint32m2_t vd, vint8m2_t vs1,
                                     vuint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dotsu_tumu(vbool8_t vm, vint32m4_t vd, vint8m4_t vs1,
                                     vuint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dotsu_tumu(vbool4_t vm, vint32m8_t vd, vint8m8_t vs1,
                                     vuint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dotsu_tumu(vbool64_t vm, vint64m1_t vd,
                                     vint16m1_t vs1, vuint16m1_t vs2,
                                     size_t vl);
vint64m2_t __riscv_nds_vd4dotsu_tumu(vbool32_t vm, vint64m2_t vd,
                                     vint16m2_t vs1, vuint16m2_t vs2,
                                     size_t vl);
vint64m4_t __riscv_nds_vd4dotsu_tumu(vbool16_t vm, vint64m4_t vd,
                                     vint16m4_t vs1, vuint16m4_t vs2,
                                     size_t vl);
vint64m8_t __riscv_nds_vd4dotsu_tumu(vbool8_t vm, vint64m8_t vd, vint16m8_t vs1,
                                     vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vd4dotu_tumu(vbool64_t vm, vuint32mf2_t vd,
                                      vuint8mf2_t vs1, vuint8mf2_t vs2,
                                      size_t vl);
vuint32m1_t __riscv_nds_vd4dotu_tumu(vbool32_t vm, vuint32m1_t vd,
                                     vuint8m1_t vs1, vuint8m1_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vd4dotu_tumu(vbool16_t vm, vuint32m2_t vd,
                                     vuint8m2_t vs1, vuint8m2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vd4dotu_tumu(vbool8_t vm, vuint32m4_t vd,
                                     vuint8m4_t vs1, vuint8m4_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vd4dotu_tumu(vbool4_t vm, vuint32m8_t vd,
                                     vuint8m8_t vs1, vuint8m8_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vd4dotu_tumu(vbool64_t vm, vuint64m1_t vd,
                                     vuint16m1_t vs1, vuint16m1_t vs2,
                                     size_t vl);
vuint64m2_t __riscv_nds_vd4dotu_tumu(vbool32_t vm, vuint64m2_t vd,
                                     vuint16m2_t vs1, vuint16m2_t vs2,
                                     size_t vl);
vuint64m4_t __riscv_nds_vd4dotu_tumu(vbool16_t vm, vuint64m4_t vd,
                                     vuint16m4_t vs1, vuint16m4_t vs2,
                                     size_t vl);
vuint64m8_t __riscv_nds_vd4dotu_tumu(vbool8_t vm, vuint64m8_t vd,
                                     vuint16m8_t vs1, vuint16m8_t vs2,
                                     size_t vl);
// masked functions
vint32mf2_t __riscv_nds_vd4dots_mu(vbool64_t vm, vint32mf2_t vd, vint8mf2_t vs1,
                                   vint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dots_mu(vbool32_t vm, vint32m1_t vd, vint8m1_t vs1,
                                  vint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dots_mu(vbool16_t vm, vint32m2_t vd, vint8m2_t vs1,
                                  vint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dots_mu(vbool8_t vm, vint32m4_t vd, vint8m4_t vs1,
                                  vint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dots_mu(vbool4_t vm, vint32m8_t vd, vint8m8_t vs1,
                                  vint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dots_mu(vbool64_t vm, vint64m1_t vd, vint16m1_t vs1,
                                  vint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dots_mu(vbool32_t vm, vint64m2_t vd, vint16m2_t vs1,
                                  vint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dots_mu(vbool16_t vm, vint64m4_t vd, vint16m4_t vs1,
                                  vint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dots_mu(vbool8_t vm, vint64m8_t vd, vint16m8_t vs1,
                                  vint16m8_t vs2, size_t vl);
vint32mf2_t __riscv_nds_vd4dotsu_mu(vbool64_t vm, vint32mf2_t vd,
                                    vint8mf2_t vs1, vuint8mf2_t vs2, size_t vl);
vint32m1_t __riscv_nds_vd4dotsu_mu(vbool32_t vm, vint32m1_t vd, vint8m1_t vs1,
                                   vuint8m1_t vs2, size_t vl);
vint32m2_t __riscv_nds_vd4dotsu_mu(vbool16_t vm, vint32m2_t vd, vint8m2_t vs1,
                                   vuint8m2_t vs2, size_t vl);
vint32m4_t __riscv_nds_vd4dotsu_mu(vbool8_t vm, vint32m4_t vd, vint8m4_t vs1,
                                   vuint8m4_t vs2, size_t vl);
vint32m8_t __riscv_nds_vd4dotsu_mu(vbool4_t vm, vint32m8_t vd, vint8m8_t vs1,
                                   vuint8m8_t vs2, size_t vl);
vint64m1_t __riscv_nds_vd4dotsu_mu(vbool64_t vm, vint64m1_t vd, vint16m1_t vs1,
                                   vuint16m1_t vs2, size_t vl);
vint64m2_t __riscv_nds_vd4dotsu_mu(vbool32_t vm, vint64m2_t vd, vint16m2_t vs1,
                                   vuint16m2_t vs2, size_t vl);
vint64m4_t __riscv_nds_vd4dotsu_mu(vbool16_t vm, vint64m4_t vd, vint16m4_t vs1,
                                   vuint16m4_t vs2, size_t vl);
vint64m8_t __riscv_nds_vd4dotsu_mu(vbool8_t vm, vint64m8_t vd, vint16m8_t vs1,
                                   vuint16m8_t vs2, size_t vl);
vuint32mf2_t __riscv_nds_vd4dotu_mu(vbool64_t vm, vuint32mf2_t vd,
                                    vuint8mf2_t vs1, vuint8mf2_t vs2,
                                    size_t vl);
vuint32m1_t __riscv_nds_vd4dotu_mu(vbool32_t vm, vuint32m1_t vd, vuint8m1_t vs1,
                                   vuint8m1_t vs2, size_t vl);
vuint32m2_t __riscv_nds_vd4dotu_mu(vbool16_t vm, vuint32m2_t vd, vuint8m2_t vs1,
                                   vuint8m2_t vs2, size_t vl);
vuint32m4_t __riscv_nds_vd4dotu_mu(vbool8_t vm, vuint32m4_t vd, vuint8m4_t vs1,
                                   vuint8m4_t vs2, size_t vl);
vuint32m8_t __riscv_nds_vd4dotu_mu(vbool4_t vm, vuint32m8_t vd, vuint8m8_t vs1,
                                   vuint8m8_t vs2, size_t vl);
vuint64m1_t __riscv_nds_vd4dotu_mu(vbool64_t vm, vuint64m1_t vd,
                                   vuint16m1_t vs1, vuint16m1_t vs2, size_t vl);
vuint64m2_t __riscv_nds_vd4dotu_mu(vbool32_t vm, vuint64m2_t vd,
                                   vuint16m2_t vs1, vuint16m2_t vs2, size_t vl);
vuint64m4_t __riscv_nds_vd4dotu_mu(vbool16_t vm, vuint64m4_t vd,
                                   vuint16m4_t vs1, vuint16m4_t vs2, size_t vl);
vuint64m8_t __riscv_nds_vd4dotu_mu(vbool8_t vm, vuint64m8_t vd, vuint16m8_t vs1,
                                   vuint16m8_t vs2, size_t vl);
----

=== Andes Vector Packed FP16 Extension(XAndesVPackFPH)

[[policy-variant-overloadedvector-packed-fp16-operations]]
==== Vector Packed FP16 Intrinsics(XAndesVPackFPH)

[,c]
----
vfloat16mf4_t __riscv_nds_vfpmadt_tu(vfloat16mf4_t vd, vfloat16mf4_t vs2,
                                     float32_t rs1, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_tu(vfloat16mf2_t vd, vfloat16mf2_t vs2,
                                     float32_t rs1, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                    float32_t rs1, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                    float32_t rs1, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                    float32_t rs1, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                    float32_t rs1, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_tu(vfloat16mf4_t vd, vfloat16mf4_t vs2,
                                     float32_t rs1, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_tu(vfloat16mf2_t vd, vfloat16mf2_t vs2,
                                     float32_t rs1, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                    float32_t rs1, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                    float32_t rs1, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                    float32_t rs1, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                    float32_t rs1, size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt_tum(vbool64_t vm, vfloat16mf4_t vd,
                                      vfloat16mf4_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_tum(vbool32_t vm, vfloat16mf2_t vd,
                                      vfloat16mf2_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_tum(vbool16_t vm, vfloat16m1_t vd,
                                     vfloat16m1_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_tum(vbool8_t vm, vfloat16m2_t vd,
                                     vfloat16m2_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_tum(vbool4_t vm, vfloat16m4_t vd,
                                     vfloat16m4_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_tum(vbool2_t vm, vfloat16m8_t vd,
                                     vfloat16m8_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_tum(vbool64_t vm, vfloat16mf4_t vd,
                                      vfloat16mf4_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_tum(vbool32_t vm, vfloat16mf2_t vd,
                                      vfloat16mf2_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_tum(vbool16_t vm, vfloat16m1_t vd,
                                     vfloat16m1_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_tum(vbool8_t vm, vfloat16m2_t vd,
                                     vfloat16m2_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_tum(vbool4_t vm, vfloat16m4_t vd,
                                     vfloat16m4_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_tum(vbool2_t vm, vfloat16m8_t vd,
                                     vfloat16m8_t vs2, float32_t rs1,
                                     size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                       vfloat16mf4_t vs2, float32_t rs1,
                                       size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                       vfloat16mf2_t vs2, float32_t rs1,
                                       size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_tumu(vbool16_t vm, vfloat16m1_t vd,
                                      vfloat16m1_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_tumu(vbool8_t vm, vfloat16m2_t vd,
                                      vfloat16m2_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_tumu(vbool4_t vm, vfloat16m4_t vd,
                                      vfloat16m4_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_tumu(vbool2_t vm, vfloat16m8_t vd,
                                      vfloat16m8_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                       vfloat16mf4_t vs2, float32_t rs1,
                                       size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                       vfloat16mf2_t vs2, float32_t rs1,
                                       size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_tumu(vbool16_t vm, vfloat16m1_t vd,
                                      vfloat16m1_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_tumu(vbool8_t vm, vfloat16m2_t vd,
                                      vfloat16m2_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_tumu(vbool4_t vm, vfloat16m4_t vd,
                                      vfloat16m4_t vs2, float32_t rs1,
                                      size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_tumu(vbool2_t vm, vfloat16m8_t vd,
                                      vfloat16m8_t vs2, float32_t rs1,
                                      size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt_mu(vbool64_t vm, vfloat16mf4_t vd,
                                     vfloat16mf4_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_mu(vbool32_t vm, vfloat16mf2_t vd,
                                     vfloat16mf2_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_mu(vbool16_t vm, vfloat16m1_t vd,
                                    vfloat16m1_t vs2, float32_t rs1, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_mu(vbool8_t vm, vfloat16m2_t vd,
                                    vfloat16m2_t vs2, float32_t rs1, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_mu(vbool4_t vm, vfloat16m4_t vd,
                                    vfloat16m4_t vs2, float32_t rs1, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_mu(vbool2_t vm, vfloat16m8_t vd,
                                    vfloat16m8_t vs2, float32_t rs1, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_mu(vbool64_t vm, vfloat16mf4_t vd,
                                     vfloat16mf4_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_mu(vbool32_t vm, vfloat16mf2_t vd,
                                     vfloat16mf2_t vs2, float32_t rs1,
                                     size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_mu(vbool16_t vm, vfloat16m1_t vd,
                                    vfloat16m1_t vs2, float32_t rs1, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_mu(vbool8_t vm, vfloat16m2_t vd,
                                    vfloat16m2_t vs2, float32_t rs1, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_mu(vbool4_t vm, vfloat16m4_t vd,
                                    vfloat16m4_t vs2, float32_t rs1, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_mu(vbool2_t vm, vfloat16m8_t vd,
                                    vfloat16m8_t vs2, float32_t rs1, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadt_tu(vfloat16mf4_t vd, vfloat16mf4_t vs2,
                                     float32_t rs1, unsigned int frm,
                                     size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_tu(vfloat16mf2_t vd, vfloat16mf2_t vs2,
                                     float32_t rs1, unsigned int frm,
                                     size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                    float32_t rs1, unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                    float32_t rs1, unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                    float32_t rs1, unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                    float32_t rs1, unsigned int frm, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_tu(vfloat16mf4_t vd, vfloat16mf4_t vs2,
                                     float32_t rs1, unsigned int frm,
                                     size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_tu(vfloat16mf2_t vd, vfloat16mf2_t vs2,
                                     float32_t rs1, unsigned int frm,
                                     size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_tu(vfloat16m1_t vd, vfloat16m1_t vs2,
                                    float32_t rs1, unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_tu(vfloat16m2_t vd, vfloat16m2_t vs2,
                                    float32_t rs1, unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_tu(vfloat16m4_t vd, vfloat16m4_t vs2,
                                    float32_t rs1, unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_tu(vfloat16m8_t vd, vfloat16m8_t vs2,
                                    float32_t rs1, unsigned int frm, size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt_tum(vbool64_t vm, vfloat16mf4_t vd,
                                      vfloat16mf4_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_tum(vbool32_t vm, vfloat16mf2_t vd,
                                      vfloat16mf2_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_tum(vbool16_t vm, vfloat16m1_t vd,
                                     vfloat16m1_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_tum(vbool8_t vm, vfloat16m2_t vd,
                                     vfloat16m2_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_tum(vbool4_t vm, vfloat16m4_t vd,
                                     vfloat16m4_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_tum(vbool2_t vm, vfloat16m8_t vd,
                                     vfloat16m8_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_tum(vbool64_t vm, vfloat16mf4_t vd,
                                      vfloat16mf4_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_tum(vbool32_t vm, vfloat16mf2_t vd,
                                      vfloat16mf2_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_tum(vbool16_t vm, vfloat16m1_t vd,
                                     vfloat16m1_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_tum(vbool8_t vm, vfloat16m2_t vd,
                                     vfloat16m2_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_tum(vbool4_t vm, vfloat16m4_t vd,
                                     vfloat16m4_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_tum(vbool2_t vm, vfloat16m8_t vd,
                                     vfloat16m8_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                       vfloat16mf4_t vs2, float32_t rs1,
                                       unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                       vfloat16mf2_t vs2, float32_t rs1,
                                       unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_tumu(vbool16_t vm, vfloat16m1_t vd,
                                      vfloat16m1_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_tumu(vbool8_t vm, vfloat16m2_t vd,
                                      vfloat16m2_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_tumu(vbool4_t vm, vfloat16m4_t vd,
                                      vfloat16m4_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_tumu(vbool2_t vm, vfloat16m8_t vd,
                                      vfloat16m8_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_tumu(vbool64_t vm, vfloat16mf4_t vd,
                                       vfloat16mf4_t vs2, float32_t rs1,
                                       unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_tumu(vbool32_t vm, vfloat16mf2_t vd,
                                       vfloat16mf2_t vs2, float32_t rs1,
                                       unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_tumu(vbool16_t vm, vfloat16m1_t vd,
                                      vfloat16m1_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_tumu(vbool8_t vm, vfloat16m2_t vd,
                                      vfloat16m2_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_tumu(vbool4_t vm, vfloat16m4_t vd,
                                      vfloat16m4_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_tumu(vbool2_t vm, vfloat16m8_t vd,
                                      vfloat16m8_t vs2, float32_t rs1,
                                      unsigned int frm, size_t vl);
// masked functions
vfloat16mf4_t __riscv_nds_vfpmadt_mu(vbool64_t vm, vfloat16mf4_t vd,
                                     vfloat16mf4_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadt_mu(vbool32_t vm, vfloat16mf2_t vd,
                                     vfloat16mf2_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadt_mu(vbool16_t vm, vfloat16m1_t vd,
                                    vfloat16m1_t vs2, float32_t rs1,
                                    unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadt_mu(vbool8_t vm, vfloat16m2_t vd,
                                    vfloat16m2_t vs2, float32_t rs1,
                                    unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadt_mu(vbool4_t vm, vfloat16m4_t vd,
                                    vfloat16m4_t vs2, float32_t rs1,
                                    unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadt_mu(vbool2_t vm, vfloat16m8_t vd,
                                    vfloat16m8_t vs2, float32_t rs1,
                                    unsigned int frm, size_t vl);
vfloat16mf4_t __riscv_nds_vfpmadb_mu(vbool64_t vm, vfloat16mf4_t vd,
                                     vfloat16mf4_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16mf2_t __riscv_nds_vfpmadb_mu(vbool32_t vm, vfloat16mf2_t vd,
                                     vfloat16mf2_t vs2, float32_t rs1,
                                     unsigned int frm, size_t vl);
vfloat16m1_t __riscv_nds_vfpmadb_mu(vbool16_t vm, vfloat16m1_t vd,
                                    vfloat16m1_t vs2, float32_t rs1,
                                    unsigned int frm, size_t vl);
vfloat16m2_t __riscv_nds_vfpmadb_mu(vbool8_t vm, vfloat16m2_t vd,
                                    vfloat16m2_t vs2, float32_t rs1,
                                    unsigned int frm, size_t vl);
vfloat16m4_t __riscv_nds_vfpmadb_mu(vbool4_t vm, vfloat16m4_t vd,
                                    vfloat16m4_t vs2, float32_t rs1,
                                    unsigned int frm, size_t vl);
vfloat16m8_t __riscv_nds_vfpmadb_mu(vbool2_t vm, vfloat16m8_t vd,
                                    vfloat16m8_t vs2, float32_t rs1,
                                    unsigned int frm, size_t vl);
----

=== Andes Vector INT4 Load Extension(XAndesVSIntLoad)

[[policy-variant-overloadedvector-int4-load-operations]]
==== Andes Vector INT4 Load Intrinsics(XAndesVSIntLoad)

[,c]
----
vint8mf8_t __riscv_nds_vln8_tu(vint8mf8_t vd, const void *rs1, size_t vl);
vint8mf4_t __riscv_nds_vln8_tu(vint8mf4_t vd, const void *rs1, size_t vl);
vint8mf2_t __riscv_nds_vln8_tu(vint8mf2_t vd, const void *rs1, size_t vl);
vint8m1_t __riscv_nds_vln8_tu(vint8m1_t vd, const void *rs1, size_t vl);
vint8m2_t __riscv_nds_vln8_tu(vint8m2_t vd, const void *rs1, size_t vl);
vint8m4_t __riscv_nds_vln8_tu(vint8m4_t vd, const void *rs1, size_t vl);
vint8m8_t __riscv_nds_vln8_tu(vint8m8_t vd, const void *rs1, size_t vl);
vuint8mf8_t __riscv_nds_vlnu8_tu(vuint8mf8_t vd, const void *rs1, size_t vl);
vuint8mf4_t __riscv_nds_vlnu8_tu(vuint8mf4_t vd, const void *rs1, size_t vl);
vuint8mf2_t __riscv_nds_vlnu8_tu(vuint8mf2_t vd, const void *rs1, size_t vl);
vuint8m1_t __riscv_nds_vlnu8_tu(vuint8m1_t vd, const void *rs1, size_t vl);
vuint8m2_t __riscv_nds_vlnu8_tu(vuint8m2_t vd, const void *rs1, size_t vl);
vuint8m4_t __riscv_nds_vlnu8_tu(vuint8m4_t vd, const void *rs1, size_t vl);
vuint8m8_t __riscv_nds_vlnu8_tu(vuint8m8_t vd, const void *rs1, size_t vl);
// masked functions
vint8mf8_t __riscv_nds_vln8_tum(vbool64_t vm, vint8mf8_t vd, const void *rs1,
                                size_t vl);
vint8mf4_t __riscv_nds_vln8_tum(vbool32_t vm, vint8mf4_t vd, const void *rs1,
                                size_t vl);
vint8mf2_t __riscv_nds_vln8_tum(vbool16_t vm, vint8mf2_t vd, const void *rs1,
                                size_t vl);
vint8m1_t __riscv_nds_vln8_tum(vbool8_t vm, vint8m1_t vd, const void *rs1,
                               size_t vl);
vint8m2_t __riscv_nds_vln8_tum(vbool4_t vm, vint8m2_t vd, const void *rs1,
                               size_t vl);
vint8m4_t __riscv_nds_vln8_tum(vbool2_t vm, vint8m4_t vd, const void *rs1,
                               size_t vl);
vint8m8_t __riscv_nds_vln8_tum(vbool1_t vm, vint8m8_t vd, const void *rs1,
                               size_t vl);
vuint8mf8_t __riscv_nds_vlnu8_tum(vbool64_t vm, vuint8mf8_t vd, const void *rs1,
                                  size_t vl);
vuint8mf4_t __riscv_nds_vlnu8_tum(vbool32_t vm, vuint8mf4_t vd, const void *rs1,
                                  size_t vl);
vuint8mf2_t __riscv_nds_vlnu8_tum(vbool16_t vm, vuint8mf2_t vd, const void *rs1,
                                  size_t vl);
vuint8m1_t __riscv_nds_vlnu8_tum(vbool8_t vm, vuint8m1_t vd, const void *rs1,
                                 size_t vl);
vuint8m2_t __riscv_nds_vlnu8_tum(vbool4_t vm, vuint8m2_t vd, const void *rs1,
                                 size_t vl);
vuint8m4_t __riscv_nds_vlnu8_tum(vbool2_t vm, vuint8m4_t vd, const void *rs1,
                                 size_t vl);
vuint8m8_t __riscv_nds_vlnu8_tum(vbool1_t vm, vuint8m8_t vd, const void *rs1,
                                 size_t vl);
// masked functions
vint8mf8_t __riscv_nds_vln8_tumu(vbool64_t vm, vint8mf8_t vd, const void *rs1,
                                 size_t vl);
vint8mf4_t __riscv_nds_vln8_tumu(vbool32_t vm, vint8mf4_t vd, const void *rs1,
                                 size_t vl);
vint8mf2_t __riscv_nds_vln8_tumu(vbool16_t vm, vint8mf2_t vd, const void *rs1,
                                 size_t vl);
vint8m1_t __riscv_nds_vln8_tumu(vbool8_t vm, vint8m1_t vd, const void *rs1,
                                size_t vl);
vint8m2_t __riscv_nds_vln8_tumu(vbool4_t vm, vint8m2_t vd, const void *rs1,
                                size_t vl);
vint8m4_t __riscv_nds_vln8_tumu(vbool2_t vm, vint8m4_t vd, const void *rs1,
                                size_t vl);
vint8m8_t __riscv_nds_vln8_tumu(vbool1_t vm, vint8m8_t vd, const void *rs1,
                                size_t vl);
vuint8mf8_t __riscv_nds_vlnu8_tumu(vbool64_t vm, vuint8mf8_t vd,
                                   const void *rs1, size_t vl);
vuint8mf4_t __riscv_nds_vlnu8_tumu(vbool32_t vm, vuint8mf4_t vd,
                                   const void *rs1, size_t vl);
vuint8mf2_t __riscv_nds_vlnu8_tumu(vbool16_t vm, vuint8mf2_t vd,
                                   const void *rs1, size_t vl);
vuint8m1_t __riscv_nds_vlnu8_tumu(vbool8_t vm, vuint8m1_t vd, const void *rs1,
                                  size_t vl);
vuint8m2_t __riscv_nds_vlnu8_tumu(vbool4_t vm, vuint8m2_t vd, const void *rs1,
                                  size_t vl);
vuint8m4_t __riscv_nds_vlnu8_tumu(vbool2_t vm, vuint8m4_t vd, const void *rs1,
                                  size_t vl);
vuint8m8_t __riscv_nds_vlnu8_tumu(vbool1_t vm, vuint8m8_t vd, const void *rs1,
                                  size_t vl);
// masked functions
vint8mf8_t __riscv_nds_vln8_mu(vbool64_t vm, vint8mf8_t vd, const void *rs1,
                               size_t vl);
vint8mf4_t __riscv_nds_vln8_mu(vbool32_t vm, vint8mf4_t vd, const void *rs1,
                               size_t vl);
vint8mf2_t __riscv_nds_vln8_mu(vbool16_t vm, vint8mf2_t vd, const void *rs1,
                               size_t vl);
vint8m1_t __riscv_nds_vln8_mu(vbool8_t vm, vint8m1_t vd, const void *rs1,
                              size_t vl);
vint8m2_t __riscv_nds_vln8_mu(vbool4_t vm, vint8m2_t vd, const void *rs1,
                              size_t vl);
vint8m4_t __riscv_nds_vln8_mu(vbool2_t vm, vint8m4_t vd, const void *rs1,
                              size_t vl);
vint8m8_t __riscv_nds_vln8_mu(vbool1_t vm, vint8m8_t vd, const void *rs1,
                              size_t vl);
vuint8mf8_t __riscv_nds_vlnu8_mu(vbool64_t vm, vuint8mf8_t vd, const void *rs1,
                                 size_t vl);
vuint8mf4_t __riscv_nds_vlnu8_mu(vbool32_t vm, vuint8mf4_t vd, const void *rs1,
                                 size_t vl);
vuint8mf2_t __riscv_nds_vlnu8_mu(vbool16_t vm, vuint8mf2_t vd, const void *rs1,
                                 size_t vl);
vuint8m1_t __riscv_nds_vlnu8_mu(vbool8_t vm, vuint8m1_t vd, const void *rs1,
                                size_t vl);
vuint8m2_t __riscv_nds_vlnu8_mu(vbool4_t vm, vuint8m2_t vd, const void *rs1,
                                size_t vl);
vuint8m4_t __riscv_nds_vlnu8_mu(vbool2_t vm, vuint8m4_t vd, const void *rs1,
                                size_t vl);
vuint8m8_t __riscv_nds_vlnu8_mu(vbool1_t vm, vuint8m8_t vd, const void *rs1,
                                size_t vl);
----
